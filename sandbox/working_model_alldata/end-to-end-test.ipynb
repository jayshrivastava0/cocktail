{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11a9cbf4-c11b-478f-9afb-8f7b0b26e836",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rasterio\n",
      "  Using cached rasterio-1.3.10-cp310-cp310-manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Collecting affine (from rasterio)\n",
      "  Using cached affine-2.4.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs in /opt/conda/lib/python3.10/site-packages (from rasterio) (23.1.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from rasterio) (2023.11.17)\n",
      "Requirement already satisfied: click>=4.0 in /opt/conda/lib/python3.10/site-packages (from rasterio) (8.1.7)\n",
      "Collecting cligj>=0.5 (from rasterio)\n",
      "  Using cached cligj-0.7.2-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rasterio) (1.24.4)\n",
      "Collecting snuggs>=1.4.1 (from rasterio)\n",
      "  Using cached snuggs-1.4.7-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting click-plugins (from rasterio)\n",
      "  Using cached click_plugins-1.1.1-py2.py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from rasterio) (68.2.2)\n",
      "Requirement already satisfied: pyparsing>=2.1.6 in /opt/conda/lib/python3.10/site-packages (from snuggs>=1.4.1->rasterio) (3.1.1)\n",
      "Using cached rasterio-1.3.10-cp310-cp310-manylinux2014_x86_64.whl (21.5 MB)\n",
      "Using cached cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
      "Using cached snuggs-1.4.7-py3-none-any.whl (5.4 kB)\n",
      "Using cached affine-2.4.0-py3-none-any.whl (15 kB)\n",
      "Using cached click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
      "Installing collected packages: snuggs, cligj, click-plugins, affine, rasterio\n",
      "Successfully installed affine-2.4.0 click-plugins-1.1.1 cligj-0.7.2 rasterio-1.3.10 snuggs-1.4.7\n",
      "Collecting geopandas\n",
      "  Using cached geopandas-0.14.4-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting fiona>=1.8.21 (from geopandas)\n",
      "  Using cached fiona-1.9.6-cp310-cp310-manylinux2014_x86_64.whl.metadata (50 kB)\n",
      "Requirement already satisfied: numpy>=1.22 in /opt/conda/lib/python3.10/site-packages (from geopandas) (1.24.4)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from geopandas) (23.2)\n",
      "Requirement already satisfied: pandas>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from geopandas) (2.0.3)\n",
      "Collecting pyproj>=3.3.0 (from geopandas)\n",
      "  Using cached pyproj-3.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: shapely>=1.8.0 in /opt/conda/lib/python3.10/site-packages (from geopandas) (2.0.2)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.10/site-packages (from fiona>=1.8.21->geopandas) (23.1.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from fiona>=1.8.21->geopandas) (2023.11.17)\n",
      "Requirement already satisfied: click~=8.0 in /opt/conda/lib/python3.10/site-packages (from fiona>=1.8.21->geopandas) (8.1.7)\n",
      "Requirement already satisfied: click-plugins>=1.0 in /opt/conda/lib/python3.10/site-packages (from fiona>=1.8.21->geopandas) (1.1.1)\n",
      "Requirement already satisfied: cligj>=0.5 in /opt/conda/lib/python3.10/site-packages (from fiona>=1.8.21->geopandas) (0.7.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fiona>=1.8.21->geopandas) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.4.0->geopandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.4.0->geopandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.4.0->geopandas) (2023.3)\n",
      "Using cached geopandas-0.14.4-py3-none-any.whl (1.1 MB)\n",
      "Using cached fiona-1.9.6-cp310-cp310-manylinux2014_x86_64.whl (15.7 MB)\n",
      "Using cached pyproj-3.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
      "Installing collected packages: pyproj, fiona, geopandas\n",
      "Successfully installed fiona-1.9.6 geopandas-0.14.4 pyproj-3.6.1\n"
     ]
    }
   ],
   "source": [
    "!pip install rasterio\n",
    "!pip install geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c213cb42-d570-4552-bc2e-ac425d92d279",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/image_folder/\n",
      "Skipped (already exists): area2_planetlabs_superdove/\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import storage\n",
    "import os\n",
    "\n",
    "\n",
    "def download_file_from_gcs(bucket_name, source_blob_name, destination_folder):\n",
    "    client = storage.Client()\n",
    "\n",
    "    # Get the bucket\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "\n",
    "    # Get the specific blob\n",
    "    blob = bucket.blob(source_blob_name)\n",
    "\n",
    "    # Define destination path\n",
    "    destination_path = os.path.join(destination_folder, os.path.basename(source_blob_name))\n",
    "    print(destination_path)\n",
    "    # Check if file already exists locally\n",
    "    if not os.path.exists(destination_path):\n",
    "        blob.download_to_filename(destination_path)\n",
    "        print(f\"Downloaded: {source_blob_name}\")\n",
    "    else:\n",
    "        print(f\"Skipped (already exists): {source_blob_name}\")\n",
    "\n",
    "\n",
    "username = os.getenv(\"USER\") or os.getenv(\"LOGNAME\")\n",
    "\n",
    "# Replace these with your specific details\n",
    "bucket_name = 'gislogics-bucket'\n",
    "source_blob_name = 'area2_planetlabs_superdove/'\n",
    "destination_folder = '/home/jupyter/image_folder/'\n",
    "\n",
    "download_file_from_gcs(bucket_name, source_blob_name, destination_folder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14cf64ea-ec6f-4d34-bd5e-019485c6564c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: area2_planetlabs_superdove/Copy of area2_0530_2022_8bands.tif\n",
      "Downloaded: area2_planetlabs_superdove/area2_0516_2023_8bands.tif\n",
      "Downloaded: area2_planetlabs_superdove/area2_0516_2023_composite.tif\n",
      "Downloaded: area2_planetlabs_superdove/area2_0529_2023_composite.tif\n",
      "Skipped (already exists): area2_planetlabs_superdove/area2_0530_2022_8bands.tif\n",
      "Downloaded: area2_planetlabs_superdove/area2_0609_2023_composite.tif\n",
      "Downloaded: area2_planetlabs_superdove/area2_0617_2023_8bands.tif\n",
      "Downloaded: area2_planetlabs_superdove/area2_0626_2023_composite.tif\n",
      "Downloaded: area2_planetlabs_superdove/area2_0909_2023_composite.tif\n",
      "Downloaded: area2_planetlabs_superdove/area2_0927_2023_composite.tif\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import storage\n",
    "import os\n",
    "\n",
    "\n",
    "def download_files_from_gcs(bucket_name, source_folder_name, destination_folder):\n",
    "    client = storage.Client()\n",
    "\n",
    "    # Get the bucket\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "\n",
    "    # List all blobs with prefix (folder path)\n",
    "    blobs = bucket.list_blobs(prefix=source_folder_name)\n",
    "\n",
    "    for blob in blobs:\n",
    "        # Get the relative path of the blob within the subfolder\n",
    "        relative_path = os.path.relpath(blob.name, source_folder_name)\n",
    "\n",
    "        # Define destination path\n",
    "        destination_path = os.path.join(destination_folder, relative_path)\n",
    "\n",
    "        # Ensure destination directory exists\n",
    "        os.makedirs(os.path.dirname(destination_path), exist_ok=True)\n",
    "\n",
    "        # Check if file already exists locally\n",
    "        if not os.path.exists(destination_path):\n",
    "            blob.download_to_filename(destination_path)\n",
    "            print(f\"Downloaded: {blob.name}\")\n",
    "        else:\n",
    "            print(f\"Skipped (already exists): {blob.name}\")\n",
    "\n",
    "\n",
    "username = os.getenv(\"USER\") or os.getenv(\"LOGNAME\")\n",
    "\n",
    "# Replace these with your specific details\n",
    "bucket_name = 'gislogics-bucket'\n",
    "source_folder_name = 'area2_planetlabs_superdove/'\n",
    "destination_folder = '/home/jupyter/image_folder/'\n",
    "\n",
    "download_files_from_gcs(bucket_name, source_folder_name, destination_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5487b962-ffe8-423a-a530-049d71477bcc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/label_folder/continuous_label_raster.tif\n",
      "Downloaded: labels/continuous_label_raster.tif\n"
     ]
    }
   ],
   "source": [
    "# Replace these with your specific details\n",
    "bucket_name = 'tf_records_bucket'\n",
    "source_blob_name = 'labels/continuous_label_raster.tif'\n",
    "destination_folder = '/home/jupyter/label_folder/'\n",
    "\n",
    "# download_file_from_gcs(bucket_name, source_blob_name, destination_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd3661e9-5ea6-4a44-af95-cac1237d43e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/label_folder/newextent_1123.geojson\n",
      "Downloaded: labels/newextent_1123.geojson\n"
     ]
    }
   ],
   "source": [
    "# Replace these with your specific details\n",
    "bucket_name = 'tf_records_bucket'\n",
    "source_blob_name = 'labels/newextent_1123.geojson'\n",
    "destination_folder = '/home/jupyter/label_folder/'\n",
    "\n",
    "download_file_from_gcs(bucket_name, source_blob_name, destination_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0308153-87f7-49eb-a18d-9ddac41fcef2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-15 02:29:34.757825: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-15 02:29:43.480458: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-15 02:29:58.118942: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-05-15 02:29:58.119181: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-05-15 02:29:58.119196: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://tf_records_bucket/baseline_test/area2_0609_2023_composite_tfrecord.tfrecord\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-15 02:30:18.694426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-05-15 02:30:19.930631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-05-15 02:30:19.933957: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-05-15 02:30:20.062338: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-15 02:30:20.083429: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-05-15 02:30:20.086318: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-05-15 02:30:20.088921: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-05-15 02:30:33.195750: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-05-15 02:30:33.211927: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-05-15 02:30:33.213551: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-05-15 02:30:33.215057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13584 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://tf_records_bucket/baseline_test/area2_0617_2023_8bands_tfrecord.tfrecord\n",
      "gs://tf_records_bucket/baseline_test/area2_0626_2023_composite_tfrecord.tfrecord\n",
      "gs://tf_records_bucket/baseline_test/area2_0909_2023_composite_tfrecord.tfrecord\n",
      "gs://tf_records_bucket/baseline_test/area2_0529_2023_composite_tfrecord.tfrecord\n",
      "gs://tf_records_bucket/baseline_test/area2_0516_2023_composite_tfrecord.tfrecord\n",
      "gs://tf_records_bucket/baseline_test/Copy of area2_0530_2022_8bands_tfrecord.tfrecord\n",
      "gs://tf_records_bucket/baseline_test/area2_0516_2023_8bands_tfrecord.tfrecord\n",
      "gs://tf_records_bucket/baseline_test/area2_0530_2022_8bands_tfrecord.tfrecord\n",
      "gs://tf_records_bucket/baseline_test/area2_0927_2023_composite_tfrecord.tfrecord\n"
     ]
    }
   ],
   "source": [
    "#TF Record Creation: Convert given input 8-band tiff file and label tiff file to tf records\n",
    "\n",
    "import os\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import mapping\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "from google.cloud import storage\n",
    "\n",
    "\n",
    "# Specify the folder path where your images are located\n",
    "folder_path = '/home/jupyter/image_folder/'\n",
    "\n",
    "# Specify the folder path where your labels are located\n",
    "label_folder = '/home/jupyter/label_folder/'\n",
    "\n",
    "#Specify the path where your clipping mask are located\n",
    "geojson_datapath = '/home/jupyter/label_folder/newextent_1123.geojson'\n",
    "\n",
    "# Specify the GCS bucket name\n",
    "\n",
    "bucket_name = 'tf_records_bucket'\n",
    "\n",
    "# Specify the GCS path for TFRecords\n",
    "gcs_output_dir = f'gs://{bucket_name}/baseline_test/'\n",
    "\n",
    "def create_gcs_bucket(bucket_name):\n",
    "    # Create a GCS client\n",
    "    client = storage.Client()\n",
    "\n",
    "    # Create a GCS bucket\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    if not bucket.exists():\n",
    "        bucket.create()\n",
    "\n",
    "\n",
    "def clip_tiff(tiff, geojson = geojson_datapath):\n",
    "\n",
    "    with open(geojson) as clip_geojson:\n",
    "        clip_geojson = gpd.read_file(clip_geojson)\n",
    "        clip_geometry = clip_geojson.geometry.values[0]\n",
    "        clip_geojson = mapping(clip_geometry)\n",
    "\n",
    "    with rasterio.open(tiff) as src:\n",
    "        # Perform the clip\n",
    "        clip_image, clip_transform = mask(src, [clip_geojson], crop=True)\n",
    "\n",
    "    return clip_image\n",
    "\n",
    "# preprocessing functions\n",
    "\n",
    "def resize_img(image,label):\n",
    "  image = tf.image.resize_with_crop_or_pad(image, label.shape[0], label.shape[1])\n",
    "  return image, label\n",
    "\n",
    "\n",
    "def process_input(image, label):\n",
    "\n",
    "    tensor_image = tf.convert_to_tensor(image)\n",
    "    tensor_image = tf.transpose(tensor_image, perm=[1, 2, 0])\n",
    "    tensor_label = tf.convert_to_tensor(label)\n",
    "    tensor_label = tf.transpose(tensor_label, perm=[1, 2, 0])\n",
    "\n",
    "    if tensor_label.shape[:2] != tensor_image.shape[:2]:\n",
    "      tensor_image, tensor_label = resize_img(tensor_image, tensor_label)\n",
    "\n",
    "    return tensor_image, tensor_label\n",
    "\n",
    "\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "\n",
    "def _float_feature(value):\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "def create_tfrecord(image, label):\n",
    "    image = clip_tiff(image)\n",
    "    label = clip_tiff(label)\n",
    "    image, label = process_input(image, label)\n",
    "    image_dims = image.shape\n",
    "    label_dims = label.shape\n",
    "\n",
    "    image = tf.reshape(image, [-1])  # flatten to 1D array\n",
    "    label = tf.reshape(label, [-1])  # flatten to 1D array\n",
    "\n",
    "    return tf.train.Example(\n",
    "        features=tf.train.Features(\n",
    "            feature={\n",
    "                \"image\": _float_feature(image.numpy()),\n",
    "                \"image_shape\": _int64_feature(\n",
    "                    [image_dims[0], image_dims[1], image_dims[2]]\n",
    "                ),\n",
    "                \"label\": _float_feature(label.numpy()),\n",
    "                \"label_shape\": _int64_feature([label_dims[0], label_dims[1], label_dims[2]]),\n",
    "            }\n",
    "        )\n",
    "    ).SerializeToString()\n",
    "\n",
    "def write_tfrecords(images, labels, gcs_output_directory):\n",
    "    create_gcs_bucket(bucket_name)\n",
    "\n",
    "    for image, label in zip(images, labels):\n",
    "        # Create GCS path for TFRecord\n",
    "        output_file_gcs = os.path.join(gcs_output_directory, f\"{image.replace(folder_path,'').replace('.tif','')}_tfrecord.tfrecord\")\n",
    "        print(output_file_gcs)\n",
    "\n",
    "        # Open a GCS file for writing\n",
    "        with tf.io.TFRecordWriter(output_file_gcs) as writer:\n",
    "            tf_example_bytes = create_tfrecord(image, label)\n",
    "            writer.write(tf_example_bytes)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "images = glob.glob(folder_path + '*.tif')  # list of file locations of all images\n",
    "number_of_images = len(images)\n",
    "labels = glob.glob(label_folder + '*.tif')\n",
    "labels = labels * number_of_images\n",
    "write_tfrecords(images, labels, gcs_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b5811c1-39cc-4e01-ba5d-88266340315c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52416d7b-f9f8-4638-9e78-6748c11df0fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from google.cloud import storage\n",
    "\n",
    "\n",
    "feature_description = {\n",
    "    \"image\": tf.io.VarLenFeature(tf.float32),\n",
    "    \"image_shape\": tf.io.VarLenFeature(tf.int64),\n",
    "    \"label\": tf.io.VarLenFeature(tf.float32),\n",
    "    \"label_shape\": tf.io.VarLenFeature(tf.int64),\n",
    "}\n",
    "\n",
    "\n",
    "def parse(serialized_examples):\n",
    "    return tf.io.parse_example(serialized_examples, feature_description)\n",
    "\n",
    "\n",
    "def create_dataset(input_directory):\n",
    "    tfrecord_files = [\n",
    "        f\"{input_directory}{file}\"\n",
    "        for file in tf.io.gfile.listdir(input_directory)\n",
    "        if file.endswith(\".tfrecord\")\n",
    "    ]\n",
    "    dataset = tf.data.TFRecordDataset(tfrecord_files)\n",
    "    dataset = dataset.map(parse)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "# pre-processing functions\n",
    "def bandwise_normalize(input_tensor, epsilon=1e-8):\n",
    "    # Convert the input_tensor to a float32 type\n",
    "    input_tensor = tf.cast(input_tensor, tf.float32)\n",
    "\n",
    "    # Calculate the minimum and maximum values along the channel axis\n",
    "    min_val = tf.reduce_min(input_tensor, axis=2, keepdims=True)\n",
    "    max_val = tf.reduce_max(input_tensor, axis=2, keepdims=True)\n",
    "\n",
    "    # Check for potential numerical instability\n",
    "    denom = max_val - min_val\n",
    "    denom = tf.where(tf.abs(denom) < epsilon, epsilon, denom)\n",
    "\n",
    "    # Normalize the tensor band-wise to the range [0, 1]\n",
    "    normalized_tensor = (input_tensor - min_val) / denom\n",
    "\n",
    "    return normalized_tensor\n",
    "\n",
    "\n",
    "def pad_to_multiple(image, TILE_HT, TILE_WD):\n",
    "    # Get the current dimensions\n",
    "    height, width, channels = image.shape\n",
    "\n",
    "    # Calculate the target dimensions\n",
    "    target_height = tf.cast(tf.math.ceil(height / TILE_HT) * TILE_HT, tf.int32)\n",
    "    target_width = tf.cast(tf.math.ceil(width / TILE_WD) * TILE_WD, tf.int32)\n",
    "\n",
    "    # Calculate the amount of padding\n",
    "    pad_height = target_height - height\n",
    "    pad_width = target_width - width\n",
    "\n",
    "    # Pad the image\n",
    "    padded_image = tf.image.resize_with_crop_or_pad(image, target_height, target_width)\n",
    "\n",
    "    return padded_image\n",
    "\n",
    "\n",
    "def tile_image(fullimg, CHANNELS, TILE_HT, TILE_WD):\n",
    "    fullimg = pad_to_multiple(fullimg, TILE_HT, TILE_WD)\n",
    "    images = tf.expand_dims(fullimg, axis=0)\n",
    "    tiles = tf.image.extract_patches(\n",
    "        images=images,\n",
    "        sizes=[1, TILE_HT, TILE_WD, 1],\n",
    "        strides=[1, TILE_HT, TILE_WD, 1],\n",
    "        rates=[1, 1, 1, 1],\n",
    "        padding=\"VALID\",\n",
    "    )\n",
    "\n",
    "    tiles = tf.squeeze(tiles, axis=0)\n",
    "    nrows = tiles.shape[0]\n",
    "    ncols = tiles.shape[1]\n",
    "    tiles = tf.reshape(tiles, [nrows, ncols, TILE_HT, TILE_WD, CHANNELS])\n",
    "    return tiles\n",
    "\n",
    "\n",
    "def sampling(label_image, threshold_percentage=99.9):\n",
    "    num_zeros = tf.reduce_sum(\n",
    "        tf.cast(tf.equal(label_image, 0), tf.float32), axis=[2, 3, 4]\n",
    "    )\n",
    "\n",
    "    # Calculate the total number of elements in each patch\n",
    "    total_elements = tf.cast(tf.reduce_prod(tf.shape(label_image)[2:]), tf.float32)\n",
    "\n",
    "    # Calculate the percentage of zeros in each patch\n",
    "    percentage_zeros = (num_zeros / total_elements) * 100.0\n",
    "\n",
    "    boolean_mask = percentage_zeros <= threshold_percentage\n",
    "    # Apply the threshold logic\n",
    "    sampled_tensor = tf.cast(percentage_zeros >= threshold_percentage, tf.int32)\n",
    "    return boolean_mask, sampled_tensor\n",
    "\n",
    "\n",
    "def one_hot_encoding(label_tensor):\n",
    "    # Assuming your pixel values are float labels\n",
    "    float_labels = tf.squeeze(\n",
    "        label_tensor, axis=-1\n",
    "    )  # Assuming channel dimension is the last one\n",
    "\n",
    "    # Determine the number of classes dynamically\n",
    "    num_classes = tf.cast(tf.reduce_max(float_labels) + 1, tf.int32)\n",
    "\n",
    "    # One-hot encode each image\n",
    "    one_hot_encoded_images = tf.one_hot(\n",
    "        tf.dtypes.cast(float_labels, tf.int32), depth=num_classes\n",
    "    )\n",
    "\n",
    "    # Print the shape of the resulting tensor and the number of classes\n",
    "    # print(\"Shape of one-hot encoded images:\", one_hot_encoded_images.shape)\n",
    "    # print(\"Number of classes:\", num_classes)\n",
    "\n",
    "    return one_hot_encoded_images\n",
    "\n",
    "\n",
    "def parsing(\n",
    "    dataset,\n",
    "    patch_height,\n",
    "    patch_width,\n",
    "    threshold_percentage,\n",
    "    image_channels,\n",
    "    label_channels,\n",
    "):\n",
    "    image_patch_tensors_list = []\n",
    "    label_patch_tensors_list = []\n",
    "\n",
    "    for parsed_example in dataset:\n",
    "        image_shape = tf.sparse.to_dense(parsed_example[\"image_shape\"])\n",
    "        image = tf.reshape(tf.sparse.to_dense(parsed_example[\"image\"]), image_shape)\n",
    "        label_shape = tf.sparse.to_dense(parsed_example[\"label_shape\"])\n",
    "        label = tf.reshape(tf.sparse.to_dense(parsed_example[\"label\"]), label_shape)\n",
    "\n",
    "        # image normalization\n",
    "        image = bandwise_normalize(image)\n",
    "\n",
    "        # image and label patching\n",
    "        image_patches = tile_image(image, image_channels, patch_height, patch_width)\n",
    "        label_patches = tile_image(label, label_channels, patch_height, patch_width)\n",
    "\n",
    "        # sampling\n",
    "        sampled_mask, sampled_tensor = sampling(label_patches, threshold_percentage)\n",
    "        sampled_image_patches = tf.boolean_mask(image_patches, sampled_mask)\n",
    "        sampled_label_patches = tf.boolean_mask(label_patches, sampled_mask)\n",
    "\n",
    "        # one-hot encoding\n",
    "        sampled_label_patches = one_hot_encoding(sampled_label_patches)\n",
    "\n",
    "        # save them in the list\n",
    "        image_patch_tensors_list.append(sampled_image_patches)\n",
    "        label_patch_tensors_list.append(sampled_label_patches)\n",
    "\n",
    "    return image_patch_tensors_list, label_patch_tensors_list\n",
    "\n",
    "\n",
    "def train_test_datasets(\n",
    "    input_directory,\n",
    "    patch_height,\n",
    "    patch_width,\n",
    "    image_channels,\n",
    "    label_channels,\n",
    "    threshold_percentage,\n",
    "    batch_size,\n",
    "):\n",
    "    dataset = create_dataset(input_directory)\n",
    "    image_patch_tensors_list, label_patch_tensors_list = parsing(\n",
    "        dataset=dataset,\n",
    "        patch_height=patch_height,\n",
    "        patch_width=patch_width,\n",
    "        image_channels=image_channels,\n",
    "        label_channels=label_channels,\n",
    "        threshold_percentage=threshold_percentage,\n",
    "    )\n",
    "\n",
    "    # Combine images and labels from different pairs\n",
    "    combined_images = tf.concat(image_patch_tensors_list, axis=0)\n",
    "    combined_labels = tf.concat(label_patch_tensors_list, axis=0)\n",
    "\n",
    "    # Shuffle the combined data\n",
    "    combined_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (combined_images, combined_labels)\n",
    "    )\n",
    "    combined_dataset = combined_dataset.shuffle(buffer_size=combined_images.shape[0])\n",
    "\n",
    "    # Split the combined dataset into training and validation sets\n",
    "    train_size = int(0.8 * combined_images.shape[0])\n",
    "    train_dataset = combined_dataset.take(train_size)\n",
    "    val_dataset = combined_dataset.skip(train_size)\n",
    "\n",
    "    # Batch the data using TensorFlow's Dataset API\n",
    "    train_dataset = train_dataset.batch(batch_size)\n",
    "    val_dataset = val_dataset.batch(batch_size)\n",
    "\n",
    "    return train_dataset, val_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "711ec89f-7ad4-4805-9f7f-3e721f07f178",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__Pack_N_3_device_/job:localhost/replica:0/task:0/device:GPU:0}} Shapes of all inputs must match: values[0].shape = [231,287,16,16,1] != values[1].shape = [66297,16,16,1] [Op:Pack]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m threshold_percentage \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m99\u001b[39m\n\u001b[1;32m      7\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m\n\u001b[0;32m----> 9\u001b[0m train_dataset, val_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_datasets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgcs_tfrecords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatch_height\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatch_width\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthreshold_percentage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 172\u001b[0m, in \u001b[0;36mtrain_test_datasets\u001b[0;34m(input_directory, patch_height, patch_width, image_channels, label_channels, threshold_percentage, batch_size)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_test_datasets\u001b[39m(\n\u001b[1;32m    163\u001b[0m     input_directory,\n\u001b[1;32m    164\u001b[0m     patch_height,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    169\u001b[0m     batch_size,\n\u001b[1;32m    170\u001b[0m ):\n\u001b[1;32m    171\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m create_dataset(input_directory)\n\u001b[0;32m--> 172\u001b[0m     image_patch_tensors_list, label_patch_tensors_list \u001b[38;5;241m=\u001b[39m \u001b[43mparsing\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpatch_height\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatch_height\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpatch_width\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatch_width\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthreshold_percentage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreshold_percentage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;66;03m# Combine images and labels from different pairs\u001b[39;00m\n\u001b[1;32m    182\u001b[0m     combined_images \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconcat(image_patch_tensors_list, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 148\u001b[0m, in \u001b[0;36mparsing\u001b[0;34m(dataset, patch_height, patch_width, threshold_percentage, image_channels, label_channels)\u001b[0m\n\u001b[1;32m    145\u001b[0m label_patches \u001b[38;5;241m=\u001b[39m tile_image(label, label_channels, patch_height, patch_width)\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# sampling\u001b[39;00m\n\u001b[0;32m--> 148\u001b[0m sampled_mask, sampled_tensor \u001b[38;5;241m=\u001b[39m \u001b[43msampling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel_patches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold_percentage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m sampled_image_patches \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mboolean_mask(image_patches, sampled_mask)\n\u001b[1;32m    150\u001b[0m sampled_label_patches \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mboolean_mask(label_patches, sampled_mask)\n",
      "Cell \u001b[0;32mIn[6], line 87\u001b[0m, in \u001b[0;36msampling\u001b[0;34m(label_image, threshold_percentage)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msampling\u001b[39m(label_image, threshold_percentage\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m99.9\u001b[39m):\n\u001b[1;32m     86\u001b[0m     num_zeros \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_sum(\n\u001b[0;32m---> 87\u001b[0m         tf\u001b[38;5;241m.\u001b[39mcast(\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mequal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m, tf\u001b[38;5;241m.\u001b[39mfloat32), axis\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m]\n\u001b[1;32m     88\u001b[0m     )\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;66;03m# Calculate the total number of elements in each patch\u001b[39;00m\n\u001b[1;32m     91\u001b[0m     total_elements \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(tf\u001b[38;5;241m.\u001b[39mreduce_prod(tf\u001b[38;5;241m.\u001b[39mshape(label_image)[\u001b[38;5;241m2\u001b[39m:]), tf\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7209\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[1;32m   7208\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 7209\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__Pack_N_3_device_/job:localhost/replica:0/task:0/device:GPU:0}} Shapes of all inputs must match: values[0].shape = [231,287,16,16,1] != values[1].shape = [66297,16,16,1] [Op:Pack]"
     ]
    }
   ],
   "source": [
    "gcs_tfrecords = 'gs://tf_records_bucket/baseline_test/'\n",
    "patch_height = 16\n",
    "patch_width = 16\n",
    "image_channels = 8\n",
    "label_channels = 1\n",
    "threshold_percentage = 99\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset, val_dataset = train_test_datasets(\n",
    "    gcs_tfrecords,\n",
    "    patch_height,\n",
    "    patch_width,\n",
    "    image_channels,\n",
    "    label_channels,\n",
    "    threshold_percentage,\n",
    "    batch_size,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d7aa6f4-505a-4a5e-8e76-b3e7bebe28c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization, Dropout, Lambda\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cc61456-f6d3-43b4-8057-5692ac997fc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.layers import Input, Conv2D, Dropout, Conv2DTranspose, UpSampling2D, concatenate\n",
    "\n",
    "def r_multi_unet_model(n_classes=21, IMG_HEIGHT=16, IMG_WIDTH=16, IMG_CHANNELS=8):\n",
    "  # Build the model\n",
    "  inputs = Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
    "\n",
    "  # Contraction path\n",
    "  c1 = Conv2D(16, (3, 3), activation='relu', padding='same')(inputs)\n",
    "  c1 = Dropout(0.2)(c1)\n",
    "  c1 = Conv2D(16, (3, 3), activation='relu', padding='same')(c1)\n",
    "\n",
    "  # Use strided convolutions instead of pooling\n",
    "  c2 = Conv2D(32, (3, 3), strides=(2, 2), activation='relu', padding='same')(c1)\n",
    "  c2 = Dropout(0.2)(c2)\n",
    "  c2 = Conv2D(32, (3, 3), activation='relu', padding='same')(c2)\n",
    "\n",
    "  c3 = Conv2D(64, (3, 3), strides=(2, 2), activation='relu', padding='same')(c2)\n",
    "  c3 = Dropout(0.2)(c3)\n",
    "  c3 = Conv2D(64, (3, 3), activation='relu', padding='same')(c3)\n",
    "\n",
    "  c4 = Conv2D(128, (3, 3), strides=(2, 2), activation='relu', padding='same')(c3)\n",
    "  c4 = Dropout(0.2)(c4)\n",
    "  c4 = Conv2D(128, (3, 3), activation='relu', padding='same')(c4)\n",
    "\n",
    "  c5 = Conv2D(256, (3, 3), activation='relu', padding='same')(c4)\n",
    "  c5 = Dropout(0.3)(c5)\n",
    "  c5 = Conv2D(256, (3, 3), activation='relu', padding='same')(c5)\n",
    "\n",
    "  # Expansive path\n",
    "  u6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n",
    "  c4_up = UpSampling2D(size=(2, 2))(c4)  # Upsample c4 to match u6 dimensions\n",
    "  u6 = concatenate([u6, c4_up], axis=3)\n",
    "  c6 = Conv2D(128, (3, 3), activation='relu', padding='same')(u6)\n",
    "  c6 = Dropout(0.2)(c6)\n",
    "  c6 = Conv2D(128, (3, 3), activation='relu', padding='same')(c6)\n",
    "\n",
    "  u7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\n",
    "  c3_up = UpSampling2D(size=(2, 2))(c3)  # Upsample c3 to match u7 dimensions\n",
    "  u7 = concatenate([u7, c3_up], axis=3)\n",
    "  c7 = Conv2D(64, (3, 3), activation='relu', padding='same')(u7)\n",
    "  c7 = Dropout(0.2)(c7)\n",
    "  c7 = Conv2D(64, (3, 3), activation='relu', padding='same')(c7)\n",
    "\n",
    "  u8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)\n",
    "  c2_up = UpSampling2D(size=(2, 2))(c2)  # Upsample c2 to match u8 dimensions\n",
    "  u8 = concatenate([u8, c2_up], axis=3)\n",
    "  c8 = Conv2D(32, (3, 3), activation='relu', padding='same')(u8)\n",
    "  c8 = Dropout(0.2)(c8)\n",
    "  c8 = Conv2D(32, (3, 3), activation='relu', padding='same')(c8)\n",
    "\n",
    "  # Output layer\n",
    "  outputs = Conv2D(n_classes, (1, 1), activation='softmax')(c8)  # Corrected typo (Conv2D, not Conv2)\n",
    "    \n",
    "  model = Model(inputs=[inputs], outputs=[outputs])\n",
    "  return model\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63f65791-62e6-44fd-9bc1-e699e8cddc49",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.functional.Functional object at 0x7f278eeaed10>\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 16, 16, 8)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 16, 16, 16)   1168        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 16, 16, 16)   0           ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 16, 16, 16)   2320        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 8, 8, 32)     4640        ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 8, 8, 32)     0           ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 8, 8, 32)     9248        ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 4, 4, 64)     18496       ['conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 4, 4, 64)     0           ['conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 4, 4, 64)     36928       ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 2, 2, 128)    73856       ['conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 2, 2, 128)    0           ['conv2d_6[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 2, 2, 128)    147584      ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 2, 2, 256)    295168      ['conv2d_7[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 2, 2, 256)    0           ['conv2d_8[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 2, 2, 256)    590080      ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_transpose (Conv2DTransp  (None, 4, 4, 128)   131200      ['conv2d_9[0][0]']               \n",
      " ose)                                                                                             \n",
      "                                                                                                  \n",
      " up_sampling2d (UpSampling2D)   (None, 4, 4, 128)    0           ['conv2d_7[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 4, 4, 256)    0           ['conv2d_transpose[0][0]',       \n",
      "                                                                  'up_sampling2d[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 4, 4, 128)    295040      ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 4, 4, 128)    0           ['conv2d_10[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 4, 4, 128)    147584      ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_transpose_1 (Conv2DTran  (None, 8, 8, 64)    32832       ['conv2d_11[0][0]']              \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " up_sampling2d_1 (UpSampling2D)  (None, 8, 8, 64)    0           ['conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 8, 8, 128)    0           ['conv2d_transpose_1[0][0]',     \n",
      "                                                                  'up_sampling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 8, 8, 64)     73792       ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 8, 8, 64)     0           ['conv2d_12[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 8, 8, 64)     36928       ['dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_transpose_2 (Conv2DTran  (None, 16, 16, 32)  8224        ['conv2d_13[0][0]']              \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " up_sampling2d_2 (UpSampling2D)  (None, 16, 16, 32)  0           ['conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 16, 16, 64)   0           ['conv2d_transpose_2[0][0]',     \n",
      "                                                                  'up_sampling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 16, 16, 32)   18464       ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 16, 16, 32)   0           ['conv2d_14[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 16, 16, 32)   9248        ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 16, 16, 21)   693         ['conv2d_15[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,933,493\n",
      "Trainable params: 1,933,493\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def get_model():\n",
    "  model = r_multi_unet_model(n_classes=21, IMG_HEIGHT=16, IMG_WIDTH=16, IMG_CHANNELS=8)\n",
    "  print(model)\n",
    "  return model  # Ensure model is returned\n",
    "\n",
    "model = get_model()\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics='accuracy')\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22941c6a-bbaf-46fc-8993-a751225a2744",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 1.1795 - accuracy: 0.5810\n",
      "Epoch 1: val_loss improved from inf to 1.17726, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 10s 13ms/step - loss: 1.1795 - accuracy: 0.5810 - val_loss: 1.1773 - val_accuracy: 0.5783\n",
      "Epoch 2/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 1.1539 - accuracy: 0.5805\n",
      "Epoch 2: val_loss improved from 1.17726 to 1.13073, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.1539 - accuracy: 0.5806 - val_loss: 1.1307 - val_accuracy: 0.5781\n",
      "Epoch 3/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 1.1435 - accuracy: 0.5818\n",
      "Epoch 3: val_loss improved from 1.13073 to 1.12289, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.1435 - accuracy: 0.5818 - val_loss: 1.1229 - val_accuracy: 0.5819\n",
      "Epoch 4/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.1278 - accuracy: 0.5807\n",
      "Epoch 4: val_loss did not improve from 1.12289\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.1278 - accuracy: 0.5807 - val_loss: 1.1322 - val_accuracy: 0.5654\n",
      "Epoch 5/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 1.1189 - accuracy: 0.5822\n",
      "Epoch 5: val_loss improved from 1.12289 to 1.09456, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.1180 - accuracy: 0.5824 - val_loss: 1.0946 - val_accuracy: 0.5909\n",
      "Epoch 6/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 1.1025 - accuracy: 0.5844\n",
      "Epoch 6: val_loss improved from 1.09456 to 1.06255, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 10s 12ms/step - loss: 1.1023 - accuracy: 0.5843 - val_loss: 1.0625 - val_accuracy: 0.5857\n",
      "Epoch 7/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0928 - accuracy: 0.5842\n",
      "Epoch 7: val_loss did not improve from 1.06255\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0928 - accuracy: 0.5843 - val_loss: 1.0854 - val_accuracy: 0.5864\n",
      "Epoch 8/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0812 - accuracy: 0.5852\n",
      "Epoch 8: val_loss improved from 1.06255 to 1.05683, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 10s 12ms/step - loss: 1.0813 - accuracy: 0.5851 - val_loss: 1.0568 - val_accuracy: 0.5858\n",
      "Epoch 9/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 1.0680 - accuracy: 0.5859\n",
      "Epoch 9: val_loss improved from 1.05683 to 1.05253, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 10s 12ms/step - loss: 1.0686 - accuracy: 0.5859 - val_loss: 1.0525 - val_accuracy: 0.5874\n",
      "Epoch 10/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 1.0631 - accuracy: 0.5870\n",
      "Epoch 10: val_loss did not improve from 1.05253\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0631 - accuracy: 0.5870 - val_loss: 1.0616 - val_accuracy: 0.5834\n",
      "Epoch 11/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0584 - accuracy: 0.5854\n",
      "Epoch 11: val_loss improved from 1.05253 to 1.04800, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 10s 13ms/step - loss: 1.0586 - accuracy: 0.5853 - val_loss: 1.0480 - val_accuracy: 0.5892\n",
      "Epoch 12/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0506 - accuracy: 0.5860\n",
      "Epoch 12: val_loss did not improve from 1.04800\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0505 - accuracy: 0.5860 - val_loss: 1.0586 - val_accuracy: 0.5874\n",
      "Epoch 13/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 1.0363 - accuracy: 0.5916\n",
      "Epoch 13: val_loss improved from 1.04800 to 1.02543, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0366 - accuracy: 0.5913 - val_loss: 1.0254 - val_accuracy: 0.5877\n",
      "Epoch 14/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 1.0364 - accuracy: 0.5902\n",
      "Epoch 14: val_loss did not improve from 1.02543\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0364 - accuracy: 0.5902 - val_loss: 1.0506 - val_accuracy: 0.5978\n",
      "Epoch 15/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 1.0256 - accuracy: 0.5909\n",
      "Epoch 15: val_loss improved from 1.02543 to 1.00653, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 10s 13ms/step - loss: 1.0256 - accuracy: 0.5909 - val_loss: 1.0065 - val_accuracy: 0.5887\n",
      "Epoch 16/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 1.0231 - accuracy: 0.5919\n",
      "Epoch 16: val_loss did not improve from 1.00653\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0231 - accuracy: 0.5919 - val_loss: 1.0186 - val_accuracy: 0.5927\n",
      "Epoch 17/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0176 - accuracy: 0.5951\n",
      "Epoch 17: val_loss did not improve from 1.00653\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0179 - accuracy: 0.5950 - val_loss: 1.0118 - val_accuracy: 0.5918\n",
      "Epoch 18/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 1.0160 - accuracy: 0.5939\n",
      "Epoch 18: val_loss improved from 1.00653 to 0.97077, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0160 - accuracy: 0.5939 - val_loss: 0.9708 - val_accuracy: 0.6083\n",
      "Epoch 19/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0112 - accuracy: 0.5936\n",
      "Epoch 19: val_loss did not improve from 0.97077\n",
      "762/762 [==============================] - 10s 12ms/step - loss: 1.0114 - accuracy: 0.5936 - val_loss: 0.9765 - val_accuracy: 0.5948\n",
      "Epoch 20/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0118 - accuracy: 0.5938\n",
      "Epoch 20: val_loss did not improve from 0.97077\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0118 - accuracy: 0.5938 - val_loss: 1.0002 - val_accuracy: 0.5899\n",
      "Epoch 21/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 1.0006 - accuracy: 0.5963\n",
      "Epoch 21: val_loss improved from 0.97077 to 0.96533, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0006 - accuracy: 0.5963 - val_loss: 0.9653 - val_accuracy: 0.6089\n",
      "Epoch 22/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 1.0005 - accuracy: 0.5939\n",
      "Epoch 22: val_loss did not improve from 0.96533\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0003 - accuracy: 0.5939 - val_loss: 0.9817 - val_accuracy: 0.6033\n",
      "Epoch 23/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0032 - accuracy: 0.5960\n",
      "Epoch 23: val_loss did not improve from 0.96533\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0032 - accuracy: 0.5961 - val_loss: 0.9736 - val_accuracy: 0.6016\n",
      "Epoch 24/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.9928 - accuracy: 0.5995\n",
      "Epoch 24: val_loss improved from 0.96533 to 0.95886, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 10s 12ms/step - loss: 0.9928 - accuracy: 0.5995 - val_loss: 0.9589 - val_accuracy: 0.6068\n",
      "Epoch 25/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.9963 - accuracy: 0.5982\n",
      "Epoch 25: val_loss did not improve from 0.95886\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9964 - accuracy: 0.5982 - val_loss: 0.9663 - val_accuracy: 0.6013\n",
      "Epoch 26/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.9909 - accuracy: 0.5989\n",
      "Epoch 26: val_loss did not improve from 0.95886\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9910 - accuracy: 0.5989 - val_loss: 0.9626 - val_accuracy: 0.6122\n",
      "Epoch 27/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.9816 - accuracy: 0.6033\n",
      "Epoch 27: val_loss did not improve from 0.95886\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9812 - accuracy: 0.6035 - val_loss: 0.9702 - val_accuracy: 0.6113\n",
      "Epoch 28/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.9845 - accuracy: 0.6050\n",
      "Epoch 28: val_loss did not improve from 0.95886\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9849 - accuracy: 0.6050 - val_loss: 0.9672 - val_accuracy: 0.6009\n",
      "Epoch 29/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.9784 - accuracy: 0.6034\n",
      "Epoch 29: val_loss improved from 0.95886 to 0.95266, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9787 - accuracy: 0.6034 - val_loss: 0.9527 - val_accuracy: 0.6090\n",
      "Epoch 30/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.9763 - accuracy: 0.6053\n",
      "Epoch 30: val_loss improved from 0.95266 to 0.94574, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9765 - accuracy: 0.6052 - val_loss: 0.9457 - val_accuracy: 0.6104\n",
      "Epoch 31/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.9766 - accuracy: 0.6043\n",
      "Epoch 31: val_loss did not improve from 0.94574\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9766 - accuracy: 0.6042 - val_loss: 0.9695 - val_accuracy: 0.6120\n",
      "Epoch 32/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.9727 - accuracy: 0.6074\n",
      "Epoch 32: val_loss did not improve from 0.94574\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9729 - accuracy: 0.6073 - val_loss: 0.9524 - val_accuracy: 0.6092\n",
      "Epoch 33/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.9714 - accuracy: 0.6055\n",
      "Epoch 33: val_loss did not improve from 0.94574\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9714 - accuracy: 0.6055 - val_loss: 0.9697 - val_accuracy: 0.6072\n",
      "Epoch 34/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.9638 - accuracy: 0.6086\n",
      "Epoch 34: val_loss improved from 0.94574 to 0.94391, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 10s 12ms/step - loss: 0.9639 - accuracy: 0.6085 - val_loss: 0.9439 - val_accuracy: 0.6190\n",
      "Epoch 35/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.9705 - accuracy: 0.6064\n",
      "Epoch 35: val_loss improved from 0.94391 to 0.94319, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9707 - accuracy: 0.6062 - val_loss: 0.9432 - val_accuracy: 0.6146\n",
      "Epoch 36/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.9663 - accuracy: 0.6068\n",
      "Epoch 36: val_loss did not improve from 0.94319\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9661 - accuracy: 0.6069 - val_loss: 0.9517 - val_accuracy: 0.6178\n",
      "Epoch 37/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.9657 - accuracy: 0.6073\n",
      "Epoch 37: val_loss improved from 0.94319 to 0.92386, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9656 - accuracy: 0.6074 - val_loss: 0.9239 - val_accuracy: 0.6109\n",
      "Epoch 38/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.9566 - accuracy: 0.6092\n",
      "Epoch 38: val_loss did not improve from 0.92386\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9568 - accuracy: 0.6091 - val_loss: 0.9273 - val_accuracy: 0.6241\n",
      "Epoch 39/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.9518 - accuracy: 0.6123\n",
      "Epoch 39: val_loss did not improve from 0.92386\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9518 - accuracy: 0.6124 - val_loss: 0.9421 - val_accuracy: 0.6199\n",
      "Epoch 40/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.9585 - accuracy: 0.6123\n",
      "Epoch 40: val_loss did not improve from 0.92386\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9586 - accuracy: 0.6123 - val_loss: 0.9325 - val_accuracy: 0.6102\n",
      "Epoch 41/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.9484 - accuracy: 0.6136\n",
      "Epoch 41: val_loss improved from 0.92386 to 0.90510, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 10s 13ms/step - loss: 0.9482 - accuracy: 0.6138 - val_loss: 0.9051 - val_accuracy: 0.6252\n",
      "Epoch 42/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.9503 - accuracy: 0.6135\n",
      "Epoch 42: val_loss did not improve from 0.90510\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9503 - accuracy: 0.6136 - val_loss: 0.9271 - val_accuracy: 0.6233\n",
      "Epoch 43/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.9413 - accuracy: 0.6158\n",
      "Epoch 43: val_loss improved from 0.90510 to 0.89478, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 10s 13ms/step - loss: 0.9413 - accuracy: 0.6157 - val_loss: 0.8948 - val_accuracy: 0.6306\n",
      "Epoch 44/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.9432 - accuracy: 0.6163\n",
      "Epoch 44: val_loss improved from 0.89478 to 0.89450, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9433 - accuracy: 0.6160 - val_loss: 0.8945 - val_accuracy: 0.6279\n",
      "Epoch 45/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.9475 - accuracy: 0.6157\n",
      "Epoch 45: val_loss did not improve from 0.89450\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9475 - accuracy: 0.6157 - val_loss: 0.9285 - val_accuracy: 0.6285\n",
      "Epoch 46/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.9420 - accuracy: 0.6195\n",
      "Epoch 46: val_loss did not improve from 0.89450\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9419 - accuracy: 0.6195 - val_loss: 0.9197 - val_accuracy: 0.6238\n",
      "Epoch 47/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.9412 - accuracy: 0.6162\n",
      "Epoch 47: val_loss did not improve from 0.89450\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9409 - accuracy: 0.6163 - val_loss: 0.9223 - val_accuracy: 0.6201\n",
      "Epoch 48/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.9378 - accuracy: 0.6192\n",
      "Epoch 48: val_loss did not improve from 0.89450\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9378 - accuracy: 0.6192 - val_loss: 0.9196 - val_accuracy: 0.6294\n",
      "Epoch 49/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.9406 - accuracy: 0.6181\n",
      "Epoch 49: val_loss improved from 0.89450 to 0.89397, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9406 - accuracy: 0.6181 - val_loss: 0.8940 - val_accuracy: 0.6421\n",
      "Epoch 50/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.9368 - accuracy: 0.6199\n",
      "Epoch 50: val_loss improved from 0.89397 to 0.87884, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9370 - accuracy: 0.6199 - val_loss: 0.8788 - val_accuracy: 0.6441\n",
      "Epoch 51/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.9331 - accuracy: 0.6217\n",
      "Epoch 51: val_loss did not improve from 0.87884\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9333 - accuracy: 0.6216 - val_loss: 0.8987 - val_accuracy: 0.6313\n",
      "Epoch 52/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.9386 - accuracy: 0.6232\n",
      "Epoch 52: val_loss did not improve from 0.87884\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9387 - accuracy: 0.6231 - val_loss: 0.8919 - val_accuracy: 0.6388\n",
      "Epoch 53/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.9323 - accuracy: 0.6212\n",
      "Epoch 53: val_loss did not improve from 0.87884\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9322 - accuracy: 0.6213 - val_loss: 0.8864 - val_accuracy: 0.6392\n",
      "Epoch 54/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.9295 - accuracy: 0.6233\n",
      "Epoch 54: val_loss did not improve from 0.87884\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9294 - accuracy: 0.6234 - val_loss: 0.9046 - val_accuracy: 0.6348\n",
      "Epoch 55/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.9324 - accuracy: 0.6232\n",
      "Epoch 55: val_loss did not improve from 0.87884\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9322 - accuracy: 0.6232 - val_loss: 0.9112 - val_accuracy: 0.6329\n",
      "Epoch 56/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.9286 - accuracy: 0.6252\n",
      "Epoch 56: val_loss did not improve from 0.87884\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9288 - accuracy: 0.6250 - val_loss: 0.9152 - val_accuracy: 0.6369\n",
      "Epoch 57/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.9277 - accuracy: 0.6246\n",
      "Epoch 57: val_loss did not improve from 0.87884\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9276 - accuracy: 0.6246 - val_loss: 0.9016 - val_accuracy: 0.6425\n",
      "Epoch 58/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.9246 - accuracy: 0.6284\n",
      "Epoch 58: val_loss did not improve from 0.87884\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9246 - accuracy: 0.6284 - val_loss: 0.8980 - val_accuracy: 0.6423\n",
      "Epoch 59/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.9276 - accuracy: 0.6246\n",
      "Epoch 59: val_loss did not improve from 0.87884\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9275 - accuracy: 0.6246 - val_loss: 0.9084 - val_accuracy: 0.6349\n",
      "Epoch 60/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.9237 - accuracy: 0.6273\n",
      "Epoch 60: val_loss improved from 0.87884 to 0.87511, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 10s 13ms/step - loss: 0.9242 - accuracy: 0.6270 - val_loss: 0.8751 - val_accuracy: 0.6471\n",
      "Epoch 61/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.9171 - accuracy: 0.6302\n",
      "Epoch 61: val_loss improved from 0.87511 to 0.86076, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9173 - accuracy: 0.6302 - val_loss: 0.8608 - val_accuracy: 0.6580\n",
      "Epoch 62/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.9148 - accuracy: 0.6307\n",
      "Epoch 62: val_loss improved from 0.86076 to 0.85386, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9148 - accuracy: 0.6307 - val_loss: 0.8539 - val_accuracy: 0.6470\n",
      "Epoch 63/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.9117 - accuracy: 0.6325\n",
      "Epoch 63: val_loss did not improve from 0.85386\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9114 - accuracy: 0.6327 - val_loss: 0.8636 - val_accuracy: 0.6518\n",
      "Epoch 64/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.9161 - accuracy: 0.6306\n",
      "Epoch 64: val_loss did not improve from 0.85386\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9162 - accuracy: 0.6306 - val_loss: 0.8653 - val_accuracy: 0.6483\n",
      "Epoch 65/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.9113 - accuracy: 0.6312\n",
      "Epoch 65: val_loss did not improve from 0.85386\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9112 - accuracy: 0.6312 - val_loss: 0.8606 - val_accuracy: 0.6531\n",
      "Epoch 66/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.9114 - accuracy: 0.6348\n",
      "Epoch 66: val_loss did not improve from 0.85386\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9115 - accuracy: 0.6347 - val_loss: 0.9178 - val_accuracy: 0.6413\n",
      "Epoch 67/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.9106 - accuracy: 0.6309\n",
      "Epoch 67: val_loss did not improve from 0.85386\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9105 - accuracy: 0.6309 - val_loss: 0.8553 - val_accuracy: 0.6606\n",
      "Epoch 68/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.9062 - accuracy: 0.6352\n",
      "Epoch 68: val_loss improved from 0.85386 to 0.83477, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9064 - accuracy: 0.6352 - val_loss: 0.8348 - val_accuracy: 0.6688\n",
      "Epoch 69/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.9016 - accuracy: 0.6362\n",
      "Epoch 69: val_loss did not improve from 0.83477\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9016 - accuracy: 0.6362 - val_loss: 0.8798 - val_accuracy: 0.6473\n",
      "Epoch 70/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.9102 - accuracy: 0.6333\n",
      "Epoch 70: val_loss did not improve from 0.83477\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9103 - accuracy: 0.6332 - val_loss: 0.8781 - val_accuracy: 0.6528\n",
      "Epoch 71/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.9027 - accuracy: 0.6345\n",
      "Epoch 71: val_loss did not improve from 0.83477\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9028 - accuracy: 0.6345 - val_loss: 0.8416 - val_accuracy: 0.6640\n",
      "Epoch 72/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.9015 - accuracy: 0.6369\n",
      "Epoch 72: val_loss did not improve from 0.83477\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9015 - accuracy: 0.6369 - val_loss: 0.8734 - val_accuracy: 0.6502\n",
      "Epoch 73/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.8970 - accuracy: 0.6402\n",
      "Epoch 73: val_loss did not improve from 0.83477\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8970 - accuracy: 0.6402 - val_loss: 0.8484 - val_accuracy: 0.6636\n",
      "Epoch 74/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.9024 - accuracy: 0.6386\n",
      "Epoch 74: val_loss improved from 0.83477 to 0.83103, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 10s 13ms/step - loss: 0.9023 - accuracy: 0.6387 - val_loss: 0.8310 - val_accuracy: 0.6632\n",
      "Epoch 75/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8982 - accuracy: 0.6389\n",
      "Epoch 75: val_loss improved from 0.83103 to 0.82604, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8981 - accuracy: 0.6389 - val_loss: 0.8260 - val_accuracy: 0.6551\n",
      "Epoch 76/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8881 - accuracy: 0.6431\n",
      "Epoch 76: val_loss did not improve from 0.82604\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8881 - accuracy: 0.6431 - val_loss: 0.8275 - val_accuracy: 0.6685\n",
      "Epoch 77/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8883 - accuracy: 0.6415\n",
      "Epoch 77: val_loss did not improve from 0.82604\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8886 - accuracy: 0.6415 - val_loss: 0.8432 - val_accuracy: 0.6601\n",
      "Epoch 78/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8920 - accuracy: 0.6425\n",
      "Epoch 78: val_loss did not improve from 0.82604\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8922 - accuracy: 0.6425 - val_loss: 0.8653 - val_accuracy: 0.6560\n",
      "Epoch 79/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.8883 - accuracy: 0.6438\n",
      "Epoch 79: val_loss did not improve from 0.82604\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8883 - accuracy: 0.6438 - val_loss: 0.8481 - val_accuracy: 0.6596\n",
      "Epoch 80/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.8980 - accuracy: 0.6407\n",
      "Epoch 80: val_loss did not improve from 0.82604\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8979 - accuracy: 0.6407 - val_loss: 0.8325 - val_accuracy: 0.6674\n",
      "Epoch 81/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8853 - accuracy: 0.6447\n",
      "Epoch 81: val_loss did not improve from 0.82604\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8853 - accuracy: 0.6447 - val_loss: 0.8479 - val_accuracy: 0.6600\n",
      "Epoch 82/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.8818 - accuracy: 0.6459\n",
      "Epoch 82: val_loss did not improve from 0.82604\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8819 - accuracy: 0.6459 - val_loss: 0.8390 - val_accuracy: 0.6698\n",
      "Epoch 83/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8893 - accuracy: 0.6435\n",
      "Epoch 83: val_loss did not improve from 0.82604\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8893 - accuracy: 0.6434 - val_loss: 0.8491 - val_accuracy: 0.6660\n",
      "Epoch 84/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.8839 - accuracy: 0.6480\n",
      "Epoch 84: val_loss did not improve from 0.82604\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8839 - accuracy: 0.6480 - val_loss: 0.8681 - val_accuracy: 0.6625\n",
      "Epoch 85/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.8829 - accuracy: 0.6483\n",
      "Epoch 85: val_loss did not improve from 0.82604\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8829 - accuracy: 0.6483 - val_loss: 0.8355 - val_accuracy: 0.6734\n",
      "Epoch 86/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.8792 - accuracy: 0.6487\n",
      "Epoch 86: val_loss did not improve from 0.82604\n",
      "762/762 [==============================] - 10s 13ms/step - loss: 0.8794 - accuracy: 0.6487 - val_loss: 0.8312 - val_accuracy: 0.6646\n",
      "Epoch 87/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8742 - accuracy: 0.6521\n",
      "Epoch 87: val_loss improved from 0.82604 to 0.80239, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8743 - accuracy: 0.6521 - val_loss: 0.8024 - val_accuracy: 0.6787\n",
      "Epoch 88/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.8778 - accuracy: 0.6503\n",
      "Epoch 88: val_loss did not improve from 0.80239\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8778 - accuracy: 0.6503 - val_loss: 0.8049 - val_accuracy: 0.6781\n",
      "Epoch 89/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.8827 - accuracy: 0.6485\n",
      "Epoch 89: val_loss did not improve from 0.80239\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8827 - accuracy: 0.6485 - val_loss: 0.8267 - val_accuracy: 0.6715\n",
      "Epoch 90/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8854 - accuracy: 0.6505\n",
      "Epoch 90: val_loss did not improve from 0.80239\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8856 - accuracy: 0.6504 - val_loss: 0.8257 - val_accuracy: 0.6681\n",
      "Epoch 91/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8744 - accuracy: 0.6525\n",
      "Epoch 91: val_loss improved from 0.80239 to 0.78995, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8743 - accuracy: 0.6525 - val_loss: 0.7899 - val_accuracy: 0.6896\n",
      "Epoch 92/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.8785 - accuracy: 0.6510\n",
      "Epoch 92: val_loss did not improve from 0.78995\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8785 - accuracy: 0.6510 - val_loss: 0.8232 - val_accuracy: 0.6755\n",
      "Epoch 93/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.8654 - accuracy: 0.6571\n",
      "Epoch 93: val_loss improved from 0.78995 to 0.78404, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8655 - accuracy: 0.6570 - val_loss: 0.7840 - val_accuracy: 0.6878\n",
      "Epoch 94/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8694 - accuracy: 0.6541\n",
      "Epoch 94: val_loss did not improve from 0.78404\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8697 - accuracy: 0.6541 - val_loss: 0.7945 - val_accuracy: 0.6838\n",
      "Epoch 95/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8694 - accuracy: 0.6565\n",
      "Epoch 95: val_loss did not improve from 0.78404\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8694 - accuracy: 0.6565 - val_loss: 0.8300 - val_accuracy: 0.6776\n",
      "Epoch 96/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8583 - accuracy: 0.6595\n",
      "Epoch 96: val_loss did not improve from 0.78404\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8583 - accuracy: 0.6596 - val_loss: 0.7884 - val_accuracy: 0.6971\n",
      "Epoch 97/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.8742 - accuracy: 0.6528\n",
      "Epoch 97: val_loss did not improve from 0.78404\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8744 - accuracy: 0.6528 - val_loss: 0.8224 - val_accuracy: 0.6789\n",
      "Epoch 98/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8643 - accuracy: 0.6560\n",
      "Epoch 98: val_loss did not improve from 0.78404\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8644 - accuracy: 0.6560 - val_loss: 0.8073 - val_accuracy: 0.6810\n",
      "Epoch 99/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.8572 - accuracy: 0.6613\n",
      "Epoch 99: val_loss did not improve from 0.78404\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8574 - accuracy: 0.6613 - val_loss: 0.8205 - val_accuracy: 0.6784\n",
      "Epoch 100/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.8578 - accuracy: 0.6589\n",
      "Epoch 100: val_loss did not improve from 0.78404\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8577 - accuracy: 0.6589 - val_loss: 0.8268 - val_accuracy: 0.6845\n",
      "Epoch 101/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8581 - accuracy: 0.6612\n",
      "Epoch 101: val_loss improved from 0.78404 to 0.77828, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 10s 12ms/step - loss: 0.8579 - accuracy: 0.6613 - val_loss: 0.7783 - val_accuracy: 0.6873\n",
      "Epoch 102/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8563 - accuracy: 0.6618\n",
      "Epoch 102: val_loss did not improve from 0.77828\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8563 - accuracy: 0.6619 - val_loss: 0.7818 - val_accuracy: 0.6915\n",
      "Epoch 103/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8615 - accuracy: 0.6585\n",
      "Epoch 103: val_loss did not improve from 0.77828\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8613 - accuracy: 0.6585 - val_loss: 0.7930 - val_accuracy: 0.6932\n",
      "Epoch 104/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.8465 - accuracy: 0.6624\n",
      "Epoch 104: val_loss did not improve from 0.77828\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8465 - accuracy: 0.6624 - val_loss: 0.7821 - val_accuracy: 0.6909\n",
      "Epoch 105/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.8541 - accuracy: 0.6598\n",
      "Epoch 105: val_loss did not improve from 0.77828\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8543 - accuracy: 0.6598 - val_loss: 0.8155 - val_accuracy: 0.6823\n",
      "Epoch 106/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8514 - accuracy: 0.6640\n",
      "Epoch 106: val_loss did not improve from 0.77828\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8516 - accuracy: 0.6640 - val_loss: 0.7928 - val_accuracy: 0.6916\n",
      "Epoch 107/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8499 - accuracy: 0.6646\n",
      "Epoch 107: val_loss improved from 0.77828 to 0.76078, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8499 - accuracy: 0.6647 - val_loss: 0.7608 - val_accuracy: 0.7015\n",
      "Epoch 108/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8410 - accuracy: 0.6681\n",
      "Epoch 108: val_loss did not improve from 0.76078\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8409 - accuracy: 0.6681 - val_loss: 0.7949 - val_accuracy: 0.6890\n",
      "Epoch 109/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8485 - accuracy: 0.6664\n",
      "Epoch 109: val_loss did not improve from 0.76078\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8485 - accuracy: 0.6664 - val_loss: 0.8044 - val_accuracy: 0.6890\n",
      "Epoch 110/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8499 - accuracy: 0.6635\n",
      "Epoch 110: val_loss did not improve from 0.76078\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8498 - accuracy: 0.6636 - val_loss: 0.7682 - val_accuracy: 0.7008\n",
      "Epoch 111/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.8445 - accuracy: 0.6657\n",
      "Epoch 111: val_loss did not improve from 0.76078\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8447 - accuracy: 0.6654 - val_loss: 0.7903 - val_accuracy: 0.6917\n",
      "Epoch 112/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.8434 - accuracy: 0.6675\n",
      "Epoch 112: val_loss did not improve from 0.76078\n",
      "762/762 [==============================] - 10s 12ms/step - loss: 0.8435 - accuracy: 0.6675 - val_loss: 0.8163 - val_accuracy: 0.6853\n",
      "Epoch 113/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.8477 - accuracy: 0.6674\n",
      "Epoch 113: val_loss improved from 0.76078 to 0.73458, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8477 - accuracy: 0.6674 - val_loss: 0.7346 - val_accuracy: 0.7142\n",
      "Epoch 114/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.8434 - accuracy: 0.6678\n",
      "Epoch 114: val_loss did not improve from 0.73458\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8434 - accuracy: 0.6678 - val_loss: 0.7800 - val_accuracy: 0.6993\n",
      "Epoch 115/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8390 - accuracy: 0.6674\n",
      "Epoch 115: val_loss did not improve from 0.73458\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8389 - accuracy: 0.6675 - val_loss: 0.7733 - val_accuracy: 0.6971\n",
      "Epoch 116/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8397 - accuracy: 0.6696\n",
      "Epoch 116: val_loss did not improve from 0.73458\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8396 - accuracy: 0.6696 - val_loss: 0.7842 - val_accuracy: 0.6888\n",
      "Epoch 117/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8312 - accuracy: 0.6733\n",
      "Epoch 117: val_loss did not improve from 0.73458\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8313 - accuracy: 0.6733 - val_loss: 0.7459 - val_accuracy: 0.7058\n",
      "Epoch 118/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.8316 - accuracy: 0.6732\n",
      "Epoch 118: val_loss did not improve from 0.73458\n",
      "762/762 [==============================] - 10s 12ms/step - loss: 0.8316 - accuracy: 0.6732 - val_loss: 0.7757 - val_accuracy: 0.7048\n",
      "Epoch 119/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.8328 - accuracy: 0.6720\n",
      "Epoch 119: val_loss did not improve from 0.73458\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8329 - accuracy: 0.6720 - val_loss: 0.7609 - val_accuracy: 0.7106\n",
      "Epoch 120/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.8370 - accuracy: 0.6714\n",
      "Epoch 120: val_loss did not improve from 0.73458\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8370 - accuracy: 0.6714 - val_loss: 0.7488 - val_accuracy: 0.7061\n",
      "Epoch 121/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8259 - accuracy: 0.6747\n",
      "Epoch 121: val_loss did not improve from 0.73458\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8261 - accuracy: 0.6746 - val_loss: 0.7601 - val_accuracy: 0.7059\n",
      "Epoch 122/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.8238 - accuracy: 0.6759\n",
      "Epoch 122: val_loss did not improve from 0.73458\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8235 - accuracy: 0.6760 - val_loss: 0.7815 - val_accuracy: 0.6988\n",
      "Epoch 123/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8297 - accuracy: 0.6719\n",
      "Epoch 123: val_loss did not improve from 0.73458\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8296 - accuracy: 0.6720 - val_loss: 0.7548 - val_accuracy: 0.7012\n",
      "Epoch 124/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.8255 - accuracy: 0.6758\n",
      "Epoch 124: val_loss improved from 0.73458 to 0.73069, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 10s 12ms/step - loss: 0.8253 - accuracy: 0.6758 - val_loss: 0.7307 - val_accuracy: 0.7193\n",
      "Epoch 125/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8359 - accuracy: 0.6744\n",
      "Epoch 125: val_loss did not improve from 0.73069\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8359 - accuracy: 0.6744 - val_loss: 0.7438 - val_accuracy: 0.7143\n",
      "Epoch 126/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.8217 - accuracy: 0.6785\n",
      "Epoch 126: val_loss did not improve from 0.73069\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8217 - accuracy: 0.6785 - val_loss: 0.8014 - val_accuracy: 0.6942\n",
      "Epoch 127/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8204 - accuracy: 0.6792\n",
      "Epoch 127: val_loss did not improve from 0.73069\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8207 - accuracy: 0.6791 - val_loss: 0.7342 - val_accuracy: 0.7112\n",
      "Epoch 128/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8249 - accuracy: 0.6758\n",
      "Epoch 128: val_loss did not improve from 0.73069\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8248 - accuracy: 0.6759 - val_loss: 0.7518 - val_accuracy: 0.7126\n",
      "Epoch 129/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.8249 - accuracy: 0.6780\n",
      "Epoch 129: val_loss did not improve from 0.73069\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8253 - accuracy: 0.6779 - val_loss: 0.7522 - val_accuracy: 0.7155\n",
      "Epoch 130/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8104 - accuracy: 0.6821\n",
      "Epoch 130: val_loss improved from 0.73069 to 0.72075, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8104 - accuracy: 0.6821 - val_loss: 0.7208 - val_accuracy: 0.7297\n",
      "Epoch 131/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.8283 - accuracy: 0.6776\n",
      "Epoch 131: val_loss did not improve from 0.72075\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8281 - accuracy: 0.6776 - val_loss: 0.7541 - val_accuracy: 0.7131\n",
      "Epoch 132/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8194 - accuracy: 0.6801\n",
      "Epoch 132: val_loss did not improve from 0.72075\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8194 - accuracy: 0.6801 - val_loss: 0.7336 - val_accuracy: 0.7159\n",
      "Epoch 133/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8229 - accuracy: 0.6787\n",
      "Epoch 133: val_loss improved from 0.72075 to 0.71274, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 10s 12ms/step - loss: 0.8228 - accuracy: 0.6787 - val_loss: 0.7127 - val_accuracy: 0.7195\n",
      "Epoch 134/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8129 - accuracy: 0.6834\n",
      "Epoch 134: val_loss did not improve from 0.71274\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8128 - accuracy: 0.6834 - val_loss: 0.8010 - val_accuracy: 0.6971\n",
      "Epoch 135/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8170 - accuracy: 0.6842\n",
      "Epoch 135: val_loss did not improve from 0.71274\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8171 - accuracy: 0.6842 - val_loss: 0.7430 - val_accuracy: 0.7119\n",
      "Epoch 136/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8049 - accuracy: 0.6859\n",
      "Epoch 136: val_loss did not improve from 0.71274\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8051 - accuracy: 0.6858 - val_loss: 0.7647 - val_accuracy: 0.7089\n",
      "Epoch 137/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.8093 - accuracy: 0.6833\n",
      "Epoch 137: val_loss did not improve from 0.71274\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8093 - accuracy: 0.6833 - val_loss: 0.7310 - val_accuracy: 0.7150\n",
      "Epoch 138/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.8118 - accuracy: 0.6840\n",
      "Epoch 138: val_loss did not improve from 0.71274\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8121 - accuracy: 0.6839 - val_loss: 0.7337 - val_accuracy: 0.7219\n",
      "Epoch 139/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8138 - accuracy: 0.6836\n",
      "Epoch 139: val_loss improved from 0.71274 to 0.71228, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8138 - accuracy: 0.6836 - val_loss: 0.7123 - val_accuracy: 0.7276\n",
      "Epoch 140/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.8143 - accuracy: 0.6839\n",
      "Epoch 140: val_loss did not improve from 0.71228\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8143 - accuracy: 0.6839 - val_loss: 0.7177 - val_accuracy: 0.7216\n",
      "Epoch 141/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.8086 - accuracy: 0.6838\n",
      "Epoch 141: val_loss did not improve from 0.71228\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8082 - accuracy: 0.6841 - val_loss: 0.7490 - val_accuracy: 0.7130\n",
      "Epoch 142/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8106 - accuracy: 0.6861\n",
      "Epoch 142: val_loss did not improve from 0.71228\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8104 - accuracy: 0.6862 - val_loss: 0.7167 - val_accuracy: 0.7326\n",
      "Epoch 143/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.8104 - accuracy: 0.6846\n",
      "Epoch 143: val_loss did not improve from 0.71228\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8104 - accuracy: 0.6847 - val_loss: 0.7208 - val_accuracy: 0.7271\n",
      "Epoch 144/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7976 - accuracy: 0.6885\n",
      "Epoch 144: val_loss did not improve from 0.71228\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7975 - accuracy: 0.6885 - val_loss: 0.7306 - val_accuracy: 0.7259\n",
      "Epoch 145/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.8034 - accuracy: 0.6866\n",
      "Epoch 145: val_loss did not improve from 0.71228\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8038 - accuracy: 0.6864 - val_loss: 0.7310 - val_accuracy: 0.7234\n",
      "Epoch 146/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.8020 - accuracy: 0.6879\n",
      "Epoch 146: val_loss did not improve from 0.71228\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8020 - accuracy: 0.6879 - val_loss: 0.7240 - val_accuracy: 0.7291\n",
      "Epoch 147/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.8074 - accuracy: 0.6857\n",
      "Epoch 147: val_loss did not improve from 0.71228\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8072 - accuracy: 0.6858 - val_loss: 0.7298 - val_accuracy: 0.7203\n",
      "Epoch 148/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7989 - accuracy: 0.6884\n",
      "Epoch 148: val_loss did not improve from 0.71228\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7993 - accuracy: 0.6883 - val_loss: 0.7839 - val_accuracy: 0.7032\n",
      "Epoch 149/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7991 - accuracy: 0.6871\n",
      "Epoch 149: val_loss did not improve from 0.71228\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7989 - accuracy: 0.6871 - val_loss: 0.7278 - val_accuracy: 0.7236\n",
      "Epoch 150/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.8017 - accuracy: 0.6880\n",
      "Epoch 150: val_loss did not improve from 0.71228\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8016 - accuracy: 0.6882 - val_loss: 0.7329 - val_accuracy: 0.7181\n",
      "Epoch 151/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7960 - accuracy: 0.6906\n",
      "Epoch 151: val_loss did not improve from 0.71228\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7961 - accuracy: 0.6905 - val_loss: 0.7928 - val_accuracy: 0.6961\n",
      "Epoch 152/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.7927 - accuracy: 0.6925\n",
      "Epoch 152: val_loss improved from 0.71228 to 0.70537, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 10s 12ms/step - loss: 0.7926 - accuracy: 0.6925 - val_loss: 0.7054 - val_accuracy: 0.7273\n",
      "Epoch 153/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7883 - accuracy: 0.6943\n",
      "Epoch 153: val_loss improved from 0.70537 to 0.69892, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7884 - accuracy: 0.6943 - val_loss: 0.6989 - val_accuracy: 0.7348\n",
      "Epoch 154/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7938 - accuracy: 0.6927\n",
      "Epoch 154: val_loss improved from 0.69892 to 0.68675, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 10s 12ms/step - loss: 0.7937 - accuracy: 0.6927 - val_loss: 0.6868 - val_accuracy: 0.7348\n",
      "Epoch 155/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7814 - accuracy: 0.6976\n",
      "Epoch 155: val_loss did not improve from 0.68675\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7814 - accuracy: 0.6977 - val_loss: 0.7077 - val_accuracy: 0.7248\n",
      "Epoch 156/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.7930 - accuracy: 0.6912\n",
      "Epoch 156: val_loss did not improve from 0.68675\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7930 - accuracy: 0.6913 - val_loss: 0.7536 - val_accuracy: 0.7210\n",
      "Epoch 157/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7844 - accuracy: 0.6950\n",
      "Epoch 157: val_loss did not improve from 0.68675\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7844 - accuracy: 0.6950 - val_loss: 0.7051 - val_accuracy: 0.7345\n",
      "Epoch 158/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7926 - accuracy: 0.6949\n",
      "Epoch 158: val_loss did not improve from 0.68675\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7927 - accuracy: 0.6949 - val_loss: 0.7214 - val_accuracy: 0.7293\n",
      "Epoch 159/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7949 - accuracy: 0.6925\n",
      "Epoch 159: val_loss improved from 0.68675 to 0.68637, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7949 - accuracy: 0.6925 - val_loss: 0.6864 - val_accuracy: 0.7359\n",
      "Epoch 160/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7818 - accuracy: 0.6944\n",
      "Epoch 160: val_loss did not improve from 0.68637\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7818 - accuracy: 0.6944 - val_loss: 0.7375 - val_accuracy: 0.7232\n",
      "Epoch 161/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7902 - accuracy: 0.6947\n",
      "Epoch 161: val_loss did not improve from 0.68637\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7902 - accuracy: 0.6946 - val_loss: 0.7053 - val_accuracy: 0.7303\n",
      "Epoch 162/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7928 - accuracy: 0.6914\n",
      "Epoch 162: val_loss improved from 0.68637 to 0.68421, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7927 - accuracy: 0.6914 - val_loss: 0.6842 - val_accuracy: 0.7456\n",
      "Epoch 163/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7784 - accuracy: 0.6982\n",
      "Epoch 163: val_loss did not improve from 0.68421\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7784 - accuracy: 0.6983 - val_loss: 0.6924 - val_accuracy: 0.7337\n",
      "Epoch 164/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.7876 - accuracy: 0.6946\n",
      "Epoch 164: val_loss did not improve from 0.68421\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7875 - accuracy: 0.6948 - val_loss: 0.6928 - val_accuracy: 0.7413\n",
      "Epoch 165/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7826 - accuracy: 0.6960\n",
      "Epoch 165: val_loss did not improve from 0.68421\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7826 - accuracy: 0.6960 - val_loss: 0.7825 - val_accuracy: 0.7149\n",
      "Epoch 166/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7770 - accuracy: 0.6971\n",
      "Epoch 166: val_loss did not improve from 0.68421\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7770 - accuracy: 0.6971 - val_loss: 0.6910 - val_accuracy: 0.7355\n",
      "Epoch 167/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.7805 - accuracy: 0.6999\n",
      "Epoch 167: val_loss did not improve from 0.68421\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7805 - accuracy: 0.6999 - val_loss: 0.6847 - val_accuracy: 0.7424\n",
      "Epoch 168/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7761 - accuracy: 0.7000\n",
      "Epoch 168: val_loss did not improve from 0.68421\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7762 - accuracy: 0.7000 - val_loss: 0.6965 - val_accuracy: 0.7419\n",
      "Epoch 169/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.7782 - accuracy: 0.6994\n",
      "Epoch 169: val_loss did not improve from 0.68421\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7779 - accuracy: 0.6995 - val_loss: 0.6959 - val_accuracy: 0.7392\n",
      "Epoch 170/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7840 - accuracy: 0.6991\n",
      "Epoch 170: val_loss did not improve from 0.68421\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7840 - accuracy: 0.6990 - val_loss: 0.7112 - val_accuracy: 0.7343\n",
      "Epoch 171/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7738 - accuracy: 0.7022\n",
      "Epoch 171: val_loss did not improve from 0.68421\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7736 - accuracy: 0.7023 - val_loss: 0.6900 - val_accuracy: 0.7368\n",
      "Epoch 172/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7774 - accuracy: 0.6988\n",
      "Epoch 172: val_loss did not improve from 0.68421\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7772 - accuracy: 0.6990 - val_loss: 0.7096 - val_accuracy: 0.7348\n",
      "Epoch 173/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7758 - accuracy: 0.7024\n",
      "Epoch 173: val_loss improved from 0.68421 to 0.68302, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7763 - accuracy: 0.7022 - val_loss: 0.6830 - val_accuracy: 0.7377\n",
      "Epoch 174/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7744 - accuracy: 0.7010\n",
      "Epoch 174: val_loss improved from 0.68302 to 0.66785, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7743 - accuracy: 0.7010 - val_loss: 0.6678 - val_accuracy: 0.7489\n",
      "Epoch 175/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.7704 - accuracy: 0.7020\n",
      "Epoch 175: val_loss did not improve from 0.66785\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7703 - accuracy: 0.7021 - val_loss: 0.7175 - val_accuracy: 0.7371\n",
      "Epoch 176/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7700 - accuracy: 0.7022\n",
      "Epoch 176: val_loss improved from 0.66785 to 0.66251, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7699 - accuracy: 0.7022 - val_loss: 0.6625 - val_accuracy: 0.7457\n",
      "Epoch 177/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.8039 - accuracy: 0.6929\n",
      "Epoch 177: val_loss did not improve from 0.66251\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8039 - accuracy: 0.6929 - val_loss: 0.7035 - val_accuracy: 0.7400\n",
      "Epoch 178/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7682 - accuracy: 0.7031\n",
      "Epoch 178: val_loss improved from 0.66251 to 0.65948, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7683 - accuracy: 0.7031 - val_loss: 0.6595 - val_accuracy: 0.7523\n",
      "Epoch 179/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7677 - accuracy: 0.7056\n",
      "Epoch 179: val_loss improved from 0.65948 to 0.65320, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7676 - accuracy: 0.7057 - val_loss: 0.6532 - val_accuracy: 0.7484\n",
      "Epoch 180/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7724 - accuracy: 0.7042\n",
      "Epoch 180: val_loss did not improve from 0.65320\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7725 - accuracy: 0.7042 - val_loss: 0.6696 - val_accuracy: 0.7502\n",
      "Epoch 181/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7664 - accuracy: 0.7057\n",
      "Epoch 181: val_loss did not improve from 0.65320\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7665 - accuracy: 0.7057 - val_loss: 0.6911 - val_accuracy: 0.7394\n",
      "Epoch 182/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.7666 - accuracy: 0.7043\n",
      "Epoch 182: val_loss did not improve from 0.65320\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7668 - accuracy: 0.7043 - val_loss: 0.7036 - val_accuracy: 0.7370\n",
      "Epoch 183/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7978 - accuracy: 0.6996\n",
      "Epoch 183: val_loss did not improve from 0.65320\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7979 - accuracy: 0.6996 - val_loss: 0.6673 - val_accuracy: 0.7488\n",
      "Epoch 184/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7671 - accuracy: 0.7060\n",
      "Epoch 184: val_loss did not improve from 0.65320\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7672 - accuracy: 0.7060 - val_loss: 0.6768 - val_accuracy: 0.7508\n",
      "Epoch 185/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7672 - accuracy: 0.7047\n",
      "Epoch 185: val_loss did not improve from 0.65320\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7672 - accuracy: 0.7048 - val_loss: 0.6720 - val_accuracy: 0.7481\n",
      "Epoch 186/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7589 - accuracy: 0.7092\n",
      "Epoch 186: val_loss did not improve from 0.65320\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7593 - accuracy: 0.7091 - val_loss: 0.6992 - val_accuracy: 0.7315\n",
      "Epoch 187/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7611 - accuracy: 0.7068\n",
      "Epoch 187: val_loss did not improve from 0.65320\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7613 - accuracy: 0.7068 - val_loss: 0.6599 - val_accuracy: 0.7463\n",
      "Epoch 188/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.7647 - accuracy: 0.7062\n",
      "Epoch 188: val_loss did not improve from 0.65320\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7649 - accuracy: 0.7061 - val_loss: 0.6666 - val_accuracy: 0.7463\n",
      "Epoch 189/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7572 - accuracy: 0.7089\n",
      "Epoch 189: val_loss did not improve from 0.65320\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7577 - accuracy: 0.7088 - val_loss: 0.6611 - val_accuracy: 0.7536\n",
      "Epoch 190/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.7666 - accuracy: 0.7057\n",
      "Epoch 190: val_loss did not improve from 0.65320\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7667 - accuracy: 0.7057 - val_loss: 0.6692 - val_accuracy: 0.7528\n",
      "Epoch 191/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7585 - accuracy: 0.7108\n",
      "Epoch 191: val_loss improved from 0.65320 to 0.64679, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7584 - accuracy: 0.7108 - val_loss: 0.6468 - val_accuracy: 0.7528\n",
      "Epoch 192/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7609 - accuracy: 0.7072\n",
      "Epoch 192: val_loss did not improve from 0.64679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7609 - accuracy: 0.7073 - val_loss: 0.6818 - val_accuracy: 0.7438\n",
      "Epoch 193/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7571 - accuracy: 0.7106\n",
      "Epoch 193: val_loss did not improve from 0.64679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7571 - accuracy: 0.7106 - val_loss: 0.6629 - val_accuracy: 0.7536\n",
      "Epoch 194/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7617 - accuracy: 0.7088\n",
      "Epoch 194: val_loss did not improve from 0.64679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7617 - accuracy: 0.7088 - val_loss: 0.6601 - val_accuracy: 0.7564\n",
      "Epoch 195/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7575 - accuracy: 0.7084\n",
      "Epoch 195: val_loss did not improve from 0.64679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7575 - accuracy: 0.7084 - val_loss: 0.6796 - val_accuracy: 0.7497\n",
      "Epoch 196/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7524 - accuracy: 0.7111\n",
      "Epoch 196: val_loss improved from 0.64679 to 0.64650, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 10s 13ms/step - loss: 0.7524 - accuracy: 0.7111 - val_loss: 0.6465 - val_accuracy: 0.7524\n",
      "Epoch 197/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7466 - accuracy: 0.7134\n",
      "Epoch 197: val_loss did not improve from 0.64650\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7466 - accuracy: 0.7135 - val_loss: 0.6559 - val_accuracy: 0.7528\n",
      "Epoch 198/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7756 - accuracy: 0.7095\n",
      "Epoch 198: val_loss did not improve from 0.64650\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7756 - accuracy: 0.7095 - val_loss: 0.6673 - val_accuracy: 0.7508\n",
      "Epoch 199/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7633 - accuracy: 0.7096\n",
      "Epoch 199: val_loss did not improve from 0.64650\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7634 - accuracy: 0.7096 - val_loss: 0.6564 - val_accuracy: 0.7516\n",
      "Epoch 200/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7521 - accuracy: 0.7130\n",
      "Epoch 200: val_loss did not improve from 0.64650\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7521 - accuracy: 0.7130 - val_loss: 0.6482 - val_accuracy: 0.7561\n",
      "Epoch 201/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7582 - accuracy: 0.7109\n",
      "Epoch 201: val_loss did not improve from 0.64650\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7583 - accuracy: 0.7108 - val_loss: 0.7174 - val_accuracy: 0.7310\n",
      "Epoch 202/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7504 - accuracy: 0.7126\n",
      "Epoch 202: val_loss did not improve from 0.64650\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7504 - accuracy: 0.7126 - val_loss: 0.6646 - val_accuracy: 0.7476\n",
      "Epoch 203/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7611 - accuracy: 0.7106\n",
      "Epoch 203: val_loss did not improve from 0.64650\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7611 - accuracy: 0.7106 - val_loss: 0.7587 - val_accuracy: 0.7173\n",
      "Epoch 204/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.7447 - accuracy: 0.7158\n",
      "Epoch 204: val_loss improved from 0.64650 to 0.64075, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7443 - accuracy: 0.7160 - val_loss: 0.6407 - val_accuracy: 0.7588\n",
      "Epoch 205/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7559 - accuracy: 0.7105\n",
      "Epoch 205: val_loss did not improve from 0.64075\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7559 - accuracy: 0.7105 - val_loss: 0.6418 - val_accuracy: 0.7617\n",
      "Epoch 206/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.7508 - accuracy: 0.7124\n",
      "Epoch 206: val_loss did not improve from 0.64075\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7509 - accuracy: 0.7123 - val_loss: 0.6748 - val_accuracy: 0.7473\n",
      "Epoch 207/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.7468 - accuracy: 0.7149\n",
      "Epoch 207: val_loss did not improve from 0.64075\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7464 - accuracy: 0.7150 - val_loss: 0.6628 - val_accuracy: 0.7543\n",
      "Epoch 208/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7707 - accuracy: 0.7083\n",
      "Epoch 208: val_loss did not improve from 0.64075\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7707 - accuracy: 0.7082 - val_loss: 0.6755 - val_accuracy: 0.7469\n",
      "Epoch 209/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7483 - accuracy: 0.7153\n",
      "Epoch 209: val_loss improved from 0.64075 to 0.63213, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7483 - accuracy: 0.7153 - val_loss: 0.6321 - val_accuracy: 0.7633\n",
      "Epoch 210/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.7427 - accuracy: 0.7152\n",
      "Epoch 210: val_loss did not improve from 0.63213\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7428 - accuracy: 0.7151 - val_loss: 0.6340 - val_accuracy: 0.7684\n",
      "Epoch 211/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7442 - accuracy: 0.7144\n",
      "Epoch 211: val_loss did not improve from 0.63213\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7442 - accuracy: 0.7144 - val_loss: 0.6322 - val_accuracy: 0.7605\n",
      "Epoch 212/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7405 - accuracy: 0.7176\n",
      "Epoch 212: val_loss improved from 0.63213 to 0.62778, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7406 - accuracy: 0.7175 - val_loss: 0.6278 - val_accuracy: 0.7633\n",
      "Epoch 213/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.7548 - accuracy: 0.7129\n",
      "Epoch 213: val_loss did not improve from 0.62778\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7550 - accuracy: 0.7127 - val_loss: 0.6701 - val_accuracy: 0.7518\n",
      "Epoch 214/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7480 - accuracy: 0.7148\n",
      "Epoch 214: val_loss did not improve from 0.62778\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7480 - accuracy: 0.7148 - val_loss: 0.7057 - val_accuracy: 0.7357\n",
      "Epoch 215/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.7408 - accuracy: 0.7182\n",
      "Epoch 215: val_loss improved from 0.62778 to 0.61813, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7408 - accuracy: 0.7181 - val_loss: 0.6181 - val_accuracy: 0.7701\n",
      "Epoch 216/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.7401 - accuracy: 0.7180\n",
      "Epoch 216: val_loss did not improve from 0.61813\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7400 - accuracy: 0.7181 - val_loss: 0.6373 - val_accuracy: 0.7605\n",
      "Epoch 217/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.7301 - accuracy: 0.7226\n",
      "Epoch 217: val_loss did not improve from 0.61813\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7298 - accuracy: 0.7227 - val_loss: 0.6391 - val_accuracy: 0.7576\n",
      "Epoch 218/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7379 - accuracy: 0.7183\n",
      "Epoch 218: val_loss did not improve from 0.61813\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7380 - accuracy: 0.7182 - val_loss: 0.6498 - val_accuracy: 0.7625\n",
      "Epoch 219/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7436 - accuracy: 0.7161\n",
      "Epoch 219: val_loss did not improve from 0.61813\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7436 - accuracy: 0.7161 - val_loss: 0.6268 - val_accuracy: 0.7685\n",
      "Epoch 220/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.7324 - accuracy: 0.7227\n",
      "Epoch 220: val_loss did not improve from 0.61813\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7323 - accuracy: 0.7225 - val_loss: 0.6760 - val_accuracy: 0.7500\n",
      "Epoch 221/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.7427 - accuracy: 0.7161\n",
      "Epoch 221: val_loss did not improve from 0.61813\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7428 - accuracy: 0.7160 - val_loss: 0.6228 - val_accuracy: 0.7694\n",
      "Epoch 222/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.7338 - accuracy: 0.7217\n",
      "Epoch 222: val_loss did not improve from 0.61813\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7338 - accuracy: 0.7218 - val_loss: 0.6353 - val_accuracy: 0.7605\n",
      "Epoch 223/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7341 - accuracy: 0.7210\n",
      "Epoch 223: val_loss improved from 0.61813 to 0.61207, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7341 - accuracy: 0.7210 - val_loss: 0.6121 - val_accuracy: 0.7711\n",
      "Epoch 224/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.7381 - accuracy: 0.7192\n",
      "Epoch 224: val_loss did not improve from 0.61207\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7380 - accuracy: 0.7193 - val_loss: 0.6476 - val_accuracy: 0.7602\n",
      "Epoch 225/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.7348 - accuracy: 0.7193\n",
      "Epoch 225: val_loss did not improve from 0.61207\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7348 - accuracy: 0.7193 - val_loss: 0.6234 - val_accuracy: 0.7708\n",
      "Epoch 226/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.7399 - accuracy: 0.7179\n",
      "Epoch 226: val_loss did not improve from 0.61207\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7398 - accuracy: 0.7179 - val_loss: 0.6803 - val_accuracy: 0.7493\n",
      "Epoch 227/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.7396 - accuracy: 0.7191\n",
      "Epoch 227: val_loss did not improve from 0.61207\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7398 - accuracy: 0.7190 - val_loss: 0.6771 - val_accuracy: 0.7514\n",
      "Epoch 228/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7311 - accuracy: 0.7234\n",
      "Epoch 228: val_loss did not improve from 0.61207\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7311 - accuracy: 0.7234 - val_loss: 0.6792 - val_accuracy: 0.7472\n",
      "Epoch 229/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7329 - accuracy: 0.7218\n",
      "Epoch 229: val_loss did not improve from 0.61207\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7328 - accuracy: 0.7218 - val_loss: 0.6359 - val_accuracy: 0.7619\n",
      "Epoch 230/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7263 - accuracy: 0.7224\n",
      "Epoch 230: val_loss did not improve from 0.61207\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7263 - accuracy: 0.7224 - val_loss: 0.6288 - val_accuracy: 0.7628\n",
      "Epoch 231/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7320 - accuracy: 0.7213\n",
      "Epoch 231: val_loss did not improve from 0.61207\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7321 - accuracy: 0.7213 - val_loss: 0.6313 - val_accuracy: 0.7578\n",
      "Epoch 232/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.7256 - accuracy: 0.7237\n",
      "Epoch 232: val_loss did not improve from 0.61207\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7258 - accuracy: 0.7236 - val_loss: 0.6162 - val_accuracy: 0.7741\n",
      "Epoch 233/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.7301 - accuracy: 0.7223\n",
      "Epoch 233: val_loss did not improve from 0.61207\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7305 - accuracy: 0.7220 - val_loss: 0.6472 - val_accuracy: 0.7525\n",
      "Epoch 234/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.7309 - accuracy: 0.7235\n",
      "Epoch 234: val_loss did not improve from 0.61207\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7310 - accuracy: 0.7235 - val_loss: 0.6552 - val_accuracy: 0.7598\n",
      "Epoch 235/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.7330 - accuracy: 0.7199\n",
      "Epoch 235: val_loss improved from 0.61207 to 0.60365, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7326 - accuracy: 0.7201 - val_loss: 0.6037 - val_accuracy: 0.7780\n",
      "Epoch 236/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7398 - accuracy: 0.7208\n",
      "Epoch 236: val_loss did not improve from 0.60365\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7398 - accuracy: 0.7208 - val_loss: 0.6442 - val_accuracy: 0.7657\n",
      "Epoch 237/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7296 - accuracy: 0.7233\n",
      "Epoch 237: val_loss did not improve from 0.60365\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7296 - accuracy: 0.7233 - val_loss: 0.6546 - val_accuracy: 0.7583\n",
      "Epoch 238/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7308 - accuracy: 0.7222\n",
      "Epoch 238: val_loss did not improve from 0.60365\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7308 - accuracy: 0.7222 - val_loss: 0.7221 - val_accuracy: 0.7363\n",
      "Epoch 239/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7337 - accuracy: 0.7202\n",
      "Epoch 239: val_loss did not improve from 0.60365\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7335 - accuracy: 0.7203 - val_loss: 0.6522 - val_accuracy: 0.7582\n",
      "Epoch 240/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.7201 - accuracy: 0.7250\n",
      "Epoch 240: val_loss did not improve from 0.60365\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7203 - accuracy: 0.7249 - val_loss: 0.6197 - val_accuracy: 0.7700\n",
      "Epoch 241/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.7179 - accuracy: 0.7269\n",
      "Epoch 241: val_loss did not improve from 0.60365\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7178 - accuracy: 0.7269 - val_loss: 0.6293 - val_accuracy: 0.7630\n",
      "Epoch 242/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.7225 - accuracy: 0.7265\n",
      "Epoch 242: val_loss did not improve from 0.60365\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7226 - accuracy: 0.7264 - val_loss: 0.6185 - val_accuracy: 0.7719\n",
      "Epoch 243/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.7161 - accuracy: 0.7274\n",
      "Epoch 243: val_loss did not improve from 0.60365\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7157 - accuracy: 0.7275 - val_loss: 0.6254 - val_accuracy: 0.7653\n",
      "Epoch 244/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.7245 - accuracy: 0.7246\n",
      "Epoch 244: val_loss did not improve from 0.60365\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7244 - accuracy: 0.7247 - val_loss: 0.6372 - val_accuracy: 0.7600\n",
      "Epoch 245/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.7165 - accuracy: 0.7273\n",
      "Epoch 245: val_loss did not improve from 0.60365\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7167 - accuracy: 0.7271 - val_loss: 0.6193 - val_accuracy: 0.7717\n",
      "Epoch 246/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.7239 - accuracy: 0.7254\n",
      "Epoch 246: val_loss did not improve from 0.60365\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7240 - accuracy: 0.7255 - val_loss: 0.6420 - val_accuracy: 0.7618\n",
      "Epoch 247/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7190 - accuracy: 0.7268\n",
      "Epoch 247: val_loss did not improve from 0.60365\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7189 - accuracy: 0.7269 - val_loss: 0.6346 - val_accuracy: 0.7659\n",
      "Epoch 248/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.7185 - accuracy: 0.7266\n",
      "Epoch 248: val_loss did not improve from 0.60365\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7192 - accuracy: 0.7263 - val_loss: 0.6240 - val_accuracy: 0.7657\n",
      "Epoch 249/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7157 - accuracy: 0.7264\n",
      "Epoch 249: val_loss did not improve from 0.60365\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7155 - accuracy: 0.7265 - val_loss: 0.6317 - val_accuracy: 0.7682\n",
      "Epoch 250/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.7234 - accuracy: 0.7259\n",
      "Epoch 250: val_loss did not improve from 0.60365\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7232 - accuracy: 0.7260 - val_loss: 0.6123 - val_accuracy: 0.7705\n",
      "Epoch 251/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.7136 - accuracy: 0.7289\n",
      "Epoch 251: val_loss did not improve from 0.60365\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7135 - accuracy: 0.7290 - val_loss: 0.6336 - val_accuracy: 0.7634\n",
      "Epoch 252/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.7153 - accuracy: 0.7267\n",
      "Epoch 252: val_loss did not improve from 0.60365\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7152 - accuracy: 0.7267 - val_loss: 0.6549 - val_accuracy: 0.7591\n",
      "Epoch 253/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7141 - accuracy: 0.7308\n",
      "Epoch 253: val_loss did not improve from 0.60365\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7141 - accuracy: 0.7308 - val_loss: 0.6551 - val_accuracy: 0.7597\n",
      "Epoch 254/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7257 - accuracy: 0.7273\n",
      "Epoch 254: val_loss did not improve from 0.60365\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7257 - accuracy: 0.7273 - val_loss: 0.6137 - val_accuracy: 0.7678\n",
      "Epoch 255/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.7085 - accuracy: 0.7312\n",
      "Epoch 255: val_loss did not improve from 0.60365\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7087 - accuracy: 0.7311 - val_loss: 0.6418 - val_accuracy: 0.7659\n",
      "Epoch 256/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.7168 - accuracy: 0.7293\n",
      "Epoch 256: val_loss did not improve from 0.60365\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7165 - accuracy: 0.7294 - val_loss: 0.6256 - val_accuracy: 0.7718\n",
      "Epoch 257/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.7114 - accuracy: 0.7296\n",
      "Epoch 257: val_loss improved from 0.60365 to 0.59823, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7113 - accuracy: 0.7297 - val_loss: 0.5982 - val_accuracy: 0.7744\n",
      "Epoch 258/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.7207 - accuracy: 0.7290\n",
      "Epoch 258: val_loss did not improve from 0.59823\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7207 - accuracy: 0.7289 - val_loss: 0.6299 - val_accuracy: 0.7664\n",
      "Epoch 259/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.7253 - accuracy: 0.7261\n",
      "Epoch 259: val_loss did not improve from 0.59823\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7250 - accuracy: 0.7262 - val_loss: 0.6164 - val_accuracy: 0.7709\n",
      "Epoch 260/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.7076 - accuracy: 0.7325\n",
      "Epoch 260: val_loss did not improve from 0.59823\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7076 - accuracy: 0.7325 - val_loss: 0.6178 - val_accuracy: 0.7747\n",
      "Epoch 261/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7066 - accuracy: 0.7320\n",
      "Epoch 261: val_loss did not improve from 0.59823\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7066 - accuracy: 0.7320 - val_loss: 0.6046 - val_accuracy: 0.7710\n",
      "Epoch 262/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.7131 - accuracy: 0.7309\n",
      "Epoch 262: val_loss did not improve from 0.59823\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7133 - accuracy: 0.7307 - val_loss: 0.6215 - val_accuracy: 0.7743\n",
      "Epoch 263/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6992 - accuracy: 0.7339\n",
      "Epoch 263: val_loss did not improve from 0.59823\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6994 - accuracy: 0.7338 - val_loss: 0.6212 - val_accuracy: 0.7674\n",
      "Epoch 264/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.7120 - accuracy: 0.7313\n",
      "Epoch 264: val_loss did not improve from 0.59823\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7122 - accuracy: 0.7313 - val_loss: 0.7085 - val_accuracy: 0.7509\n",
      "Epoch 265/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.7110 - accuracy: 0.7300\n",
      "Epoch 265: val_loss improved from 0.59823 to 0.59470, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7109 - accuracy: 0.7300 - val_loss: 0.5947 - val_accuracy: 0.7729\n",
      "Epoch 266/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.7105 - accuracy: 0.7316\n",
      "Epoch 266: val_loss did not improve from 0.59470\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7106 - accuracy: 0.7315 - val_loss: 0.6186 - val_accuracy: 0.7724\n",
      "Epoch 267/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.7124 - accuracy: 0.7306\n",
      "Epoch 267: val_loss did not improve from 0.59470\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7125 - accuracy: 0.7306 - val_loss: 0.6573 - val_accuracy: 0.7663\n",
      "Epoch 268/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7119 - accuracy: 0.7328\n",
      "Epoch 268: val_loss did not improve from 0.59470\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7118 - accuracy: 0.7328 - val_loss: 0.5956 - val_accuracy: 0.7775\n",
      "Epoch 269/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7065 - accuracy: 0.7342\n",
      "Epoch 269: val_loss did not improve from 0.59470\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7065 - accuracy: 0.7342 - val_loss: 0.6024 - val_accuracy: 0.7779\n",
      "Epoch 270/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7218 - accuracy: 0.7303\n",
      "Epoch 270: val_loss did not improve from 0.59470\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7218 - accuracy: 0.7303 - val_loss: 0.6090 - val_accuracy: 0.7662\n",
      "Epoch 271/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7165 - accuracy: 0.7296\n",
      "Epoch 271: val_loss did not improve from 0.59470\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7166 - accuracy: 0.7295 - val_loss: 0.6467 - val_accuracy: 0.7559\n",
      "Epoch 272/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.7054 - accuracy: 0.7328\n",
      "Epoch 272: val_loss did not improve from 0.59470\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7055 - accuracy: 0.7328 - val_loss: 0.6253 - val_accuracy: 0.7747\n",
      "Epoch 273/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6974 - accuracy: 0.7361\n",
      "Epoch 273: val_loss improved from 0.59470 to 0.58495, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6974 - accuracy: 0.7361 - val_loss: 0.5850 - val_accuracy: 0.7793\n",
      "Epoch 274/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.7037 - accuracy: 0.7341\n",
      "Epoch 274: val_loss did not improve from 0.58495\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7038 - accuracy: 0.7341 - val_loss: 0.5888 - val_accuracy: 0.7837\n",
      "Epoch 275/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.7091 - accuracy: 0.7322\n",
      "Epoch 275: val_loss did not improve from 0.58495\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7095 - accuracy: 0.7320 - val_loss: 0.7231 - val_accuracy: 0.7455\n",
      "Epoch 276/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.7010 - accuracy: 0.7351\n",
      "Epoch 276: val_loss did not improve from 0.58495\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7014 - accuracy: 0.7348 - val_loss: 0.6026 - val_accuracy: 0.7795\n",
      "Epoch 277/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7016 - accuracy: 0.7357\n",
      "Epoch 277: val_loss did not improve from 0.58495\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7016 - accuracy: 0.7357 - val_loss: 0.6412 - val_accuracy: 0.7729\n",
      "Epoch 278/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6929 - accuracy: 0.7376\n",
      "Epoch 278: val_loss did not improve from 0.58495\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6929 - accuracy: 0.7377 - val_loss: 0.5895 - val_accuracy: 0.7858\n",
      "Epoch 279/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6942 - accuracy: 0.7387\n",
      "Epoch 279: val_loss did not improve from 0.58495\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6943 - accuracy: 0.7387 - val_loss: 0.6263 - val_accuracy: 0.7710\n",
      "Epoch 280/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7031 - accuracy: 0.7342\n",
      "Epoch 280: val_loss did not improve from 0.58495\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7033 - accuracy: 0.7341 - val_loss: 0.6064 - val_accuracy: 0.7781\n",
      "Epoch 281/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6952 - accuracy: 0.7375\n",
      "Epoch 281: val_loss did not improve from 0.58495\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6950 - accuracy: 0.7376 - val_loss: 0.6784 - val_accuracy: 0.7447\n",
      "Epoch 282/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6979 - accuracy: 0.7359\n",
      "Epoch 282: val_loss did not improve from 0.58495\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6978 - accuracy: 0.7359 - val_loss: 0.6105 - val_accuracy: 0.7728\n",
      "Epoch 283/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6999 - accuracy: 0.7357\n",
      "Epoch 283: val_loss did not improve from 0.58495\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6999 - accuracy: 0.7356 - val_loss: 0.6300 - val_accuracy: 0.7721\n",
      "Epoch 284/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6969 - accuracy: 0.7377\n",
      "Epoch 284: val_loss did not improve from 0.58495\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6976 - accuracy: 0.7374 - val_loss: 0.6118 - val_accuracy: 0.7757\n",
      "Epoch 285/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6966 - accuracy: 0.7371\n",
      "Epoch 285: val_loss did not improve from 0.58495\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6965 - accuracy: 0.7371 - val_loss: 0.5905 - val_accuracy: 0.7850\n",
      "Epoch 286/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.7138 - accuracy: 0.7301\n",
      "Epoch 286: val_loss did not improve from 0.58495\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7137 - accuracy: 0.7302 - val_loss: 0.5894 - val_accuracy: 0.7838\n",
      "Epoch 287/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6998 - accuracy: 0.7355\n",
      "Epoch 287: val_loss did not improve from 0.58495\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6998 - accuracy: 0.7356 - val_loss: 0.6220 - val_accuracy: 0.7726\n",
      "Epoch 288/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6956 - accuracy: 0.7383\n",
      "Epoch 288: val_loss did not improve from 0.58495\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6957 - accuracy: 0.7383 - val_loss: 0.6188 - val_accuracy: 0.7725\n",
      "Epoch 289/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6864 - accuracy: 0.7416\n",
      "Epoch 289: val_loss improved from 0.58495 to 0.58065, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6862 - accuracy: 0.7416 - val_loss: 0.5807 - val_accuracy: 0.7839\n",
      "Epoch 290/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6879 - accuracy: 0.7388\n",
      "Epoch 290: val_loss did not improve from 0.58065\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6879 - accuracy: 0.7388 - val_loss: 0.5974 - val_accuracy: 0.7780\n",
      "Epoch 291/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6952 - accuracy: 0.7370\n",
      "Epoch 291: val_loss did not improve from 0.58065\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6956 - accuracy: 0.7368 - val_loss: 0.5900 - val_accuracy: 0.7802\n",
      "Epoch 292/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6928 - accuracy: 0.7395\n",
      "Epoch 292: val_loss did not improve from 0.58065\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6928 - accuracy: 0.7395 - val_loss: 0.5891 - val_accuracy: 0.7821\n",
      "Epoch 293/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6858 - accuracy: 0.7422\n",
      "Epoch 293: val_loss did not improve from 0.58065\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6859 - accuracy: 0.7421 - val_loss: 0.6086 - val_accuracy: 0.7743\n",
      "Epoch 294/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6964 - accuracy: 0.7382\n",
      "Epoch 294: val_loss improved from 0.58065 to 0.56637, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6962 - accuracy: 0.7382 - val_loss: 0.5664 - val_accuracy: 0.7910\n",
      "Epoch 295/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6811 - accuracy: 0.7430\n",
      "Epoch 295: val_loss did not improve from 0.56637\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6812 - accuracy: 0.7430 - val_loss: 0.5798 - val_accuracy: 0.7833\n",
      "Epoch 296/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6906 - accuracy: 0.7392\n",
      "Epoch 296: val_loss did not improve from 0.56637\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6904 - accuracy: 0.7393 - val_loss: 0.5794 - val_accuracy: 0.7883\n",
      "Epoch 297/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6944 - accuracy: 0.7417\n",
      "Epoch 297: val_loss did not improve from 0.56637\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6944 - accuracy: 0.7417 - val_loss: 0.6324 - val_accuracy: 0.7668\n",
      "Epoch 298/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6978 - accuracy: 0.7378\n",
      "Epoch 298: val_loss did not improve from 0.56637\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6979 - accuracy: 0.7377 - val_loss: 0.6132 - val_accuracy: 0.7764\n",
      "Epoch 299/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6771 - accuracy: 0.7443\n",
      "Epoch 299: val_loss did not improve from 0.56637\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6771 - accuracy: 0.7443 - val_loss: 0.5872 - val_accuracy: 0.7841\n",
      "Epoch 300/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6902 - accuracy: 0.7403\n",
      "Epoch 300: val_loss did not improve from 0.56637\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6902 - accuracy: 0.7403 - val_loss: 0.6444 - val_accuracy: 0.7643\n",
      "Epoch 301/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6874 - accuracy: 0.7404\n",
      "Epoch 301: val_loss did not improve from 0.56637\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6876 - accuracy: 0.7403 - val_loss: 0.6648 - val_accuracy: 0.7614\n",
      "Epoch 302/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6811 - accuracy: 0.7426\n",
      "Epoch 302: val_loss did not improve from 0.56637\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6811 - accuracy: 0.7426 - val_loss: 0.5926 - val_accuracy: 0.7803\n",
      "Epoch 303/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6878 - accuracy: 0.7422\n",
      "Epoch 303: val_loss did not improve from 0.56637\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6878 - accuracy: 0.7422 - val_loss: 0.6220 - val_accuracy: 0.7714\n",
      "Epoch 304/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6803 - accuracy: 0.7423\n",
      "Epoch 304: val_loss did not improve from 0.56637\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6803 - accuracy: 0.7423 - val_loss: 0.5838 - val_accuracy: 0.7813\n",
      "Epoch 305/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6786 - accuracy: 0.7437\n",
      "Epoch 305: val_loss did not improve from 0.56637\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6792 - accuracy: 0.7436 - val_loss: 0.5903 - val_accuracy: 0.7855\n",
      "Epoch 306/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6837 - accuracy: 0.7420\n",
      "Epoch 306: val_loss did not improve from 0.56637\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6838 - accuracy: 0.7420 - val_loss: 0.6086 - val_accuracy: 0.7783\n",
      "Epoch 307/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6825 - accuracy: 0.7434\n",
      "Epoch 307: val_loss did not improve from 0.56637\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6825 - accuracy: 0.7434 - val_loss: 0.6165 - val_accuracy: 0.7747\n",
      "Epoch 308/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6842 - accuracy: 0.7424\n",
      "Epoch 308: val_loss did not improve from 0.56637\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6841 - accuracy: 0.7425 - val_loss: 0.6001 - val_accuracy: 0.7848\n",
      "Epoch 309/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6854 - accuracy: 0.7424\n",
      "Epoch 309: val_loss did not improve from 0.56637\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6854 - accuracy: 0.7422 - val_loss: 0.6137 - val_accuracy: 0.7735\n",
      "Epoch 310/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6775 - accuracy: 0.7463\n",
      "Epoch 310: val_loss did not improve from 0.56637\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6772 - accuracy: 0.7464 - val_loss: 0.5709 - val_accuracy: 0.7892\n",
      "Epoch 311/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6857 - accuracy: 0.7411\n",
      "Epoch 311: val_loss did not improve from 0.56637\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6858 - accuracy: 0.7410 - val_loss: 0.6135 - val_accuracy: 0.7749\n",
      "Epoch 312/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6880 - accuracy: 0.7405\n",
      "Epoch 312: val_loss did not improve from 0.56637\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6883 - accuracy: 0.7402 - val_loss: 0.5997 - val_accuracy: 0.7780\n",
      "Epoch 313/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6864 - accuracy: 0.7423\n",
      "Epoch 313: val_loss did not improve from 0.56637\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6868 - accuracy: 0.7423 - val_loss: 0.6070 - val_accuracy: 0.7753\n",
      "Epoch 314/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6799 - accuracy: 0.7450\n",
      "Epoch 314: val_loss did not improve from 0.56637\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6799 - accuracy: 0.7450 - val_loss: 0.5815 - val_accuracy: 0.7894\n",
      "Epoch 315/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6769 - accuracy: 0.7459\n",
      "Epoch 315: val_loss did not improve from 0.56637\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6769 - accuracy: 0.7459 - val_loss: 0.5895 - val_accuracy: 0.7839\n",
      "Epoch 316/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6738 - accuracy: 0.7464\n",
      "Epoch 316: val_loss improved from 0.56637 to 0.56572, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6739 - accuracy: 0.7464 - val_loss: 0.5657 - val_accuracy: 0.7890\n",
      "Epoch 317/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6897 - accuracy: 0.7410\n",
      "Epoch 317: val_loss did not improve from 0.56572\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6896 - accuracy: 0.7411 - val_loss: 0.5798 - val_accuracy: 0.7850\n",
      "Epoch 318/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6689 - accuracy: 0.7485\n",
      "Epoch 318: val_loss did not improve from 0.56572\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6689 - accuracy: 0.7486 - val_loss: 0.5685 - val_accuracy: 0.7873\n",
      "Epoch 319/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6744 - accuracy: 0.7439\n",
      "Epoch 319: val_loss did not improve from 0.56572\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6743 - accuracy: 0.7439 - val_loss: 0.5660 - val_accuracy: 0.7876\n",
      "Epoch 320/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6717 - accuracy: 0.7488\n",
      "Epoch 320: val_loss did not improve from 0.56572\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6715 - accuracy: 0.7488 - val_loss: 0.5798 - val_accuracy: 0.7857\n",
      "Epoch 321/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6728 - accuracy: 0.7469\n",
      "Epoch 321: val_loss did not improve from 0.56572\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6729 - accuracy: 0.7469 - val_loss: 0.5995 - val_accuracy: 0.7789\n",
      "Epoch 322/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6826 - accuracy: 0.7451\n",
      "Epoch 322: val_loss did not improve from 0.56572\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6826 - accuracy: 0.7452 - val_loss: 0.5795 - val_accuracy: 0.7856\n",
      "Epoch 323/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6811 - accuracy: 0.7445\n",
      "Epoch 323: val_loss did not improve from 0.56572\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6809 - accuracy: 0.7445 - val_loss: 0.5795 - val_accuracy: 0.7861\n",
      "Epoch 324/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6736 - accuracy: 0.7470\n",
      "Epoch 324: val_loss improved from 0.56572 to 0.56077, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6734 - accuracy: 0.7470 - val_loss: 0.5608 - val_accuracy: 0.7959\n",
      "Epoch 325/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7102 - accuracy: 0.7408\n",
      "Epoch 325: val_loss did not improve from 0.56077\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7102 - accuracy: 0.7408 - val_loss: 0.6313 - val_accuracy: 0.7733\n",
      "Epoch 326/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6768 - accuracy: 0.7463\n",
      "Epoch 326: val_loss did not improve from 0.56077\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6768 - accuracy: 0.7463 - val_loss: 0.5900 - val_accuracy: 0.7851\n",
      "Epoch 327/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6735 - accuracy: 0.7481\n",
      "Epoch 327: val_loss improved from 0.56077 to 0.55311, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6735 - accuracy: 0.7481 - val_loss: 0.5531 - val_accuracy: 0.7916\n",
      "Epoch 328/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6873 - accuracy: 0.7426\n",
      "Epoch 328: val_loss did not improve from 0.55311\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6875 - accuracy: 0.7425 - val_loss: 0.5723 - val_accuracy: 0.7912\n",
      "Epoch 329/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6722 - accuracy: 0.7486\n",
      "Epoch 329: val_loss did not improve from 0.55311\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6722 - accuracy: 0.7486 - val_loss: 0.5815 - val_accuracy: 0.7820\n",
      "Epoch 330/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6748 - accuracy: 0.7465\n",
      "Epoch 330: val_loss did not improve from 0.55311\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6754 - accuracy: 0.7463 - val_loss: 0.5629 - val_accuracy: 0.7918\n",
      "Epoch 331/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6684 - accuracy: 0.7496\n",
      "Epoch 331: val_loss did not improve from 0.55311\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6686 - accuracy: 0.7496 - val_loss: 0.6366 - val_accuracy: 0.7698\n",
      "Epoch 332/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6804 - accuracy: 0.7453\n",
      "Epoch 332: val_loss did not improve from 0.55311\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6802 - accuracy: 0.7454 - val_loss: 0.5598 - val_accuracy: 0.7919\n",
      "Epoch 333/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6767 - accuracy: 0.7461\n",
      "Epoch 333: val_loss did not improve from 0.55311\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6763 - accuracy: 0.7463 - val_loss: 0.5668 - val_accuracy: 0.7917\n",
      "Epoch 334/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6773 - accuracy: 0.7467\n",
      "Epoch 334: val_loss did not improve from 0.55311\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6771 - accuracy: 0.7468 - val_loss: 0.5620 - val_accuracy: 0.7951\n",
      "Epoch 335/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6649 - accuracy: 0.7492\n",
      "Epoch 335: val_loss did not improve from 0.55311\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6647 - accuracy: 0.7492 - val_loss: 0.5710 - val_accuracy: 0.7925\n",
      "Epoch 336/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6693 - accuracy: 0.7498\n",
      "Epoch 336: val_loss did not improve from 0.55311\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6693 - accuracy: 0.7498 - val_loss: 0.6028 - val_accuracy: 0.7784\n",
      "Epoch 337/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6776 - accuracy: 0.7467\n",
      "Epoch 337: val_loss did not improve from 0.55311\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6778 - accuracy: 0.7466 - val_loss: 0.5715 - val_accuracy: 0.7919\n",
      "Epoch 338/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6676 - accuracy: 0.7501\n",
      "Epoch 338: val_loss did not improve from 0.55311\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6680 - accuracy: 0.7499 - val_loss: 0.6087 - val_accuracy: 0.7790\n",
      "Epoch 339/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6630 - accuracy: 0.7517\n",
      "Epoch 339: val_loss did not improve from 0.55311\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6630 - accuracy: 0.7517 - val_loss: 0.5815 - val_accuracy: 0.7845\n",
      "Epoch 340/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6697 - accuracy: 0.7489\n",
      "Epoch 340: val_loss did not improve from 0.55311\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6697 - accuracy: 0.7490 - val_loss: 0.5914 - val_accuracy: 0.7843\n",
      "Epoch 341/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6784 - accuracy: 0.7470\n",
      "Epoch 341: val_loss did not improve from 0.55311\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6784 - accuracy: 0.7469 - val_loss: 0.5741 - val_accuracy: 0.7810\n",
      "Epoch 342/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6647 - accuracy: 0.7510\n",
      "Epoch 342: val_loss did not improve from 0.55311\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6648 - accuracy: 0.7510 - val_loss: 0.6031 - val_accuracy: 0.7802\n",
      "Epoch 343/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6923 - accuracy: 0.7468\n",
      "Epoch 343: val_loss did not improve from 0.55311\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6923 - accuracy: 0.7468 - val_loss: 0.5837 - val_accuracy: 0.7922\n",
      "Epoch 344/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6770 - accuracy: 0.7472\n",
      "Epoch 344: val_loss did not improve from 0.55311\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6768 - accuracy: 0.7473 - val_loss: 0.5850 - val_accuracy: 0.7856\n",
      "Epoch 345/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6733 - accuracy: 0.7474\n",
      "Epoch 345: val_loss did not improve from 0.55311\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6732 - accuracy: 0.7475 - val_loss: 0.5633 - val_accuracy: 0.7927\n",
      "Epoch 346/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6667 - accuracy: 0.7520\n",
      "Epoch 346: val_loss did not improve from 0.55311\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6670 - accuracy: 0.7519 - val_loss: 0.5975 - val_accuracy: 0.7855\n",
      "Epoch 347/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6702 - accuracy: 0.7506\n",
      "Epoch 347: val_loss did not improve from 0.55311\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6701 - accuracy: 0.7505 - val_loss: 0.5853 - val_accuracy: 0.7835\n",
      "Epoch 348/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6651 - accuracy: 0.7510\n",
      "Epoch 348: val_loss did not improve from 0.55311\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6654 - accuracy: 0.7509 - val_loss: 0.5833 - val_accuracy: 0.7896\n",
      "Epoch 349/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6619 - accuracy: 0.7521\n",
      "Epoch 349: val_loss did not improve from 0.55311\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6618 - accuracy: 0.7521 - val_loss: 0.5624 - val_accuracy: 0.7922\n",
      "Epoch 350/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6649 - accuracy: 0.7528\n",
      "Epoch 350: val_loss did not improve from 0.55311\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6645 - accuracy: 0.7529 - val_loss: 0.5573 - val_accuracy: 0.7989\n",
      "Epoch 351/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6699 - accuracy: 0.7499\n",
      "Epoch 351: val_loss did not improve from 0.55311\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6699 - accuracy: 0.7498 - val_loss: 0.6084 - val_accuracy: 0.7833\n",
      "Epoch 352/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6742 - accuracy: 0.7481\n",
      "Epoch 352: val_loss did not improve from 0.55311\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6744 - accuracy: 0.7480 - val_loss: 0.5773 - val_accuracy: 0.7857\n",
      "Epoch 353/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6702 - accuracy: 0.7513\n",
      "Epoch 353: val_loss did not improve from 0.55311\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6707 - accuracy: 0.7511 - val_loss: 0.5780 - val_accuracy: 0.7902\n",
      "Epoch 354/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6660 - accuracy: 0.7521\n",
      "Epoch 354: val_loss did not improve from 0.55311\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6658 - accuracy: 0.7522 - val_loss: 0.5861 - val_accuracy: 0.7901\n",
      "Epoch 355/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6615 - accuracy: 0.7528\n",
      "Epoch 355: val_loss did not improve from 0.55311\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6614 - accuracy: 0.7528 - val_loss: 0.6015 - val_accuracy: 0.7817\n",
      "Epoch 356/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6789 - accuracy: 0.7482\n",
      "Epoch 356: val_loss did not improve from 0.55311\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6787 - accuracy: 0.7482 - val_loss: 0.5821 - val_accuracy: 0.7895\n",
      "Epoch 357/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6668 - accuracy: 0.7505\n",
      "Epoch 357: val_loss did not improve from 0.55311\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6666 - accuracy: 0.7506 - val_loss: 0.5660 - val_accuracy: 0.7936\n",
      "Epoch 358/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6687 - accuracy: 0.7513\n",
      "Epoch 358: val_loss did not improve from 0.55311\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6683 - accuracy: 0.7514 - val_loss: 0.5605 - val_accuracy: 0.7884\n",
      "Epoch 359/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6584 - accuracy: 0.7523\n",
      "Epoch 359: val_loss did not improve from 0.55311\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6582 - accuracy: 0.7523 - val_loss: 0.5618 - val_accuracy: 0.7945\n",
      "Epoch 360/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6567 - accuracy: 0.7524\n",
      "Epoch 360: val_loss did not improve from 0.55311\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6569 - accuracy: 0.7524 - val_loss: 0.5679 - val_accuracy: 0.7919\n",
      "Epoch 361/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6628 - accuracy: 0.7538\n",
      "Epoch 361: val_loss did not improve from 0.55311\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6638 - accuracy: 0.7535 - val_loss: 0.5694 - val_accuracy: 0.7906\n",
      "Epoch 362/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6712 - accuracy: 0.7501\n",
      "Epoch 362: val_loss did not improve from 0.55311\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6714 - accuracy: 0.7500 - val_loss: 0.5864 - val_accuracy: 0.7897\n",
      "Epoch 363/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6763 - accuracy: 0.7493\n",
      "Epoch 363: val_loss did not improve from 0.55311\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6764 - accuracy: 0.7492 - val_loss: 0.5737 - val_accuracy: 0.7898\n",
      "Epoch 364/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6697 - accuracy: 0.7515\n",
      "Epoch 364: val_loss did not improve from 0.55311\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6700 - accuracy: 0.7514 - val_loss: 0.7069 - val_accuracy: 0.7512\n",
      "Epoch 365/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6557 - accuracy: 0.7554\n",
      "Epoch 365: val_loss did not improve from 0.55311\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6555 - accuracy: 0.7556 - val_loss: 0.5825 - val_accuracy: 0.7897\n",
      "Epoch 366/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6596 - accuracy: 0.7525\n",
      "Epoch 366: val_loss did not improve from 0.55311\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6596 - accuracy: 0.7525 - val_loss: 0.5872 - val_accuracy: 0.7927\n",
      "Epoch 367/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6710 - accuracy: 0.7498\n",
      "Epoch 367: val_loss did not improve from 0.55311\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6711 - accuracy: 0.7498 - val_loss: 0.5722 - val_accuracy: 0.7942\n",
      "Epoch 368/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6555 - accuracy: 0.7553\n",
      "Epoch 368: val_loss improved from 0.55311 to 0.54000, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6559 - accuracy: 0.7552 - val_loss: 0.5400 - val_accuracy: 0.8047\n",
      "Epoch 369/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6575 - accuracy: 0.7527\n",
      "Epoch 369: val_loss did not improve from 0.54000\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6572 - accuracy: 0.7527 - val_loss: 0.5813 - val_accuracy: 0.7827\n",
      "Epoch 370/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6573 - accuracy: 0.7539\n",
      "Epoch 370: val_loss did not improve from 0.54000\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6574 - accuracy: 0.7538 - val_loss: 0.5740 - val_accuracy: 0.7894\n",
      "Epoch 371/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6683 - accuracy: 0.7510\n",
      "Epoch 371: val_loss did not improve from 0.54000\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6682 - accuracy: 0.7510 - val_loss: 0.5736 - val_accuracy: 0.7972\n",
      "Epoch 372/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6492 - accuracy: 0.7572\n",
      "Epoch 372: val_loss improved from 0.54000 to 0.53624, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6492 - accuracy: 0.7571 - val_loss: 0.5362 - val_accuracy: 0.8015\n",
      "Epoch 373/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6620 - accuracy: 0.7525\n",
      "Epoch 373: val_loss did not improve from 0.53624\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6620 - accuracy: 0.7525 - val_loss: 0.5454 - val_accuracy: 0.7999\n",
      "Epoch 374/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6441 - accuracy: 0.7582\n",
      "Epoch 374: val_loss did not improve from 0.53624\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6440 - accuracy: 0.7583 - val_loss: 0.5526 - val_accuracy: 0.7995\n",
      "Epoch 375/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6614 - accuracy: 0.7528\n",
      "Epoch 375: val_loss did not improve from 0.53624\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6613 - accuracy: 0.7528 - val_loss: 0.5873 - val_accuracy: 0.7908\n",
      "Epoch 376/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6622 - accuracy: 0.7532\n",
      "Epoch 376: val_loss did not improve from 0.53624\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6623 - accuracy: 0.7532 - val_loss: 0.5706 - val_accuracy: 0.7914\n",
      "Epoch 377/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6552 - accuracy: 0.7555\n",
      "Epoch 377: val_loss did not improve from 0.53624\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6550 - accuracy: 0.7556 - val_loss: 0.5730 - val_accuracy: 0.7922\n",
      "Epoch 378/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6591 - accuracy: 0.7546\n",
      "Epoch 378: val_loss did not improve from 0.53624\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6587 - accuracy: 0.7548 - val_loss: 0.5659 - val_accuracy: 0.7899\n",
      "Epoch 379/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6528 - accuracy: 0.7567\n",
      "Epoch 379: val_loss did not improve from 0.53624\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6529 - accuracy: 0.7568 - val_loss: 0.5462 - val_accuracy: 0.7956\n",
      "Epoch 380/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6561 - accuracy: 0.7554\n",
      "Epoch 380: val_loss did not improve from 0.53624\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6561 - accuracy: 0.7554 - val_loss: 0.5726 - val_accuracy: 0.7909\n",
      "Epoch 381/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6544 - accuracy: 0.7559\n",
      "Epoch 381: val_loss did not improve from 0.53624\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6547 - accuracy: 0.7558 - val_loss: 0.5602 - val_accuracy: 0.7933\n",
      "Epoch 382/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6563 - accuracy: 0.7559\n",
      "Epoch 382: val_loss did not improve from 0.53624\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6562 - accuracy: 0.7559 - val_loss: 0.5553 - val_accuracy: 0.7967\n",
      "Epoch 383/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6499 - accuracy: 0.7568\n",
      "Epoch 383: val_loss did not improve from 0.53624\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6497 - accuracy: 0.7568 - val_loss: 0.6160 - val_accuracy: 0.7716\n",
      "Epoch 384/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6603 - accuracy: 0.7540\n",
      "Epoch 384: val_loss did not improve from 0.53624\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6601 - accuracy: 0.7542 - val_loss: 0.5795 - val_accuracy: 0.7865\n",
      "Epoch 385/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6500 - accuracy: 0.7578\n",
      "Epoch 385: val_loss did not improve from 0.53624\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6500 - accuracy: 0.7579 - val_loss: 0.5472 - val_accuracy: 0.7987\n",
      "Epoch 386/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6508 - accuracy: 0.7571\n",
      "Epoch 386: val_loss did not improve from 0.53624\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6508 - accuracy: 0.7571 - val_loss: 0.5890 - val_accuracy: 0.7865\n",
      "Epoch 387/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6566 - accuracy: 0.7554\n",
      "Epoch 387: val_loss did not improve from 0.53624\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6568 - accuracy: 0.7553 - val_loss: 0.5787 - val_accuracy: 0.7927\n",
      "Epoch 388/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6524 - accuracy: 0.7576\n",
      "Epoch 388: val_loss did not improve from 0.53624\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6524 - accuracy: 0.7576 - val_loss: 0.5602 - val_accuracy: 0.7973\n",
      "Epoch 389/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6556 - accuracy: 0.7558\n",
      "Epoch 389: val_loss did not improve from 0.53624\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6557 - accuracy: 0.7558 - val_loss: 0.5994 - val_accuracy: 0.7856\n",
      "Epoch 390/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6507 - accuracy: 0.7576\n",
      "Epoch 390: val_loss did not improve from 0.53624\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6510 - accuracy: 0.7575 - val_loss: 0.6125 - val_accuracy: 0.7829\n",
      "Epoch 391/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6487 - accuracy: 0.7573\n",
      "Epoch 391: val_loss did not improve from 0.53624\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6487 - accuracy: 0.7573 - val_loss: 0.5400 - val_accuracy: 0.8018\n",
      "Epoch 392/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6672 - accuracy: 0.7546\n",
      "Epoch 392: val_loss did not improve from 0.53624\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6668 - accuracy: 0.7547 - val_loss: 0.5721 - val_accuracy: 0.7947\n",
      "Epoch 393/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6467 - accuracy: 0.7597\n",
      "Epoch 393: val_loss did not improve from 0.53624\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6467 - accuracy: 0.7597 - val_loss: 0.5937 - val_accuracy: 0.7861\n",
      "Epoch 394/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6629 - accuracy: 0.7530\n",
      "Epoch 394: val_loss did not improve from 0.53624\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6627 - accuracy: 0.7530 - val_loss: 0.5659 - val_accuracy: 0.7899\n",
      "Epoch 395/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6629 - accuracy: 0.7548\n",
      "Epoch 395: val_loss did not improve from 0.53624\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6627 - accuracy: 0.7548 - val_loss: 0.5756 - val_accuracy: 0.7935\n",
      "Epoch 396/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6559 - accuracy: 0.7568\n",
      "Epoch 396: val_loss did not improve from 0.53624\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6559 - accuracy: 0.7568 - val_loss: 0.5637 - val_accuracy: 0.8003\n",
      "Epoch 397/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7855 - accuracy: 0.7521\n",
      "Epoch 397: val_loss did not improve from 0.53624\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7855 - accuracy: 0.7520 - val_loss: 0.5634 - val_accuracy: 0.7963\n",
      "Epoch 398/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6475 - accuracy: 0.7607\n",
      "Epoch 398: val_loss did not improve from 0.53624\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6475 - accuracy: 0.7607 - val_loss: 0.6135 - val_accuracy: 0.7783\n",
      "Epoch 399/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6493 - accuracy: 0.7593\n",
      "Epoch 399: val_loss did not improve from 0.53624\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6496 - accuracy: 0.7591 - val_loss: 0.5576 - val_accuracy: 0.7975\n",
      "Epoch 400/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6526 - accuracy: 0.7584\n",
      "Epoch 400: val_loss did not improve from 0.53624\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6529 - accuracy: 0.7583 - val_loss: 0.6251 - val_accuracy: 0.7775\n",
      "Epoch 401/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6533 - accuracy: 0.7565\n",
      "Epoch 401: val_loss did not improve from 0.53624\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6533 - accuracy: 0.7565 - val_loss: 0.5730 - val_accuracy: 0.7927\n",
      "Epoch 402/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6528 - accuracy: 0.7570\n",
      "Epoch 402: val_loss did not improve from 0.53624\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6527 - accuracy: 0.7571 - val_loss: 0.5642 - val_accuracy: 0.7942\n",
      "Epoch 403/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6464 - accuracy: 0.7606\n",
      "Epoch 403: val_loss did not improve from 0.53624\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6464 - accuracy: 0.7607 - val_loss: 0.5929 - val_accuracy: 0.7894\n",
      "Epoch 404/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6521 - accuracy: 0.7583\n",
      "Epoch 404: val_loss did not improve from 0.53624\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6524 - accuracy: 0.7581 - val_loss: 0.5622 - val_accuracy: 0.7988\n",
      "Epoch 405/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6544 - accuracy: 0.7581\n",
      "Epoch 405: val_loss did not improve from 0.53624\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6544 - accuracy: 0.7582 - val_loss: 0.6151 - val_accuracy: 0.7801\n",
      "Epoch 406/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6412 - accuracy: 0.7621\n",
      "Epoch 406: val_loss did not improve from 0.53624\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6412 - accuracy: 0.7621 - val_loss: 0.5497 - val_accuracy: 0.8008\n",
      "Epoch 407/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6483 - accuracy: 0.7602\n",
      "Epoch 407: val_loss did not improve from 0.53624\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6482 - accuracy: 0.7603 - val_loss: 0.5451 - val_accuracy: 0.7996\n",
      "Epoch 408/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6497 - accuracy: 0.7595\n",
      "Epoch 408: val_loss did not improve from 0.53624\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6496 - accuracy: 0.7595 - val_loss: 0.5499 - val_accuracy: 0.7977\n",
      "Epoch 409/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6409 - accuracy: 0.7614\n",
      "Epoch 409: val_loss improved from 0.53624 to 0.53269, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6406 - accuracy: 0.7615 - val_loss: 0.5327 - val_accuracy: 0.8028\n",
      "Epoch 410/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6362 - accuracy: 0.7630\n",
      "Epoch 410: val_loss did not improve from 0.53269\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6363 - accuracy: 0.7630 - val_loss: 0.5897 - val_accuracy: 0.7858\n",
      "Epoch 411/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6421 - accuracy: 0.7616\n",
      "Epoch 411: val_loss did not improve from 0.53269\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6421 - accuracy: 0.7615 - val_loss: 0.5614 - val_accuracy: 0.7964\n",
      "Epoch 412/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6422 - accuracy: 0.7610\n",
      "Epoch 412: val_loss did not improve from 0.53269\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6421 - accuracy: 0.7610 - val_loss: 0.5640 - val_accuracy: 0.7934\n",
      "Epoch 413/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6400 - accuracy: 0.7623\n",
      "Epoch 413: val_loss did not improve from 0.53269\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6395 - accuracy: 0.7624 - val_loss: 0.5533 - val_accuracy: 0.7956\n",
      "Epoch 414/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6581 - accuracy: 0.7562\n",
      "Epoch 414: val_loss did not improve from 0.53269\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6578 - accuracy: 0.7563 - val_loss: 0.5542 - val_accuracy: 0.7974\n",
      "Epoch 415/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6408 - accuracy: 0.7614\n",
      "Epoch 415: val_loss did not improve from 0.53269\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6411 - accuracy: 0.7613 - val_loss: 0.5801 - val_accuracy: 0.7877\n",
      "Epoch 416/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6495 - accuracy: 0.7581\n",
      "Epoch 416: val_loss did not improve from 0.53269\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6495 - accuracy: 0.7581 - val_loss: 0.5565 - val_accuracy: 0.7924\n",
      "Epoch 417/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6434 - accuracy: 0.7607\n",
      "Epoch 417: val_loss did not improve from 0.53269\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6434 - accuracy: 0.7607 - val_loss: 0.5576 - val_accuracy: 0.7968\n",
      "Epoch 418/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6300 - accuracy: 0.7642\n",
      "Epoch 418: val_loss did not improve from 0.53269\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6301 - accuracy: 0.7642 - val_loss: 0.5499 - val_accuracy: 0.7980\n",
      "Epoch 419/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6456 - accuracy: 0.7605\n",
      "Epoch 419: val_loss did not improve from 0.53269\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6456 - accuracy: 0.7603 - val_loss: 0.5582 - val_accuracy: 0.7989\n",
      "Epoch 420/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6427 - accuracy: 0.7600\n",
      "Epoch 420: val_loss did not improve from 0.53269\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6429 - accuracy: 0.7598 - val_loss: 0.6811 - val_accuracy: 0.7537\n",
      "Epoch 421/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6436 - accuracy: 0.7609\n",
      "Epoch 421: val_loss did not improve from 0.53269\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6438 - accuracy: 0.7609 - val_loss: 0.5665 - val_accuracy: 0.7894\n",
      "Epoch 422/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6362 - accuracy: 0.7635\n",
      "Epoch 422: val_loss did not improve from 0.53269\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6362 - accuracy: 0.7635 - val_loss: 0.5550 - val_accuracy: 0.7978\n",
      "Epoch 423/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6469 - accuracy: 0.7612\n",
      "Epoch 423: val_loss did not improve from 0.53269\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6471 - accuracy: 0.7612 - val_loss: 0.6042 - val_accuracy: 0.7890\n",
      "Epoch 424/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6372 - accuracy: 0.7621\n",
      "Epoch 424: val_loss did not improve from 0.53269\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6372 - accuracy: 0.7621 - val_loss: 0.5567 - val_accuracy: 0.8004\n",
      "Epoch 425/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6456 - accuracy: 0.7614\n",
      "Epoch 425: val_loss did not improve from 0.53269\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6457 - accuracy: 0.7613 - val_loss: 0.5501 - val_accuracy: 0.7984\n",
      "Epoch 426/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6518 - accuracy: 0.7593\n",
      "Epoch 426: val_loss did not improve from 0.53269\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6518 - accuracy: 0.7593 - val_loss: 0.5539 - val_accuracy: 0.7982\n",
      "Epoch 427/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6398 - accuracy: 0.7622\n",
      "Epoch 427: val_loss did not improve from 0.53269\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6397 - accuracy: 0.7622 - val_loss: 0.5598 - val_accuracy: 0.7931\n",
      "Epoch 428/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6366 - accuracy: 0.7624\n",
      "Epoch 428: val_loss did not improve from 0.53269\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6366 - accuracy: 0.7624 - val_loss: 0.5502 - val_accuracy: 0.7971\n",
      "Epoch 429/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6368 - accuracy: 0.7633\n",
      "Epoch 429: val_loss did not improve from 0.53269\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6364 - accuracy: 0.7635 - val_loss: 0.5744 - val_accuracy: 0.7839\n",
      "Epoch 430/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6381 - accuracy: 0.7635\n",
      "Epoch 430: val_loss did not improve from 0.53269\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6383 - accuracy: 0.7634 - val_loss: 0.5407 - val_accuracy: 0.8060\n",
      "Epoch 431/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6435 - accuracy: 0.7616\n",
      "Epoch 431: val_loss did not improve from 0.53269\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6435 - accuracy: 0.7616 - val_loss: 0.5954 - val_accuracy: 0.7791\n",
      "Epoch 432/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6533 - accuracy: 0.7604\n",
      "Epoch 432: val_loss did not improve from 0.53269\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6533 - accuracy: 0.7604 - val_loss: 0.5634 - val_accuracy: 0.7996\n",
      "Epoch 433/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6430 - accuracy: 0.7626\n",
      "Epoch 433: val_loss did not improve from 0.53269\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6431 - accuracy: 0.7625 - val_loss: 0.5547 - val_accuracy: 0.7984\n",
      "Epoch 434/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6454 - accuracy: 0.7619\n",
      "Epoch 434: val_loss did not improve from 0.53269\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6453 - accuracy: 0.7619 - val_loss: 0.5532 - val_accuracy: 0.8022\n",
      "Epoch 435/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6366 - accuracy: 0.7636\n",
      "Epoch 435: val_loss improved from 0.53269 to 0.52165, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6366 - accuracy: 0.7636 - val_loss: 0.5217 - val_accuracy: 0.8078\n",
      "Epoch 436/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6263 - accuracy: 0.7666\n",
      "Epoch 436: val_loss did not improve from 0.52165\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6262 - accuracy: 0.7667 - val_loss: 0.5478 - val_accuracy: 0.7988\n",
      "Epoch 437/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6429 - accuracy: 0.7623\n",
      "Epoch 437: val_loss did not improve from 0.52165\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6428 - accuracy: 0.7623 - val_loss: 0.5479 - val_accuracy: 0.8005\n",
      "Epoch 438/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6460 - accuracy: 0.7624\n",
      "Epoch 438: val_loss did not improve from 0.52165\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6459 - accuracy: 0.7623 - val_loss: 0.5658 - val_accuracy: 0.7975\n",
      "Epoch 439/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6397 - accuracy: 0.7644\n",
      "Epoch 439: val_loss did not improve from 0.52165\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6402 - accuracy: 0.7642 - val_loss: 0.5419 - val_accuracy: 0.7993\n",
      "Epoch 440/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6389 - accuracy: 0.7633\n",
      "Epoch 440: val_loss did not improve from 0.52165\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6387 - accuracy: 0.7634 - val_loss: 0.6011 - val_accuracy: 0.7865\n",
      "Epoch 441/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6362 - accuracy: 0.7626\n",
      "Epoch 441: val_loss did not improve from 0.52165\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6357 - accuracy: 0.7627 - val_loss: 0.5575 - val_accuracy: 0.7969\n",
      "Epoch 442/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6389 - accuracy: 0.7642\n",
      "Epoch 442: val_loss did not improve from 0.52165\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6390 - accuracy: 0.7642 - val_loss: 0.5714 - val_accuracy: 0.7939\n",
      "Epoch 443/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6287 - accuracy: 0.7686\n",
      "Epoch 443: val_loss did not improve from 0.52165\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6287 - accuracy: 0.7686 - val_loss: 0.5798 - val_accuracy: 0.7906\n",
      "Epoch 444/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6420 - accuracy: 0.7629\n",
      "Epoch 444: val_loss did not improve from 0.52165\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6420 - accuracy: 0.7629 - val_loss: 0.5725 - val_accuracy: 0.7893\n",
      "Epoch 445/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6342 - accuracy: 0.7656\n",
      "Epoch 445: val_loss did not improve from 0.52165\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6342 - accuracy: 0.7656 - val_loss: 0.5694 - val_accuracy: 0.7948\n",
      "Epoch 446/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6353 - accuracy: 0.7651\n",
      "Epoch 446: val_loss did not improve from 0.52165\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6353 - accuracy: 0.7651 - val_loss: 0.5477 - val_accuracy: 0.8016\n",
      "Epoch 447/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6404 - accuracy: 0.7636\n",
      "Epoch 447: val_loss did not improve from 0.52165\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6404 - accuracy: 0.7636 - val_loss: 0.5515 - val_accuracy: 0.7991\n",
      "Epoch 448/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6226 - accuracy: 0.7701\n",
      "Epoch 448: val_loss did not improve from 0.52165\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6226 - accuracy: 0.7701 - val_loss: 0.5327 - val_accuracy: 0.8019\n",
      "Epoch 449/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6281 - accuracy: 0.7668\n",
      "Epoch 449: val_loss did not improve from 0.52165\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6282 - accuracy: 0.7667 - val_loss: 0.5361 - val_accuracy: 0.7995\n",
      "Epoch 450/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6303 - accuracy: 0.7669\n",
      "Epoch 450: val_loss did not improve from 0.52165\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6303 - accuracy: 0.7669 - val_loss: 0.5410 - val_accuracy: 0.8001\n",
      "Epoch 451/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6418 - accuracy: 0.7628\n",
      "Epoch 451: val_loss did not improve from 0.52165\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6415 - accuracy: 0.7629 - val_loss: 0.5389 - val_accuracy: 0.8006\n",
      "Epoch 452/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6339 - accuracy: 0.7655\n",
      "Epoch 452: val_loss did not improve from 0.52165\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6341 - accuracy: 0.7654 - val_loss: 0.5503 - val_accuracy: 0.7999\n",
      "Epoch 453/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6332 - accuracy: 0.7657\n",
      "Epoch 453: val_loss did not improve from 0.52165\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6331 - accuracy: 0.7657 - val_loss: 0.5339 - val_accuracy: 0.8004\n",
      "Epoch 454/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6258 - accuracy: 0.7681\n",
      "Epoch 454: val_loss improved from 0.52165 to 0.50679, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6258 - accuracy: 0.7681 - val_loss: 0.5068 - val_accuracy: 0.8168\n",
      "Epoch 455/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6228 - accuracy: 0.7686\n",
      "Epoch 455: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6228 - accuracy: 0.7686 - val_loss: 0.5636 - val_accuracy: 0.7988\n",
      "Epoch 456/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6372 - accuracy: 0.7630\n",
      "Epoch 456: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6371 - accuracy: 0.7630 - val_loss: 0.5502 - val_accuracy: 0.8022\n",
      "Epoch 457/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6258 - accuracy: 0.7664\n",
      "Epoch 457: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6256 - accuracy: 0.7665 - val_loss: 0.5489 - val_accuracy: 0.7986\n",
      "Epoch 458/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6310 - accuracy: 0.7690\n",
      "Epoch 458: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6310 - accuracy: 0.7690 - val_loss: 0.5334 - val_accuracy: 0.8059\n",
      "Epoch 459/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6279 - accuracy: 0.7682\n",
      "Epoch 459: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6275 - accuracy: 0.7683 - val_loss: 0.5773 - val_accuracy: 0.7859\n",
      "Epoch 460/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6400 - accuracy: 0.7639\n",
      "Epoch 460: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6399 - accuracy: 0.7638 - val_loss: 0.5476 - val_accuracy: 0.8011\n",
      "Epoch 461/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6434 - accuracy: 0.7630\n",
      "Epoch 461: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6434 - accuracy: 0.7630 - val_loss: 0.5424 - val_accuracy: 0.8032\n",
      "Epoch 462/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6487 - accuracy: 0.7621\n",
      "Epoch 462: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6491 - accuracy: 0.7619 - val_loss: 0.5712 - val_accuracy: 0.7964\n",
      "Epoch 463/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6367 - accuracy: 0.7643\n",
      "Epoch 463: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6369 - accuracy: 0.7644 - val_loss: 0.5461 - val_accuracy: 0.8019\n",
      "Epoch 464/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6226 - accuracy: 0.7695\n",
      "Epoch 464: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6225 - accuracy: 0.7695 - val_loss: 0.5344 - val_accuracy: 0.8028\n",
      "Epoch 465/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6327 - accuracy: 0.7668\n",
      "Epoch 465: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6327 - accuracy: 0.7667 - val_loss: 0.5428 - val_accuracy: 0.7998\n",
      "Epoch 466/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6257 - accuracy: 0.7697\n",
      "Epoch 466: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6256 - accuracy: 0.7697 - val_loss: 0.5414 - val_accuracy: 0.8068\n",
      "Epoch 467/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6324 - accuracy: 0.7657\n",
      "Epoch 467: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6324 - accuracy: 0.7657 - val_loss: 0.5778 - val_accuracy: 0.7915\n",
      "Epoch 468/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6281 - accuracy: 0.7670\n",
      "Epoch 468: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6278 - accuracy: 0.7671 - val_loss: 0.5498 - val_accuracy: 0.7981\n",
      "Epoch 469/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6686 - accuracy: 0.7570\n",
      "Epoch 469: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6686 - accuracy: 0.7570 - val_loss: 0.5470 - val_accuracy: 0.8025\n",
      "Epoch 470/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6248 - accuracy: 0.7683\n",
      "Epoch 470: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6249 - accuracy: 0.7683 - val_loss: 0.5330 - val_accuracy: 0.8032\n",
      "Epoch 471/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6298 - accuracy: 0.7687\n",
      "Epoch 471: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6294 - accuracy: 0.7689 - val_loss: 0.5426 - val_accuracy: 0.8035\n",
      "Epoch 472/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6346 - accuracy: 0.7663\n",
      "Epoch 472: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6346 - accuracy: 0.7663 - val_loss: 0.6024 - val_accuracy: 0.7904\n",
      "Epoch 473/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6425 - accuracy: 0.7630\n",
      "Epoch 473: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6425 - accuracy: 0.7630 - val_loss: 0.5278 - val_accuracy: 0.8074\n",
      "Epoch 474/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6288 - accuracy: 0.7669\n",
      "Epoch 474: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6285 - accuracy: 0.7670 - val_loss: 0.5747 - val_accuracy: 0.7947\n",
      "Epoch 475/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6153 - accuracy: 0.7723\n",
      "Epoch 475: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6150 - accuracy: 0.7723 - val_loss: 0.5375 - val_accuracy: 0.8006\n",
      "Epoch 476/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6662 - accuracy: 0.7565\n",
      "Epoch 476: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6662 - accuracy: 0.7565 - val_loss: 0.5262 - val_accuracy: 0.8149\n",
      "Epoch 477/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6367 - accuracy: 0.7648\n",
      "Epoch 477: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6369 - accuracy: 0.7647 - val_loss: 0.5434 - val_accuracy: 0.7995\n",
      "Epoch 478/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6400 - accuracy: 0.7633\n",
      "Epoch 478: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6399 - accuracy: 0.7634 - val_loss: 0.5255 - val_accuracy: 0.8012\n",
      "Epoch 479/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6152 - accuracy: 0.7714\n",
      "Epoch 479: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6152 - accuracy: 0.7714 - val_loss: 0.5552 - val_accuracy: 0.8001\n",
      "Epoch 480/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6247 - accuracy: 0.7701\n",
      "Epoch 480: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6243 - accuracy: 0.7701 - val_loss: 0.5294 - val_accuracy: 0.8058\n",
      "Epoch 481/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6369 - accuracy: 0.7668\n",
      "Epoch 481: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6366 - accuracy: 0.7668 - val_loss: 0.5426 - val_accuracy: 0.7969\n",
      "Epoch 482/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6358 - accuracy: 0.7664\n",
      "Epoch 482: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6359 - accuracy: 0.7663 - val_loss: 0.5509 - val_accuracy: 0.8000\n",
      "Epoch 483/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6354 - accuracy: 0.7659\n",
      "Epoch 483: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6353 - accuracy: 0.7659 - val_loss: 0.5157 - val_accuracy: 0.8108\n",
      "Epoch 484/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6205 - accuracy: 0.7724\n",
      "Epoch 484: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6204 - accuracy: 0.7724 - val_loss: 0.5493 - val_accuracy: 0.7998\n",
      "Epoch 485/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6183 - accuracy: 0.7716\n",
      "Epoch 485: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6189 - accuracy: 0.7715 - val_loss: 0.5852 - val_accuracy: 0.7975\n",
      "Epoch 486/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6307 - accuracy: 0.7668\n",
      "Epoch 486: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6309 - accuracy: 0.7668 - val_loss: 0.5452 - val_accuracy: 0.8019\n",
      "Epoch 487/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6160 - accuracy: 0.7713\n",
      "Epoch 487: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6160 - accuracy: 0.7713 - val_loss: 0.5435 - val_accuracy: 0.8042\n",
      "Epoch 488/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6169 - accuracy: 0.7721\n",
      "Epoch 488: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6168 - accuracy: 0.7722 - val_loss: 0.5433 - val_accuracy: 0.8029\n",
      "Epoch 489/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6218 - accuracy: 0.7697\n",
      "Epoch 489: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6217 - accuracy: 0.7697 - val_loss: 0.5466 - val_accuracy: 0.8018\n",
      "Epoch 490/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6216 - accuracy: 0.7712\n",
      "Epoch 490: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6216 - accuracy: 0.7713 - val_loss: 0.5165 - val_accuracy: 0.8085\n",
      "Epoch 491/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6260 - accuracy: 0.7696\n",
      "Epoch 491: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6263 - accuracy: 0.7695 - val_loss: 0.5488 - val_accuracy: 0.8013\n",
      "Epoch 492/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6307 - accuracy: 0.7670\n",
      "Epoch 492: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6302 - accuracy: 0.7672 - val_loss: 0.5359 - val_accuracy: 0.8036\n",
      "Epoch 493/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6183 - accuracy: 0.7723\n",
      "Epoch 493: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6183 - accuracy: 0.7723 - val_loss: 0.5474 - val_accuracy: 0.8042\n",
      "Epoch 494/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6223 - accuracy: 0.7701\n",
      "Epoch 494: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6226 - accuracy: 0.7700 - val_loss: 0.5242 - val_accuracy: 0.8055\n",
      "Epoch 495/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6134 - accuracy: 0.7740\n",
      "Epoch 495: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6132 - accuracy: 0.7741 - val_loss: 0.5296 - val_accuracy: 0.8043\n",
      "Epoch 496/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6323 - accuracy: 0.7686\n",
      "Epoch 496: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6323 - accuracy: 0.7686 - val_loss: 0.5387 - val_accuracy: 0.8095\n",
      "Epoch 497/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6143 - accuracy: 0.7736\n",
      "Epoch 497: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6155 - accuracy: 0.7732 - val_loss: 0.5650 - val_accuracy: 0.7911\n",
      "Epoch 498/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6315 - accuracy: 0.7679\n",
      "Epoch 498: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6313 - accuracy: 0.7679 - val_loss: 0.5558 - val_accuracy: 0.8004\n",
      "Epoch 499/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6232 - accuracy: 0.7715\n",
      "Epoch 499: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6232 - accuracy: 0.7715 - val_loss: 0.5293 - val_accuracy: 0.8031\n",
      "Epoch 500/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6184 - accuracy: 0.7715\n",
      "Epoch 500: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6182 - accuracy: 0.7716 - val_loss: 0.5553 - val_accuracy: 0.8019\n",
      "Epoch 501/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6169 - accuracy: 0.7733\n",
      "Epoch 501: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6169 - accuracy: 0.7733 - val_loss: 0.5395 - val_accuracy: 0.8042\n",
      "Epoch 502/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6293 - accuracy: 0.7669\n",
      "Epoch 502: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6293 - accuracy: 0.7669 - val_loss: 0.5443 - val_accuracy: 0.8039\n",
      "Epoch 503/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6147 - accuracy: 0.7723\n",
      "Epoch 503: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6146 - accuracy: 0.7725 - val_loss: 0.5191 - val_accuracy: 0.8071\n",
      "Epoch 504/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6160 - accuracy: 0.7732\n",
      "Epoch 504: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6160 - accuracy: 0.7732 - val_loss: 0.5508 - val_accuracy: 0.7979\n",
      "Epoch 505/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6162 - accuracy: 0.7730\n",
      "Epoch 505: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6163 - accuracy: 0.7729 - val_loss: 0.5655 - val_accuracy: 0.7922\n",
      "Epoch 506/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6189 - accuracy: 0.7720\n",
      "Epoch 506: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6186 - accuracy: 0.7721 - val_loss: 0.5468 - val_accuracy: 0.7967\n",
      "Epoch 507/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6215 - accuracy: 0.7700\n",
      "Epoch 507: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6222 - accuracy: 0.7700 - val_loss: 0.5683 - val_accuracy: 0.7917\n",
      "Epoch 508/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6276 - accuracy: 0.7702\n",
      "Epoch 508: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6275 - accuracy: 0.7702 - val_loss: 0.5218 - val_accuracy: 0.8112\n",
      "Epoch 509/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6256 - accuracy: 0.7682\n",
      "Epoch 509: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6257 - accuracy: 0.7682 - val_loss: 0.5450 - val_accuracy: 0.8051\n",
      "Epoch 510/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6097 - accuracy: 0.7735\n",
      "Epoch 510: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6098 - accuracy: 0.7736 - val_loss: 0.5497 - val_accuracy: 0.7981\n",
      "Epoch 511/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6165 - accuracy: 0.7721\n",
      "Epoch 511: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6165 - accuracy: 0.7720 - val_loss: 0.5521 - val_accuracy: 0.7961\n",
      "Epoch 512/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6233 - accuracy: 0.7723\n",
      "Epoch 512: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6233 - accuracy: 0.7723 - val_loss: 0.5255 - val_accuracy: 0.8058\n",
      "Epoch 513/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6197 - accuracy: 0.7736\n",
      "Epoch 513: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6195 - accuracy: 0.7738 - val_loss: 0.5277 - val_accuracy: 0.8104\n",
      "Epoch 514/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6236 - accuracy: 0.7720\n",
      "Epoch 514: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6236 - accuracy: 0.7720 - val_loss: 0.5182 - val_accuracy: 0.8103\n",
      "Epoch 515/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6136 - accuracy: 0.7724\n",
      "Epoch 515: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6138 - accuracy: 0.7722 - val_loss: 0.5322 - val_accuracy: 0.8072\n",
      "Epoch 516/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6171 - accuracy: 0.7726\n",
      "Epoch 516: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6168 - accuracy: 0.7727 - val_loss: 0.5484 - val_accuracy: 0.7966\n",
      "Epoch 517/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6083 - accuracy: 0.7756\n",
      "Epoch 517: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6082 - accuracy: 0.7758 - val_loss: 0.5180 - val_accuracy: 0.8122\n",
      "Epoch 518/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6247 - accuracy: 0.7722\n",
      "Epoch 518: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6249 - accuracy: 0.7720 - val_loss: 0.5291 - val_accuracy: 0.8076\n",
      "Epoch 519/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6140 - accuracy: 0.7745\n",
      "Epoch 519: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6138 - accuracy: 0.7746 - val_loss: 0.5363 - val_accuracy: 0.8035\n",
      "Epoch 520/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6175 - accuracy: 0.7733\n",
      "Epoch 520: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6175 - accuracy: 0.7733 - val_loss: 0.5404 - val_accuracy: 0.8011\n",
      "Epoch 521/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6357 - accuracy: 0.7683\n",
      "Epoch 521: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6355 - accuracy: 0.7683 - val_loss: 0.5164 - val_accuracy: 0.8144\n",
      "Epoch 522/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6208 - accuracy: 0.7718\n",
      "Epoch 522: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6210 - accuracy: 0.7718 - val_loss: 0.5361 - val_accuracy: 0.8050\n",
      "Epoch 523/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6147 - accuracy: 0.7734\n",
      "Epoch 523: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6147 - accuracy: 0.7732 - val_loss: 0.5274 - val_accuracy: 0.8018\n",
      "Epoch 524/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6217 - accuracy: 0.7737\n",
      "Epoch 524: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6218 - accuracy: 0.7737 - val_loss: 0.6682 - val_accuracy: 0.7596\n",
      "Epoch 525/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6173 - accuracy: 0.7732\n",
      "Epoch 525: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6172 - accuracy: 0.7733 - val_loss: 0.5486 - val_accuracy: 0.8032\n",
      "Epoch 526/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6618 - accuracy: 0.7583\n",
      "Epoch 526: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6618 - accuracy: 0.7583 - val_loss: 0.5593 - val_accuracy: 0.8001\n",
      "Epoch 527/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6173 - accuracy: 0.7720\n",
      "Epoch 527: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6175 - accuracy: 0.7718 - val_loss: 0.5312 - val_accuracy: 0.8021\n",
      "Epoch 528/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6161 - accuracy: 0.7741\n",
      "Epoch 528: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6161 - accuracy: 0.7741 - val_loss: 0.5239 - val_accuracy: 0.8093\n",
      "Epoch 529/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6142 - accuracy: 0.7729\n",
      "Epoch 529: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6142 - accuracy: 0.7729 - val_loss: 0.5216 - val_accuracy: 0.8100\n",
      "Epoch 530/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6403 - accuracy: 0.7708\n",
      "Epoch 530: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6404 - accuracy: 0.7708 - val_loss: 0.5588 - val_accuracy: 0.8023\n",
      "Epoch 531/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6230 - accuracy: 0.7725\n",
      "Epoch 531: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6229 - accuracy: 0.7725 - val_loss: 0.5500 - val_accuracy: 0.7993\n",
      "Epoch 532/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6230 - accuracy: 0.7722\n",
      "Epoch 532: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6230 - accuracy: 0.7722 - val_loss: 0.5089 - val_accuracy: 0.8151\n",
      "Epoch 533/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6263 - accuracy: 0.7711\n",
      "Epoch 533: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6262 - accuracy: 0.7712 - val_loss: 0.5440 - val_accuracy: 0.8042\n",
      "Epoch 534/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6125 - accuracy: 0.7752\n",
      "Epoch 534: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6125 - accuracy: 0.7751 - val_loss: 0.5327 - val_accuracy: 0.8085\n",
      "Epoch 535/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6217 - accuracy: 0.7726\n",
      "Epoch 535: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6215 - accuracy: 0.7726 - val_loss: 0.5280 - val_accuracy: 0.8112\n",
      "Epoch 536/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6203 - accuracy: 0.7727\n",
      "Epoch 536: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6203 - accuracy: 0.7727 - val_loss: 0.5227 - val_accuracy: 0.8103\n",
      "Epoch 537/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6136 - accuracy: 0.7739\n",
      "Epoch 537: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6136 - accuracy: 0.7739 - val_loss: 0.5233 - val_accuracy: 0.8104\n",
      "Epoch 538/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6141 - accuracy: 0.7742\n",
      "Epoch 538: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6141 - accuracy: 0.7742 - val_loss: 0.5322 - val_accuracy: 0.8015\n",
      "Epoch 539/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6239 - accuracy: 0.7718\n",
      "Epoch 539: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6245 - accuracy: 0.7715 - val_loss: 0.5328 - val_accuracy: 0.8026\n",
      "Epoch 540/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6116 - accuracy: 0.7757\n",
      "Epoch 540: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6116 - accuracy: 0.7757 - val_loss: 0.5157 - val_accuracy: 0.8137\n",
      "Epoch 541/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6111 - accuracy: 0.7757\n",
      "Epoch 541: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6111 - accuracy: 0.7756 - val_loss: 0.5196 - val_accuracy: 0.8101\n",
      "Epoch 542/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6129 - accuracy: 0.7754\n",
      "Epoch 542: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6128 - accuracy: 0.7754 - val_loss: 0.5209 - val_accuracy: 0.8086\n",
      "Epoch 543/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6206 - accuracy: 0.7728\n",
      "Epoch 543: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6206 - accuracy: 0.7727 - val_loss: 0.5259 - val_accuracy: 0.8072\n",
      "Epoch 544/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6215 - accuracy: 0.7731\n",
      "Epoch 544: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6214 - accuracy: 0.7731 - val_loss: 0.5518 - val_accuracy: 0.8024\n",
      "Epoch 545/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6155 - accuracy: 0.7778\n",
      "Epoch 545: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6156 - accuracy: 0.7778 - val_loss: 0.5212 - val_accuracy: 0.8115\n",
      "Epoch 546/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6420 - accuracy: 0.7672\n",
      "Epoch 546: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6420 - accuracy: 0.7672 - val_loss: 0.5157 - val_accuracy: 0.8093\n",
      "Epoch 547/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6074 - accuracy: 0.7771\n",
      "Epoch 547: val_loss did not improve from 0.50679\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6074 - accuracy: 0.7771 - val_loss: 0.5179 - val_accuracy: 0.8137\n",
      "Epoch 548/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6030 - accuracy: 0.7792\n",
      "Epoch 548: val_loss improved from 0.50679 to 0.50534, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6034 - accuracy: 0.7791 - val_loss: 0.5053 - val_accuracy: 0.8146\n",
      "Epoch 549/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6134 - accuracy: 0.7738\n",
      "Epoch 549: val_loss did not improve from 0.50534\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6134 - accuracy: 0.7738 - val_loss: 0.5506 - val_accuracy: 0.7961\n",
      "Epoch 550/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6095 - accuracy: 0.7765\n",
      "Epoch 550: val_loss did not improve from 0.50534\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6095 - accuracy: 0.7765 - val_loss: 0.5647 - val_accuracy: 0.8006\n",
      "Epoch 551/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6160 - accuracy: 0.7749\n",
      "Epoch 551: val_loss did not improve from 0.50534\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6160 - accuracy: 0.7749 - val_loss: 0.5263 - val_accuracy: 0.8136\n",
      "Epoch 552/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6121 - accuracy: 0.7762\n",
      "Epoch 552: val_loss did not improve from 0.50534\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6121 - accuracy: 0.7762 - val_loss: 0.5111 - val_accuracy: 0.8115\n",
      "Epoch 553/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6027 - accuracy: 0.7783\n",
      "Epoch 553: val_loss improved from 0.50534 to 0.49966, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6028 - accuracy: 0.7783 - val_loss: 0.4997 - val_accuracy: 0.8194\n",
      "Epoch 554/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6135 - accuracy: 0.7751\n",
      "Epoch 554: val_loss did not improve from 0.49966\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6132 - accuracy: 0.7750 - val_loss: 0.5156 - val_accuracy: 0.8118\n",
      "Epoch 555/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6129 - accuracy: 0.7735\n",
      "Epoch 555: val_loss did not improve from 0.49966\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6129 - accuracy: 0.7735 - val_loss: 0.5457 - val_accuracy: 0.8043\n",
      "Epoch 556/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6140 - accuracy: 0.7746\n",
      "Epoch 556: val_loss did not improve from 0.49966\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6137 - accuracy: 0.7748 - val_loss: 0.5297 - val_accuracy: 0.8107\n",
      "Epoch 557/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5991 - accuracy: 0.7786\n",
      "Epoch 557: val_loss did not improve from 0.49966\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5990 - accuracy: 0.7786 - val_loss: 0.5320 - val_accuracy: 0.8057\n",
      "Epoch 558/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6073 - accuracy: 0.7781\n",
      "Epoch 558: val_loss did not improve from 0.49966\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6074 - accuracy: 0.7782 - val_loss: 0.5264 - val_accuracy: 0.8067\n",
      "Epoch 559/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6186 - accuracy: 0.7734\n",
      "Epoch 559: val_loss did not improve from 0.49966\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6185 - accuracy: 0.7735 - val_loss: 0.5244 - val_accuracy: 0.8104\n",
      "Epoch 560/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6139 - accuracy: 0.7740\n",
      "Epoch 560: val_loss did not improve from 0.49966\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6139 - accuracy: 0.7740 - val_loss: 0.5527 - val_accuracy: 0.7950\n",
      "Epoch 561/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6234 - accuracy: 0.7706\n",
      "Epoch 561: val_loss did not improve from 0.49966\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6235 - accuracy: 0.7705 - val_loss: 0.5158 - val_accuracy: 0.8103\n",
      "Epoch 562/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6029 - accuracy: 0.7797\n",
      "Epoch 562: val_loss did not improve from 0.49966\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6029 - accuracy: 0.7797 - val_loss: 0.5406 - val_accuracy: 0.8018\n",
      "Epoch 563/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6095 - accuracy: 0.7763\n",
      "Epoch 563: val_loss did not improve from 0.49966\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6097 - accuracy: 0.7762 - val_loss: 0.5164 - val_accuracy: 0.8107\n",
      "Epoch 564/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6080 - accuracy: 0.7771\n",
      "Epoch 564: val_loss did not improve from 0.49966\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6078 - accuracy: 0.7772 - val_loss: 0.5342 - val_accuracy: 0.8064\n",
      "Epoch 565/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6003 - accuracy: 0.7789\n",
      "Epoch 565: val_loss did not improve from 0.49966\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6003 - accuracy: 0.7789 - val_loss: 0.5248 - val_accuracy: 0.8088\n",
      "Epoch 566/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6130 - accuracy: 0.7765\n",
      "Epoch 566: val_loss did not improve from 0.49966\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6132 - accuracy: 0.7766 - val_loss: 0.5177 - val_accuracy: 0.8162\n",
      "Epoch 567/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6094 - accuracy: 0.7773\n",
      "Epoch 567: val_loss did not improve from 0.49966\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6094 - accuracy: 0.7773 - val_loss: 0.5681 - val_accuracy: 0.7982\n",
      "Epoch 568/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6146 - accuracy: 0.7765\n",
      "Epoch 568: val_loss did not improve from 0.49966\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6143 - accuracy: 0.7766 - val_loss: 0.5156 - val_accuracy: 0.8146\n",
      "Epoch 569/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6052 - accuracy: 0.7777\n",
      "Epoch 569: val_loss did not improve from 0.49966\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6052 - accuracy: 0.7777 - val_loss: 0.6180 - val_accuracy: 0.7797\n",
      "Epoch 570/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6103 - accuracy: 0.7759\n",
      "Epoch 570: val_loss did not improve from 0.49966\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6103 - accuracy: 0.7759 - val_loss: 0.5149 - val_accuracy: 0.8090\n",
      "Epoch 571/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6047 - accuracy: 0.7776\n",
      "Epoch 571: val_loss improved from 0.49966 to 0.49015, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6048 - accuracy: 0.7775 - val_loss: 0.4902 - val_accuracy: 0.8181\n",
      "Epoch 572/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6097 - accuracy: 0.7766\n",
      "Epoch 572: val_loss did not improve from 0.49015\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6098 - accuracy: 0.7765 - val_loss: 0.5131 - val_accuracy: 0.8110\n",
      "Epoch 573/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6055 - accuracy: 0.7777\n",
      "Epoch 573: val_loss did not improve from 0.49015\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6053 - accuracy: 0.7778 - val_loss: 0.5280 - val_accuracy: 0.8076\n",
      "Epoch 574/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5957 - accuracy: 0.7807\n",
      "Epoch 574: val_loss did not improve from 0.49015\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5957 - accuracy: 0.7807 - val_loss: 0.5022 - val_accuracy: 0.8175\n",
      "Epoch 575/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6032 - accuracy: 0.7783\n",
      "Epoch 575: val_loss did not improve from 0.49015\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6032 - accuracy: 0.7784 - val_loss: 0.5513 - val_accuracy: 0.7992\n",
      "Epoch 576/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6045 - accuracy: 0.7780\n",
      "Epoch 576: val_loss did not improve from 0.49015\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6045 - accuracy: 0.7780 - val_loss: 0.4951 - val_accuracy: 0.8201\n",
      "Epoch 577/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6251 - accuracy: 0.7733\n",
      "Epoch 577: val_loss did not improve from 0.49015\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6251 - accuracy: 0.7733 - val_loss: 0.5267 - val_accuracy: 0.8046\n",
      "Epoch 578/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6008 - accuracy: 0.7783\n",
      "Epoch 578: val_loss did not improve from 0.49015\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6010 - accuracy: 0.7783 - val_loss: 0.5750 - val_accuracy: 0.7884\n",
      "Epoch 579/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6069 - accuracy: 0.7785\n",
      "Epoch 579: val_loss did not improve from 0.49015\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6069 - accuracy: 0.7785 - val_loss: 0.5884 - val_accuracy: 0.7885\n",
      "Epoch 580/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6102 - accuracy: 0.7768\n",
      "Epoch 580: val_loss did not improve from 0.49015\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6102 - accuracy: 0.7767 - val_loss: 0.5247 - val_accuracy: 0.8068\n",
      "Epoch 581/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6003 - accuracy: 0.7800\n",
      "Epoch 581: val_loss did not improve from 0.49015\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6003 - accuracy: 0.7800 - val_loss: 0.5476 - val_accuracy: 0.7966\n",
      "Epoch 582/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6026 - accuracy: 0.7783\n",
      "Epoch 582: val_loss did not improve from 0.49015\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6023 - accuracy: 0.7785 - val_loss: 0.5748 - val_accuracy: 0.7918\n",
      "Epoch 583/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6057 - accuracy: 0.7779\n",
      "Epoch 583: val_loss did not improve from 0.49015\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6061 - accuracy: 0.7778 - val_loss: 0.5143 - val_accuracy: 0.8146\n",
      "Epoch 584/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6214 - accuracy: 0.7752\n",
      "Epoch 584: val_loss did not improve from 0.49015\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6214 - accuracy: 0.7752 - val_loss: 0.5433 - val_accuracy: 0.8059\n",
      "Epoch 585/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5997 - accuracy: 0.7789\n",
      "Epoch 585: val_loss did not improve from 0.49015\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5999 - accuracy: 0.7789 - val_loss: 0.5312 - val_accuracy: 0.8082\n",
      "Epoch 586/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6011 - accuracy: 0.7786\n",
      "Epoch 586: val_loss did not improve from 0.49015\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6011 - accuracy: 0.7786 - val_loss: 0.5041 - val_accuracy: 0.8180\n",
      "Epoch 587/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5953 - accuracy: 0.7805\n",
      "Epoch 587: val_loss did not improve from 0.49015\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5952 - accuracy: 0.7805 - val_loss: 0.5073 - val_accuracy: 0.8133\n",
      "Epoch 588/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6114 - accuracy: 0.7772\n",
      "Epoch 588: val_loss did not improve from 0.49015\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6114 - accuracy: 0.7772 - val_loss: 0.5431 - val_accuracy: 0.8069\n",
      "Epoch 589/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6116 - accuracy: 0.7764\n",
      "Epoch 589: val_loss did not improve from 0.49015\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6115 - accuracy: 0.7764 - val_loss: 0.5096 - val_accuracy: 0.8160\n",
      "Epoch 590/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6333 - accuracy: 0.7696\n",
      "Epoch 590: val_loss did not improve from 0.49015\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6330 - accuracy: 0.7698 - val_loss: 0.5055 - val_accuracy: 0.8145\n",
      "Epoch 591/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5947 - accuracy: 0.7818\n",
      "Epoch 591: val_loss did not improve from 0.49015\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5948 - accuracy: 0.7818 - val_loss: 0.5155 - val_accuracy: 0.8071\n",
      "Epoch 592/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6030 - accuracy: 0.7792\n",
      "Epoch 592: val_loss did not improve from 0.49015\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6027 - accuracy: 0.7793 - val_loss: 0.5690 - val_accuracy: 0.7942\n",
      "Epoch 593/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6011 - accuracy: 0.7798\n",
      "Epoch 593: val_loss did not improve from 0.49015\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6011 - accuracy: 0.7798 - val_loss: 0.5210 - val_accuracy: 0.8160\n",
      "Epoch 594/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6036 - accuracy: 0.7785\n",
      "Epoch 594: val_loss did not improve from 0.49015\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6032 - accuracy: 0.7787 - val_loss: 0.5277 - val_accuracy: 0.8072\n",
      "Epoch 595/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6081 - accuracy: 0.7786\n",
      "Epoch 595: val_loss did not improve from 0.49015\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6084 - accuracy: 0.7785 - val_loss: 0.5558 - val_accuracy: 0.8008\n",
      "Epoch 596/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5976 - accuracy: 0.7818\n",
      "Epoch 596: val_loss did not improve from 0.49015\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5976 - accuracy: 0.7818 - val_loss: 0.5396 - val_accuracy: 0.8059\n",
      "Epoch 597/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5979 - accuracy: 0.7803\n",
      "Epoch 597: val_loss did not improve from 0.49015\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5979 - accuracy: 0.7803 - val_loss: 0.5234 - val_accuracy: 0.8109\n",
      "Epoch 598/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6002 - accuracy: 0.7798\n",
      "Epoch 598: val_loss did not improve from 0.49015\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6002 - accuracy: 0.7798 - val_loss: 0.5107 - val_accuracy: 0.8160\n",
      "Epoch 599/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6261 - accuracy: 0.7736\n",
      "Epoch 599: val_loss did not improve from 0.49015\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6261 - accuracy: 0.7736 - val_loss: 0.5216 - val_accuracy: 0.8104\n",
      "Epoch 600/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5977 - accuracy: 0.7801\n",
      "Epoch 600: val_loss did not improve from 0.49015\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5976 - accuracy: 0.7800 - val_loss: 0.5209 - val_accuracy: 0.8103\n",
      "Epoch 601/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5999 - accuracy: 0.7813\n",
      "Epoch 601: val_loss did not improve from 0.49015\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6000 - accuracy: 0.7813 - val_loss: 0.5083 - val_accuracy: 0.8177\n",
      "Epoch 602/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6077 - accuracy: 0.7790\n",
      "Epoch 602: val_loss did not improve from 0.49015\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6080 - accuracy: 0.7790 - val_loss: 0.5744 - val_accuracy: 0.8038\n",
      "Epoch 603/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6002 - accuracy: 0.7807\n",
      "Epoch 603: val_loss did not improve from 0.49015\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6003 - accuracy: 0.7806 - val_loss: 0.5425 - val_accuracy: 0.8085\n",
      "Epoch 604/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6105 - accuracy: 0.7783\n",
      "Epoch 604: val_loss did not improve from 0.49015\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6105 - accuracy: 0.7783 - val_loss: 0.5174 - val_accuracy: 0.8146\n",
      "Epoch 605/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6035 - accuracy: 0.7794\n",
      "Epoch 605: val_loss did not improve from 0.49015\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6035 - accuracy: 0.7794 - val_loss: 0.5085 - val_accuracy: 0.8148\n",
      "Epoch 606/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6313 - accuracy: 0.7710\n",
      "Epoch 606: val_loss did not improve from 0.49015\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6313 - accuracy: 0.7710 - val_loss: 0.5496 - val_accuracy: 0.8021\n",
      "Epoch 607/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5955 - accuracy: 0.7825\n",
      "Epoch 607: val_loss did not improve from 0.49015\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5956 - accuracy: 0.7825 - val_loss: 0.5135 - val_accuracy: 0.8133\n",
      "Epoch 608/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6000 - accuracy: 0.7794\n",
      "Epoch 608: val_loss did not improve from 0.49015\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6000 - accuracy: 0.7794 - val_loss: 0.5139 - val_accuracy: 0.8113\n",
      "Epoch 609/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5951 - accuracy: 0.7831\n",
      "Epoch 609: val_loss did not improve from 0.49015\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5951 - accuracy: 0.7831 - val_loss: 0.5443 - val_accuracy: 0.8103\n",
      "Epoch 610/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6094 - accuracy: 0.7785\n",
      "Epoch 610: val_loss did not improve from 0.49015\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6096 - accuracy: 0.7783 - val_loss: 0.5759 - val_accuracy: 0.7917\n",
      "Epoch 611/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6075 - accuracy: 0.7780\n",
      "Epoch 611: val_loss did not improve from 0.49015\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6075 - accuracy: 0.7780 - val_loss: 0.5356 - val_accuracy: 0.8030\n",
      "Epoch 612/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6162 - accuracy: 0.7764\n",
      "Epoch 612: val_loss did not improve from 0.49015\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6166 - accuracy: 0.7763 - val_loss: 0.5326 - val_accuracy: 0.8079\n",
      "Epoch 613/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6041 - accuracy: 0.7792\n",
      "Epoch 613: val_loss did not improve from 0.49015\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6045 - accuracy: 0.7791 - val_loss: 0.5120 - val_accuracy: 0.8175\n",
      "Epoch 614/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5967 - accuracy: 0.7811\n",
      "Epoch 614: val_loss did not improve from 0.49015\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5967 - accuracy: 0.7811 - val_loss: 0.5468 - val_accuracy: 0.7960\n",
      "Epoch 615/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5946 - accuracy: 0.7820\n",
      "Epoch 615: val_loss did not improve from 0.49015\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5946 - accuracy: 0.7820 - val_loss: 0.5684 - val_accuracy: 0.8049\n",
      "Epoch 616/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5974 - accuracy: 0.7811\n",
      "Epoch 616: val_loss did not improve from 0.49015\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5974 - accuracy: 0.7811 - val_loss: 0.5331 - val_accuracy: 0.8118\n",
      "Epoch 617/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6000 - accuracy: 0.7818\n",
      "Epoch 617: val_loss did not improve from 0.49015\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5999 - accuracy: 0.7818 - val_loss: 0.5207 - val_accuracy: 0.8111\n",
      "Epoch 618/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5994 - accuracy: 0.7803\n",
      "Epoch 618: val_loss did not improve from 0.49015\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5994 - accuracy: 0.7802 - val_loss: 0.5168 - val_accuracy: 0.8081\n",
      "Epoch 619/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5970 - accuracy: 0.7820\n",
      "Epoch 619: val_loss did not improve from 0.49015\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5970 - accuracy: 0.7820 - val_loss: 0.5132 - val_accuracy: 0.8166\n",
      "Epoch 620/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6104 - accuracy: 0.7788\n",
      "Epoch 620: val_loss did not improve from 0.49015\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6104 - accuracy: 0.7788 - val_loss: 0.5154 - val_accuracy: 0.8137\n",
      "Epoch 621/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7521 - accuracy: 0.7582\n",
      "Epoch 621: val_loss did not improve from 0.49015\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7521 - accuracy: 0.7582 - val_loss: 0.7395 - val_accuracy: 0.7526\n",
      "Epoch 622/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6578 - accuracy: 0.7633\n",
      "Epoch 622: val_loss did not improve from 0.49015\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6577 - accuracy: 0.7634 - val_loss: 0.5228 - val_accuracy: 0.8093\n",
      "Epoch 623/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6101 - accuracy: 0.7788\n",
      "Epoch 623: val_loss did not improve from 0.49015\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6102 - accuracy: 0.7788 - val_loss: 0.5183 - val_accuracy: 0.8131\n",
      "Epoch 624/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5980 - accuracy: 0.7823\n",
      "Epoch 624: val_loss did not improve from 0.49015\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5980 - accuracy: 0.7822 - val_loss: 0.5113 - val_accuracy: 0.8114\n",
      "Epoch 625/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6094 - accuracy: 0.7788\n",
      "Epoch 625: val_loss did not improve from 0.49015\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6091 - accuracy: 0.7788 - val_loss: 0.5616 - val_accuracy: 0.8020\n",
      "Epoch 626/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6050 - accuracy: 0.7794\n",
      "Epoch 626: val_loss did not improve from 0.49015\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6050 - accuracy: 0.7795 - val_loss: 0.5216 - val_accuracy: 0.8125\n",
      "Epoch 627/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6038 - accuracy: 0.7802\n",
      "Epoch 627: val_loss did not improve from 0.49015\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6038 - accuracy: 0.7803 - val_loss: 0.5498 - val_accuracy: 0.8096\n",
      "Epoch 628/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5999 - accuracy: 0.7814\n",
      "Epoch 628: val_loss did not improve from 0.49015\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5999 - accuracy: 0.7815 - val_loss: 0.5482 - val_accuracy: 0.8058\n",
      "Epoch 629/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5904 - accuracy: 0.7834\n",
      "Epoch 629: val_loss did not improve from 0.49015\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5902 - accuracy: 0.7834 - val_loss: 0.5304 - val_accuracy: 0.8121\n",
      "Epoch 630/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6095 - accuracy: 0.7785\n",
      "Epoch 630: val_loss did not improve from 0.49015\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6108 - accuracy: 0.7781 - val_loss: 0.8693 - val_accuracy: 0.7095\n",
      "Epoch 631/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6147 - accuracy: 0.7759\n",
      "Epoch 631: val_loss did not improve from 0.49015\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6147 - accuracy: 0.7759 - val_loss: 0.5439 - val_accuracy: 0.8064\n",
      "Epoch 632/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6054 - accuracy: 0.7798\n",
      "Epoch 632: val_loss did not improve from 0.49015\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6069 - accuracy: 0.7794 - val_loss: 1.0740 - val_accuracy: 0.6731\n",
      "Epoch 633/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6280 - accuracy: 0.7747\n",
      "Epoch 633: val_loss did not improve from 0.49015\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6281 - accuracy: 0.7747 - val_loss: 0.5330 - val_accuracy: 0.8092\n",
      "Epoch 634/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5927 - accuracy: 0.7831\n",
      "Epoch 634: val_loss did not improve from 0.49015\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5927 - accuracy: 0.7832 - val_loss: 0.5091 - val_accuracy: 0.8165\n",
      "Epoch 635/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5940 - accuracy: 0.7829\n",
      "Epoch 635: val_loss improved from 0.49015 to 0.48988, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5940 - accuracy: 0.7829 - val_loss: 0.4899 - val_accuracy: 0.8214\n",
      "Epoch 636/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6960 - accuracy: 0.7537\n",
      "Epoch 636: val_loss did not improve from 0.48988\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6963 - accuracy: 0.7534 - val_loss: 0.5783 - val_accuracy: 0.7957\n",
      "Epoch 637/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6116 - accuracy: 0.7777\n",
      "Epoch 637: val_loss did not improve from 0.48988\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6116 - accuracy: 0.7777 - val_loss: 0.5395 - val_accuracy: 0.8070\n",
      "Epoch 638/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6068 - accuracy: 0.7787\n",
      "Epoch 638: val_loss did not improve from 0.48988\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6068 - accuracy: 0.7787 - val_loss: 0.5315 - val_accuracy: 0.8075\n",
      "Epoch 639/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5911 - accuracy: 0.7841\n",
      "Epoch 639: val_loss did not improve from 0.48988\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5912 - accuracy: 0.7841 - val_loss: 0.5183 - val_accuracy: 0.8151\n",
      "Epoch 640/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5890 - accuracy: 0.7840\n",
      "Epoch 640: val_loss did not improve from 0.48988\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5889 - accuracy: 0.7841 - val_loss: 0.5128 - val_accuracy: 0.8169\n",
      "Epoch 641/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5947 - accuracy: 0.7815\n",
      "Epoch 641: val_loss did not improve from 0.48988\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5949 - accuracy: 0.7816 - val_loss: 0.5326 - val_accuracy: 0.8102\n",
      "Epoch 642/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5984 - accuracy: 0.7828\n",
      "Epoch 642: val_loss did not improve from 0.48988\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5990 - accuracy: 0.7826 - val_loss: 0.5406 - val_accuracy: 0.8064\n",
      "Epoch 643/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6038 - accuracy: 0.7807\n",
      "Epoch 643: val_loss did not improve from 0.48988\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6037 - accuracy: 0.7807 - val_loss: 0.5268 - val_accuracy: 0.8093\n",
      "Epoch 644/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6010 - accuracy: 0.7812\n",
      "Epoch 644: val_loss did not improve from 0.48988\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6010 - accuracy: 0.7812 - val_loss: 0.5186 - val_accuracy: 0.8125\n",
      "Epoch 645/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5949 - accuracy: 0.7841\n",
      "Epoch 645: val_loss did not improve from 0.48988\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5950 - accuracy: 0.7840 - val_loss: 0.5038 - val_accuracy: 0.8190\n",
      "Epoch 646/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5900 - accuracy: 0.7831\n",
      "Epoch 646: val_loss did not improve from 0.48988\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5901 - accuracy: 0.7830 - val_loss: 0.5006 - val_accuracy: 0.8140\n",
      "Epoch 647/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5893 - accuracy: 0.7840\n",
      "Epoch 647: val_loss did not improve from 0.48988\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5895 - accuracy: 0.7838 - val_loss: 0.4960 - val_accuracy: 0.8197\n",
      "Epoch 648/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6027 - accuracy: 0.7816\n",
      "Epoch 648: val_loss did not improve from 0.48988\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6026 - accuracy: 0.7816 - val_loss: 0.5032 - val_accuracy: 0.8158\n",
      "Epoch 649/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6432 - accuracy: 0.7687\n",
      "Epoch 649: val_loss did not improve from 0.48988\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6436 - accuracy: 0.7686 - val_loss: 0.5486 - val_accuracy: 0.8069\n",
      "Epoch 650/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6046 - accuracy: 0.7781\n",
      "Epoch 650: val_loss did not improve from 0.48988\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6046 - accuracy: 0.7781 - val_loss: 0.5141 - val_accuracy: 0.8115\n",
      "Epoch 651/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5938 - accuracy: 0.7829\n",
      "Epoch 651: val_loss did not improve from 0.48988\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5938 - accuracy: 0.7829 - val_loss: 0.5552 - val_accuracy: 0.8051\n",
      "Epoch 652/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5896 - accuracy: 0.7845\n",
      "Epoch 652: val_loss did not improve from 0.48988\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5898 - accuracy: 0.7844 - val_loss: 0.5059 - val_accuracy: 0.8145\n",
      "Epoch 653/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5985 - accuracy: 0.7816\n",
      "Epoch 653: val_loss did not improve from 0.48988\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5989 - accuracy: 0.7816 - val_loss: 0.5139 - val_accuracy: 0.8149\n",
      "Epoch 654/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6179 - accuracy: 0.7759\n",
      "Epoch 654: val_loss did not improve from 0.48988\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6176 - accuracy: 0.7760 - val_loss: 0.4908 - val_accuracy: 0.8259\n",
      "Epoch 655/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5930 - accuracy: 0.7839\n",
      "Epoch 655: val_loss did not improve from 0.48988\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5931 - accuracy: 0.7839 - val_loss: 0.5011 - val_accuracy: 0.8200\n",
      "Epoch 656/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6157 - accuracy: 0.7751\n",
      "Epoch 656: val_loss did not improve from 0.48988\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6156 - accuracy: 0.7752 - val_loss: 0.5092 - val_accuracy: 0.8143\n",
      "Epoch 657/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5978 - accuracy: 0.7814\n",
      "Epoch 657: val_loss did not improve from 0.48988\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5980 - accuracy: 0.7813 - val_loss: 0.5288 - val_accuracy: 0.8143\n",
      "Epoch 658/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5830 - accuracy: 0.7858\n",
      "Epoch 658: val_loss did not improve from 0.48988\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5835 - accuracy: 0.7856 - val_loss: 0.5430 - val_accuracy: 0.7982\n",
      "Epoch 659/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5850 - accuracy: 0.7874\n",
      "Epoch 659: val_loss did not improve from 0.48988\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5850 - accuracy: 0.7875 - val_loss: 0.5038 - val_accuracy: 0.8182\n",
      "Epoch 660/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5935 - accuracy: 0.7844\n",
      "Epoch 660: val_loss did not improve from 0.48988\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5934 - accuracy: 0.7844 - val_loss: 0.5215 - val_accuracy: 0.8149\n",
      "Epoch 661/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6031 - accuracy: 0.7824\n",
      "Epoch 661: val_loss did not improve from 0.48988\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6031 - accuracy: 0.7824 - val_loss: 0.5065 - val_accuracy: 0.8185\n",
      "Epoch 662/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6008 - accuracy: 0.7812\n",
      "Epoch 662: val_loss did not improve from 0.48988\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6008 - accuracy: 0.7812 - val_loss: 0.5483 - val_accuracy: 0.8049\n",
      "Epoch 663/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5906 - accuracy: 0.7845\n",
      "Epoch 663: val_loss did not improve from 0.48988\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5908 - accuracy: 0.7844 - val_loss: 0.5086 - val_accuracy: 0.8116\n",
      "Epoch 664/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6039 - accuracy: 0.7818\n",
      "Epoch 664: val_loss did not improve from 0.48988\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6040 - accuracy: 0.7818 - val_loss: 0.5263 - val_accuracy: 0.8065\n",
      "Epoch 665/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5897 - accuracy: 0.7838\n",
      "Epoch 665: val_loss did not improve from 0.48988\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5893 - accuracy: 0.7841 - val_loss: 0.5378 - val_accuracy: 0.8087\n",
      "Epoch 666/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5878 - accuracy: 0.7865\n",
      "Epoch 666: val_loss did not improve from 0.48988\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5878 - accuracy: 0.7865 - val_loss: 0.4994 - val_accuracy: 0.8188\n",
      "Epoch 667/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6036 - accuracy: 0.7817\n",
      "Epoch 667: val_loss did not improve from 0.48988\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6035 - accuracy: 0.7818 - val_loss: 0.5057 - val_accuracy: 0.8158\n",
      "Epoch 668/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6053 - accuracy: 0.7803\n",
      "Epoch 668: val_loss did not improve from 0.48988\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6054 - accuracy: 0.7802 - val_loss: 0.5068 - val_accuracy: 0.8171\n",
      "Epoch 669/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5966 - accuracy: 0.7827\n",
      "Epoch 669: val_loss did not improve from 0.48988\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5966 - accuracy: 0.7828 - val_loss: 0.5055 - val_accuracy: 0.8197\n",
      "Epoch 670/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5918 - accuracy: 0.7832\n",
      "Epoch 670: val_loss did not improve from 0.48988\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5918 - accuracy: 0.7832 - val_loss: 0.5182 - val_accuracy: 0.8132\n",
      "Epoch 671/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5943 - accuracy: 0.7827\n",
      "Epoch 671: val_loss did not improve from 0.48988\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5944 - accuracy: 0.7827 - val_loss: 0.5062 - val_accuracy: 0.8137\n",
      "Epoch 672/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5944 - accuracy: 0.7834\n",
      "Epoch 672: val_loss did not improve from 0.48988\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5945 - accuracy: 0.7834 - val_loss: 0.5200 - val_accuracy: 0.8101\n",
      "Epoch 673/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5955 - accuracy: 0.7841\n",
      "Epoch 673: val_loss did not improve from 0.48988\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5953 - accuracy: 0.7842 - val_loss: 0.5120 - val_accuracy: 0.8161\n",
      "Epoch 674/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5857 - accuracy: 0.7858\n",
      "Epoch 674: val_loss did not improve from 0.48988\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5857 - accuracy: 0.7858 - val_loss: 0.5301 - val_accuracy: 0.8118\n",
      "Epoch 675/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6083 - accuracy: 0.7784\n",
      "Epoch 675: val_loss did not improve from 0.48988\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6084 - accuracy: 0.7783 - val_loss: 0.5566 - val_accuracy: 0.7992\n",
      "Epoch 676/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5942 - accuracy: 0.7839\n",
      "Epoch 676: val_loss did not improve from 0.48988\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5943 - accuracy: 0.7838 - val_loss: 0.5021 - val_accuracy: 0.8154\n",
      "Epoch 677/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5927 - accuracy: 0.7843\n",
      "Epoch 677: val_loss did not improve from 0.48988\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5927 - accuracy: 0.7843 - val_loss: 0.5456 - val_accuracy: 0.8065\n",
      "Epoch 678/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5904 - accuracy: 0.7846\n",
      "Epoch 678: val_loss did not improve from 0.48988\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5899 - accuracy: 0.7848 - val_loss: 0.5197 - val_accuracy: 0.8138\n",
      "Epoch 679/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6150 - accuracy: 0.7775\n",
      "Epoch 679: val_loss did not improve from 0.48988\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6150 - accuracy: 0.7775 - val_loss: 0.5535 - val_accuracy: 0.8040\n",
      "Epoch 680/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6023 - accuracy: 0.7810\n",
      "Epoch 680: val_loss did not improve from 0.48988\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6022 - accuracy: 0.7810 - val_loss: 0.5143 - val_accuracy: 0.8167\n",
      "Epoch 681/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5875 - accuracy: 0.7862\n",
      "Epoch 681: val_loss did not improve from 0.48988\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5876 - accuracy: 0.7861 - val_loss: 0.5041 - val_accuracy: 0.8144\n",
      "Epoch 682/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6091 - accuracy: 0.7786\n",
      "Epoch 682: val_loss did not improve from 0.48988\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6092 - accuracy: 0.7785 - val_loss: 0.5269 - val_accuracy: 0.8060\n",
      "Epoch 683/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5830 - accuracy: 0.7879\n",
      "Epoch 683: val_loss did not improve from 0.48988\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5830 - accuracy: 0.7879 - val_loss: 0.5669 - val_accuracy: 0.7902\n",
      "Epoch 684/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6476 - accuracy: 0.7768\n",
      "Epoch 684: val_loss did not improve from 0.48988\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6481 - accuracy: 0.7768 - val_loss: 0.5427 - val_accuracy: 0.8104\n",
      "Epoch 685/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5940 - accuracy: 0.7833\n",
      "Epoch 685: val_loss did not improve from 0.48988\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5940 - accuracy: 0.7833 - val_loss: 0.5165 - val_accuracy: 0.8143\n",
      "Epoch 686/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5992 - accuracy: 0.7810\n",
      "Epoch 686: val_loss did not improve from 0.48988\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5992 - accuracy: 0.7811 - val_loss: 0.5095 - val_accuracy: 0.8154\n",
      "Epoch 687/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5885 - accuracy: 0.7856\n",
      "Epoch 687: val_loss improved from 0.48988 to 0.47263, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5887 - accuracy: 0.7854 - val_loss: 0.4726 - val_accuracy: 0.8291\n",
      "Epoch 688/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5864 - accuracy: 0.7868\n",
      "Epoch 688: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5863 - accuracy: 0.7869 - val_loss: 0.4838 - val_accuracy: 0.8245\n",
      "Epoch 689/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6012 - accuracy: 0.7818\n",
      "Epoch 689: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6012 - accuracy: 0.7818 - val_loss: 0.4964 - val_accuracy: 0.8221\n",
      "Epoch 690/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5944 - accuracy: 0.7836\n",
      "Epoch 690: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5940 - accuracy: 0.7837 - val_loss: 0.5112 - val_accuracy: 0.8156\n",
      "Epoch 691/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5873 - accuracy: 0.7874\n",
      "Epoch 691: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5871 - accuracy: 0.7875 - val_loss: 0.4922 - val_accuracy: 0.8208\n",
      "Epoch 692/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5799 - accuracy: 0.7870\n",
      "Epoch 692: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5804 - accuracy: 0.7868 - val_loss: 0.5027 - val_accuracy: 0.8150\n",
      "Epoch 693/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5846 - accuracy: 0.7850\n",
      "Epoch 693: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5846 - accuracy: 0.7850 - val_loss: 0.5118 - val_accuracy: 0.8180\n",
      "Epoch 694/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6141 - accuracy: 0.7793\n",
      "Epoch 694: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6140 - accuracy: 0.7793 - val_loss: 0.5430 - val_accuracy: 0.8074\n",
      "Epoch 695/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6164 - accuracy: 0.7792\n",
      "Epoch 695: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6164 - accuracy: 0.7792 - val_loss: 0.5413 - val_accuracy: 0.8073\n",
      "Epoch 696/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5912 - accuracy: 0.7854\n",
      "Epoch 696: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5914 - accuracy: 0.7854 - val_loss: 0.5255 - val_accuracy: 0.8035\n",
      "Epoch 697/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5817 - accuracy: 0.7879\n",
      "Epoch 697: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5821 - accuracy: 0.7879 - val_loss: 0.5255 - val_accuracy: 0.8130\n",
      "Epoch 698/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6110 - accuracy: 0.7798\n",
      "Epoch 698: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6109 - accuracy: 0.7797 - val_loss: 0.5688 - val_accuracy: 0.8020\n",
      "Epoch 699/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6009 - accuracy: 0.7833\n",
      "Epoch 699: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6011 - accuracy: 0.7833 - val_loss: 0.5131 - val_accuracy: 0.8135\n",
      "Epoch 700/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5939 - accuracy: 0.7844\n",
      "Epoch 700: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5937 - accuracy: 0.7844 - val_loss: 0.4950 - val_accuracy: 0.8216\n",
      "Epoch 701/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5922 - accuracy: 0.7854\n",
      "Epoch 701: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5923 - accuracy: 0.7854 - val_loss: 0.4863 - val_accuracy: 0.8237\n",
      "Epoch 702/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5902 - accuracy: 0.7859\n",
      "Epoch 702: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5902 - accuracy: 0.7859 - val_loss: 0.5032 - val_accuracy: 0.8189\n",
      "Epoch 703/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6007 - accuracy: 0.7822\n",
      "Epoch 703: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6006 - accuracy: 0.7822 - val_loss: 0.4954 - val_accuracy: 0.8227\n",
      "Epoch 704/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5831 - accuracy: 0.7877\n",
      "Epoch 704: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5835 - accuracy: 0.7875 - val_loss: 0.5070 - val_accuracy: 0.8124\n",
      "Epoch 705/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5990 - accuracy: 0.7824\n",
      "Epoch 705: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5990 - accuracy: 0.7824 - val_loss: 0.5220 - val_accuracy: 0.8131\n",
      "Epoch 706/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5987 - accuracy: 0.7838\n",
      "Epoch 706: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5988 - accuracy: 0.7838 - val_loss: 0.4990 - val_accuracy: 0.8229\n",
      "Epoch 707/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5876 - accuracy: 0.7871\n",
      "Epoch 707: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5878 - accuracy: 0.7871 - val_loss: 0.4976 - val_accuracy: 0.8227\n",
      "Epoch 708/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5836 - accuracy: 0.7862\n",
      "Epoch 708: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5837 - accuracy: 0.7862 - val_loss: 0.6019 - val_accuracy: 0.7881\n",
      "Epoch 709/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6138 - accuracy: 0.7808\n",
      "Epoch 709: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6138 - accuracy: 0.7807 - val_loss: 0.5110 - val_accuracy: 0.8186\n",
      "Epoch 710/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6065 - accuracy: 0.7816\n",
      "Epoch 710: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6065 - accuracy: 0.7816 - val_loss: 0.5377 - val_accuracy: 0.8056\n",
      "Epoch 711/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6067 - accuracy: 0.7804\n",
      "Epoch 711: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6068 - accuracy: 0.7804 - val_loss: 0.5366 - val_accuracy: 0.8134\n",
      "Epoch 712/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5793 - accuracy: 0.7891\n",
      "Epoch 712: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5789 - accuracy: 0.7893 - val_loss: 0.5156 - val_accuracy: 0.8130\n",
      "Epoch 713/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5851 - accuracy: 0.7870\n",
      "Epoch 713: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5852 - accuracy: 0.7870 - val_loss: 0.5504 - val_accuracy: 0.8009\n",
      "Epoch 714/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6015 - accuracy: 0.7820\n",
      "Epoch 714: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6015 - accuracy: 0.7820 - val_loss: 0.5015 - val_accuracy: 0.8229\n",
      "Epoch 715/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5841 - accuracy: 0.7872\n",
      "Epoch 715: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5844 - accuracy: 0.7870 - val_loss: 0.5230 - val_accuracy: 0.8074\n",
      "Epoch 716/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5777 - accuracy: 0.7899\n",
      "Epoch 716: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5779 - accuracy: 0.7898 - val_loss: 0.5176 - val_accuracy: 0.8148\n",
      "Epoch 717/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5938 - accuracy: 0.7846\n",
      "Epoch 717: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5937 - accuracy: 0.7846 - val_loss: 0.4930 - val_accuracy: 0.8186\n",
      "Epoch 718/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5736 - accuracy: 0.7918\n",
      "Epoch 718: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5736 - accuracy: 0.7918 - val_loss: 0.5583 - val_accuracy: 0.8052\n",
      "Epoch 719/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5950 - accuracy: 0.7842\n",
      "Epoch 719: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5950 - accuracy: 0.7842 - val_loss: 0.5593 - val_accuracy: 0.8038\n",
      "Epoch 720/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5808 - accuracy: 0.7885\n",
      "Epoch 720: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5808 - accuracy: 0.7885 - val_loss: 0.4906 - val_accuracy: 0.8248\n",
      "Epoch 721/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6143 - accuracy: 0.7781\n",
      "Epoch 721: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6143 - accuracy: 0.7781 - val_loss: 0.5156 - val_accuracy: 0.8145\n",
      "Epoch 722/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5837 - accuracy: 0.7881\n",
      "Epoch 722: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5837 - accuracy: 0.7881 - val_loss: 0.5407 - val_accuracy: 0.8073\n",
      "Epoch 723/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5937 - accuracy: 0.7837\n",
      "Epoch 723: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5935 - accuracy: 0.7838 - val_loss: 0.5012 - val_accuracy: 0.8156\n",
      "Epoch 724/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5851 - accuracy: 0.7872\n",
      "Epoch 724: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5851 - accuracy: 0.7872 - val_loss: 0.5155 - val_accuracy: 0.8134\n",
      "Epoch 725/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5773 - accuracy: 0.7890\n",
      "Epoch 725: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5776 - accuracy: 0.7889 - val_loss: 0.5164 - val_accuracy: 0.8109\n",
      "Epoch 726/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5854 - accuracy: 0.7882\n",
      "Epoch 726: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5855 - accuracy: 0.7883 - val_loss: 0.5193 - val_accuracy: 0.8145\n",
      "Epoch 727/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.7720 - accuracy: 0.7743\n",
      "Epoch 727: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7716 - accuracy: 0.7743 - val_loss: 0.5038 - val_accuracy: 0.8186\n",
      "Epoch 728/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5909 - accuracy: 0.7869\n",
      "Epoch 728: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5909 - accuracy: 0.7870 - val_loss: 0.5346 - val_accuracy: 0.8065\n",
      "Epoch 729/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6046 - accuracy: 0.7842\n",
      "Epoch 729: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6045 - accuracy: 0.7841 - val_loss: 0.5224 - val_accuracy: 0.8103\n",
      "Epoch 730/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5825 - accuracy: 0.7882\n",
      "Epoch 730: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5825 - accuracy: 0.7882 - val_loss: 0.5029 - val_accuracy: 0.8136\n",
      "Epoch 731/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5862 - accuracy: 0.7885\n",
      "Epoch 731: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5864 - accuracy: 0.7883 - val_loss: 0.5478 - val_accuracy: 0.8047\n",
      "Epoch 732/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5893 - accuracy: 0.7869\n",
      "Epoch 732: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5899 - accuracy: 0.7868 - val_loss: 0.5188 - val_accuracy: 0.8138\n",
      "Epoch 733/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6051 - accuracy: 0.7817\n",
      "Epoch 733: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6050 - accuracy: 0.7818 - val_loss: 0.5523 - val_accuracy: 0.8010\n",
      "Epoch 734/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5907 - accuracy: 0.7867\n",
      "Epoch 734: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5907 - accuracy: 0.7867 - val_loss: 0.5249 - val_accuracy: 0.8090\n",
      "Epoch 735/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5853 - accuracy: 0.7872\n",
      "Epoch 735: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5853 - accuracy: 0.7872 - val_loss: 0.4807 - val_accuracy: 0.8260\n",
      "Epoch 736/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5855 - accuracy: 0.7884\n",
      "Epoch 736: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5851 - accuracy: 0.7886 - val_loss: 0.4868 - val_accuracy: 0.8246\n",
      "Epoch 737/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5868 - accuracy: 0.7874\n",
      "Epoch 737: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5867 - accuracy: 0.7875 - val_loss: 0.5168 - val_accuracy: 0.8112\n",
      "Epoch 738/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5822 - accuracy: 0.7875\n",
      "Epoch 738: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5821 - accuracy: 0.7875 - val_loss: 0.5000 - val_accuracy: 0.8186\n",
      "Epoch 739/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6019 - accuracy: 0.7849\n",
      "Epoch 739: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6019 - accuracy: 0.7849 - val_loss: 0.5261 - val_accuracy: 0.8155\n",
      "Epoch 740/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5865 - accuracy: 0.7870\n",
      "Epoch 740: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5865 - accuracy: 0.7870 - val_loss: 0.5510 - val_accuracy: 0.8050\n",
      "Epoch 741/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6353 - accuracy: 0.7739\n",
      "Epoch 741: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6353 - accuracy: 0.7738 - val_loss: 0.4979 - val_accuracy: 0.8249\n",
      "Epoch 742/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5855 - accuracy: 0.7860\n",
      "Epoch 742: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5855 - accuracy: 0.7860 - val_loss: 0.5063 - val_accuracy: 0.8169\n",
      "Epoch 743/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5858 - accuracy: 0.7877\n",
      "Epoch 743: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5858 - accuracy: 0.7877 - val_loss: 0.5179 - val_accuracy: 0.8122\n",
      "Epoch 744/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5819 - accuracy: 0.7889\n",
      "Epoch 744: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5819 - accuracy: 0.7889 - val_loss: 0.5000 - val_accuracy: 0.8190\n",
      "Epoch 745/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5909 - accuracy: 0.7863\n",
      "Epoch 745: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5910 - accuracy: 0.7861 - val_loss: 0.5348 - val_accuracy: 0.8093\n",
      "Epoch 746/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5886 - accuracy: 0.7862\n",
      "Epoch 746: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5886 - accuracy: 0.7862 - val_loss: 0.5338 - val_accuracy: 0.8038\n",
      "Epoch 747/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5900 - accuracy: 0.7852\n",
      "Epoch 747: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5899 - accuracy: 0.7852 - val_loss: 0.5308 - val_accuracy: 0.8121\n",
      "Epoch 748/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5867 - accuracy: 0.7869\n",
      "Epoch 748: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5867 - accuracy: 0.7869 - val_loss: 0.4963 - val_accuracy: 0.8259\n",
      "Epoch 749/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5763 - accuracy: 0.7897\n",
      "Epoch 749: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5764 - accuracy: 0.7897 - val_loss: 0.5106 - val_accuracy: 0.8204\n",
      "Epoch 750/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5933 - accuracy: 0.7856\n",
      "Epoch 750: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5933 - accuracy: 0.7856 - val_loss: 0.5310 - val_accuracy: 0.8064\n",
      "Epoch 751/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5990 - accuracy: 0.7834\n",
      "Epoch 751: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5990 - accuracy: 0.7834 - val_loss: 0.4907 - val_accuracy: 0.8240\n",
      "Epoch 752/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5891 - accuracy: 0.7867\n",
      "Epoch 752: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5891 - accuracy: 0.7866 - val_loss: 0.4920 - val_accuracy: 0.8203\n",
      "Epoch 753/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5869 - accuracy: 0.7876\n",
      "Epoch 753: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5869 - accuracy: 0.7876 - val_loss: 0.5127 - val_accuracy: 0.8143\n",
      "Epoch 754/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5858 - accuracy: 0.7882\n",
      "Epoch 754: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5858 - accuracy: 0.7882 - val_loss: 0.4808 - val_accuracy: 0.8222\n",
      "Epoch 755/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5824 - accuracy: 0.7884\n",
      "Epoch 755: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5824 - accuracy: 0.7884 - val_loss: 0.5045 - val_accuracy: 0.8184\n",
      "Epoch 756/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5984 - accuracy: 0.7833\n",
      "Epoch 756: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5983 - accuracy: 0.7834 - val_loss: 0.4877 - val_accuracy: 0.8190\n",
      "Epoch 757/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5810 - accuracy: 0.7896\n",
      "Epoch 757: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5812 - accuracy: 0.7894 - val_loss: 0.5588 - val_accuracy: 0.8090\n",
      "Epoch 758/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5783 - accuracy: 0.7905\n",
      "Epoch 758: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5782 - accuracy: 0.7905 - val_loss: 0.5096 - val_accuracy: 0.8155\n",
      "Epoch 759/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5827 - accuracy: 0.7874\n",
      "Epoch 759: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5825 - accuracy: 0.7875 - val_loss: 0.4991 - val_accuracy: 0.8221\n",
      "Epoch 760/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5888 - accuracy: 0.7865\n",
      "Epoch 760: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5888 - accuracy: 0.7865 - val_loss: 0.4797 - val_accuracy: 0.8286\n",
      "Epoch 761/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5837 - accuracy: 0.7883\n",
      "Epoch 761: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5835 - accuracy: 0.7882 - val_loss: 0.5266 - val_accuracy: 0.8096\n",
      "Epoch 762/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5891 - accuracy: 0.7860\n",
      "Epoch 762: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5891 - accuracy: 0.7859 - val_loss: 0.5282 - val_accuracy: 0.8106\n",
      "Epoch 763/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6088 - accuracy: 0.7827\n",
      "Epoch 763: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6091 - accuracy: 0.7826 - val_loss: 0.5270 - val_accuracy: 0.8128\n",
      "Epoch 764/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6134 - accuracy: 0.7801\n",
      "Epoch 764: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6134 - accuracy: 0.7801 - val_loss: 0.5093 - val_accuracy: 0.8185\n",
      "Epoch 765/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5862 - accuracy: 0.7887\n",
      "Epoch 765: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5861 - accuracy: 0.7886 - val_loss: 0.5068 - val_accuracy: 0.8233\n",
      "Epoch 766/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5839 - accuracy: 0.7874\n",
      "Epoch 766: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5843 - accuracy: 0.7873 - val_loss: 0.5199 - val_accuracy: 0.8088\n",
      "Epoch 767/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5847 - accuracy: 0.7883\n",
      "Epoch 767: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5846 - accuracy: 0.7883 - val_loss: 0.4800 - val_accuracy: 0.8240\n",
      "Epoch 768/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5924 - accuracy: 0.7855\n",
      "Epoch 768: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5923 - accuracy: 0.7857 - val_loss: 0.5097 - val_accuracy: 0.8175\n",
      "Epoch 769/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6022 - accuracy: 0.7831\n",
      "Epoch 769: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6022 - accuracy: 0.7831 - val_loss: 0.6665 - val_accuracy: 0.7757\n",
      "Epoch 770/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5906 - accuracy: 0.7854\n",
      "Epoch 770: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5908 - accuracy: 0.7853 - val_loss: 0.4989 - val_accuracy: 0.8163\n",
      "Epoch 771/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5992 - accuracy: 0.7853\n",
      "Epoch 771: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5992 - accuracy: 0.7853 - val_loss: 0.5059 - val_accuracy: 0.8143\n",
      "Epoch 772/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5828 - accuracy: 0.7883\n",
      "Epoch 772: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5826 - accuracy: 0.7883 - val_loss: 0.4807 - val_accuracy: 0.8243\n",
      "Epoch 773/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5767 - accuracy: 0.7911\n",
      "Epoch 773: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5767 - accuracy: 0.7911 - val_loss: 0.5029 - val_accuracy: 0.8155\n",
      "Epoch 774/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5982 - accuracy: 0.7836\n",
      "Epoch 774: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5983 - accuracy: 0.7836 - val_loss: 0.5609 - val_accuracy: 0.7986\n",
      "Epoch 775/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5858 - accuracy: 0.7872\n",
      "Epoch 775: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5858 - accuracy: 0.7873 - val_loss: 0.4925 - val_accuracy: 0.8238\n",
      "Epoch 776/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6032 - accuracy: 0.7846\n",
      "Epoch 776: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6035 - accuracy: 0.7844 - val_loss: 0.5476 - val_accuracy: 0.8015\n",
      "Epoch 777/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5897 - accuracy: 0.7867\n",
      "Epoch 777: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5898 - accuracy: 0.7867 - val_loss: 0.5084 - val_accuracy: 0.8230\n",
      "Epoch 778/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5825 - accuracy: 0.7879\n",
      "Epoch 778: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5823 - accuracy: 0.7880 - val_loss: 0.5207 - val_accuracy: 0.8166\n",
      "Epoch 779/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5946 - accuracy: 0.7837\n",
      "Epoch 779: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5943 - accuracy: 0.7838 - val_loss: 0.5152 - val_accuracy: 0.8167\n",
      "Epoch 780/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6214 - accuracy: 0.7800\n",
      "Epoch 780: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6214 - accuracy: 0.7800 - val_loss: 0.8365 - val_accuracy: 0.7220\n",
      "Epoch 781/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.8516 - accuracy: 0.7018\n",
      "Epoch 781: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8507 - accuracy: 0.7021 - val_loss: 0.5499 - val_accuracy: 0.7995\n",
      "Epoch 782/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5927 - accuracy: 0.7847\n",
      "Epoch 782: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5927 - accuracy: 0.7847 - val_loss: 0.4868 - val_accuracy: 0.8242\n",
      "Epoch 783/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5705 - accuracy: 0.7923\n",
      "Epoch 783: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5705 - accuracy: 0.7923 - val_loss: 0.5088 - val_accuracy: 0.8150\n",
      "Epoch 784/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5719 - accuracy: 0.7918\n",
      "Epoch 784: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5719 - accuracy: 0.7918 - val_loss: 0.4900 - val_accuracy: 0.8199\n",
      "Epoch 785/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5767 - accuracy: 0.7909\n",
      "Epoch 785: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5769 - accuracy: 0.7907 - val_loss: 0.5342 - val_accuracy: 0.8080\n",
      "Epoch 786/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6097 - accuracy: 0.7819\n",
      "Epoch 786: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6094 - accuracy: 0.7820 - val_loss: 0.4827 - val_accuracy: 0.8244\n",
      "Epoch 787/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5868 - accuracy: 0.7878\n",
      "Epoch 787: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5868 - accuracy: 0.7878 - val_loss: 0.5028 - val_accuracy: 0.8201\n",
      "Epoch 788/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5814 - accuracy: 0.7890\n",
      "Epoch 788: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5814 - accuracy: 0.7890 - val_loss: 0.4861 - val_accuracy: 0.8224\n",
      "Epoch 789/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5776 - accuracy: 0.7902\n",
      "Epoch 789: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5776 - accuracy: 0.7901 - val_loss: 0.5249 - val_accuracy: 0.8141\n",
      "Epoch 790/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5812 - accuracy: 0.7903\n",
      "Epoch 790: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5813 - accuracy: 0.7903 - val_loss: 0.5811 - val_accuracy: 0.7958\n",
      "Epoch 791/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5876 - accuracy: 0.7870\n",
      "Epoch 791: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5875 - accuracy: 0.7872 - val_loss: 0.5039 - val_accuracy: 0.8130\n",
      "Epoch 792/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.9233 - accuracy: 0.6915\n",
      "Epoch 792: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9230 - accuracy: 0.6915 - val_loss: 0.7470 - val_accuracy: 0.7413\n",
      "Epoch 793/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7169 - accuracy: 0.7449\n",
      "Epoch 793: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7170 - accuracy: 0.7448 - val_loss: 0.5771 - val_accuracy: 0.7928\n",
      "Epoch 794/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6233 - accuracy: 0.7766\n",
      "Epoch 794: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6233 - accuracy: 0.7766 - val_loss: 0.5036 - val_accuracy: 0.8155\n",
      "Epoch 795/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6038 - accuracy: 0.7818\n",
      "Epoch 795: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6037 - accuracy: 0.7820 - val_loss: 0.5138 - val_accuracy: 0.8189\n",
      "Epoch 796/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5966 - accuracy: 0.7838\n",
      "Epoch 796: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5966 - accuracy: 0.7838 - val_loss: 0.5073 - val_accuracy: 0.8079\n",
      "Epoch 797/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5841 - accuracy: 0.7876\n",
      "Epoch 797: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5841 - accuracy: 0.7876 - val_loss: 0.5264 - val_accuracy: 0.8047\n",
      "Epoch 798/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6013 - accuracy: 0.7841\n",
      "Epoch 798: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6014 - accuracy: 0.7841 - val_loss: 0.5454 - val_accuracy: 0.8058\n",
      "Epoch 799/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5939 - accuracy: 0.7854\n",
      "Epoch 799: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5939 - accuracy: 0.7854 - val_loss: 0.5475 - val_accuracy: 0.8095\n",
      "Epoch 800/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5822 - accuracy: 0.7889\n",
      "Epoch 800: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5822 - accuracy: 0.7889 - val_loss: 0.5271 - val_accuracy: 0.8087\n",
      "Epoch 801/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5850 - accuracy: 0.7882\n",
      "Epoch 801: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5850 - accuracy: 0.7882 - val_loss: 0.5114 - val_accuracy: 0.8192\n",
      "Epoch 802/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5998 - accuracy: 0.7860\n",
      "Epoch 802: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6000 - accuracy: 0.7860 - val_loss: 0.4982 - val_accuracy: 0.8169\n",
      "Epoch 803/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5849 - accuracy: 0.7885\n",
      "Epoch 803: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5851 - accuracy: 0.7885 - val_loss: 0.4786 - val_accuracy: 0.8270\n",
      "Epoch 804/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5829 - accuracy: 0.7898\n",
      "Epoch 804: val_loss did not improve from 0.47263\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5830 - accuracy: 0.7898 - val_loss: 0.5110 - val_accuracy: 0.8185\n",
      "Epoch 805/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5720 - accuracy: 0.7912\n",
      "Epoch 805: val_loss improved from 0.47263 to 0.47219, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5721 - accuracy: 0.7912 - val_loss: 0.4722 - val_accuracy: 0.8289\n",
      "Epoch 806/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6079 - accuracy: 0.7838\n",
      "Epoch 806: val_loss did not improve from 0.47219\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6081 - accuracy: 0.7836 - val_loss: 0.5334 - val_accuracy: 0.8065\n",
      "Epoch 807/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5920 - accuracy: 0.7871\n",
      "Epoch 807: val_loss did not improve from 0.47219\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5920 - accuracy: 0.7871 - val_loss: 0.5559 - val_accuracy: 0.8000\n",
      "Epoch 808/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5948 - accuracy: 0.7853\n",
      "Epoch 808: val_loss did not improve from 0.47219\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5948 - accuracy: 0.7853 - val_loss: 0.6983 - val_accuracy: 0.7590\n",
      "Epoch 809/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6054 - accuracy: 0.7855\n",
      "Epoch 809: val_loss did not improve from 0.47219\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6055 - accuracy: 0.7855 - val_loss: 0.5398 - val_accuracy: 0.8041\n",
      "Epoch 810/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6037 - accuracy: 0.7842\n",
      "Epoch 810: val_loss did not improve from 0.47219\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6033 - accuracy: 0.7844 - val_loss: 0.4939 - val_accuracy: 0.8208\n",
      "Epoch 811/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5942 - accuracy: 0.7842\n",
      "Epoch 811: val_loss did not improve from 0.47219\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5942 - accuracy: 0.7842 - val_loss: 0.4765 - val_accuracy: 0.8237\n",
      "Epoch 812/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5882 - accuracy: 0.7876\n",
      "Epoch 812: val_loss did not improve from 0.47219\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5881 - accuracy: 0.7876 - val_loss: 0.4913 - val_accuracy: 0.8244\n",
      "Epoch 813/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5877 - accuracy: 0.7862\n",
      "Epoch 813: val_loss did not improve from 0.47219\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5875 - accuracy: 0.7863 - val_loss: 0.5121 - val_accuracy: 0.8151\n",
      "Epoch 814/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5899 - accuracy: 0.7862\n",
      "Epoch 814: val_loss did not improve from 0.47219\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5899 - accuracy: 0.7862 - val_loss: 0.4863 - val_accuracy: 0.8217\n",
      "Epoch 815/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5854 - accuracy: 0.7864\n",
      "Epoch 815: val_loss did not improve from 0.47219\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5855 - accuracy: 0.7863 - val_loss: 0.5082 - val_accuracy: 0.8176\n",
      "Epoch 816/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5874 - accuracy: 0.7863\n",
      "Epoch 816: val_loss did not improve from 0.47219\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5872 - accuracy: 0.7864 - val_loss: 0.5065 - val_accuracy: 0.8184\n",
      "Epoch 817/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5775 - accuracy: 0.7916\n",
      "Epoch 817: val_loss did not improve from 0.47219\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5776 - accuracy: 0.7917 - val_loss: 0.5358 - val_accuracy: 0.8041\n",
      "Epoch 818/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5775 - accuracy: 0.7901\n",
      "Epoch 818: val_loss did not improve from 0.47219\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5779 - accuracy: 0.7900 - val_loss: 0.5911 - val_accuracy: 0.7899\n",
      "Epoch 819/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.8115 - accuracy: 0.7098\n",
      "Epoch 819: val_loss did not improve from 0.47219\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8119 - accuracy: 0.7097 - val_loss: 1.0058 - val_accuracy: 0.6301\n",
      "Epoch 820/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.7381 - accuracy: 0.7360\n",
      "Epoch 820: val_loss did not improve from 0.47219\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7374 - accuracy: 0.7362 - val_loss: 0.5062 - val_accuracy: 0.8137\n",
      "Epoch 821/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5847 - accuracy: 0.7888\n",
      "Epoch 821: val_loss did not improve from 0.47219\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5855 - accuracy: 0.7887 - val_loss: 0.5216 - val_accuracy: 0.8141\n",
      "Epoch 822/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5892 - accuracy: 0.7876\n",
      "Epoch 822: val_loss did not improve from 0.47219\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5891 - accuracy: 0.7877 - val_loss: 0.5171 - val_accuracy: 0.8125\n",
      "Epoch 823/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5722 - accuracy: 0.7925\n",
      "Epoch 823: val_loss did not improve from 0.47219\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5725 - accuracy: 0.7924 - val_loss: 0.4999 - val_accuracy: 0.8180\n",
      "Epoch 824/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5821 - accuracy: 0.7905\n",
      "Epoch 824: val_loss did not improve from 0.47219\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5822 - accuracy: 0.7904 - val_loss: 0.6203 - val_accuracy: 0.7879\n",
      "Epoch 825/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6025 - accuracy: 0.7851\n",
      "Epoch 825: val_loss did not improve from 0.47219\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6026 - accuracy: 0.7850 - val_loss: 0.5518 - val_accuracy: 0.8078\n",
      "Epoch 826/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5848 - accuracy: 0.7890\n",
      "Epoch 826: val_loss did not improve from 0.47219\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5848 - accuracy: 0.7890 - val_loss: 0.4900 - val_accuracy: 0.8248\n",
      "Epoch 827/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5876 - accuracy: 0.7880\n",
      "Epoch 827: val_loss did not improve from 0.47219\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5876 - accuracy: 0.7880 - val_loss: 0.4820 - val_accuracy: 0.8229\n",
      "Epoch 828/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5866 - accuracy: 0.7888\n",
      "Epoch 828: val_loss did not improve from 0.47219\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5865 - accuracy: 0.7888 - val_loss: 0.4905 - val_accuracy: 0.8279\n",
      "Epoch 829/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5936 - accuracy: 0.7861\n",
      "Epoch 829: val_loss did not improve from 0.47219\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5935 - accuracy: 0.7861 - val_loss: 0.4967 - val_accuracy: 0.8242\n",
      "Epoch 830/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5971 - accuracy: 0.7851\n",
      "Epoch 830: val_loss did not improve from 0.47219\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5970 - accuracy: 0.7851 - val_loss: 0.5016 - val_accuracy: 0.8195\n",
      "Epoch 831/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5800 - accuracy: 0.7899\n",
      "Epoch 831: val_loss did not improve from 0.47219\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5799 - accuracy: 0.7900 - val_loss: 0.4790 - val_accuracy: 0.8213\n",
      "Epoch 832/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5895 - accuracy: 0.7877\n",
      "Epoch 832: val_loss did not improve from 0.47219\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5897 - accuracy: 0.7875 - val_loss: 0.4830 - val_accuracy: 0.8294\n",
      "Epoch 833/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5790 - accuracy: 0.7909\n",
      "Epoch 833: val_loss did not improve from 0.47219\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5794 - accuracy: 0.7908 - val_loss: 0.4934 - val_accuracy: 0.8204\n",
      "Epoch 834/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5784 - accuracy: 0.7908\n",
      "Epoch 834: val_loss did not improve from 0.47219\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5783 - accuracy: 0.7908 - val_loss: 0.5164 - val_accuracy: 0.8151\n",
      "Epoch 835/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5738 - accuracy: 0.7927\n",
      "Epoch 835: val_loss did not improve from 0.47219\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5737 - accuracy: 0.7927 - val_loss: 0.5273 - val_accuracy: 0.8089\n",
      "Epoch 836/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5782 - accuracy: 0.7915\n",
      "Epoch 836: val_loss did not improve from 0.47219\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5782 - accuracy: 0.7916 - val_loss: 0.4755 - val_accuracy: 0.8251\n",
      "Epoch 837/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5821 - accuracy: 0.7903\n",
      "Epoch 837: val_loss did not improve from 0.47219\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5821 - accuracy: 0.7903 - val_loss: 0.5553 - val_accuracy: 0.7977\n",
      "Epoch 838/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5867 - accuracy: 0.7874\n",
      "Epoch 838: val_loss did not improve from 0.47219\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5865 - accuracy: 0.7874 - val_loss: 0.5220 - val_accuracy: 0.8084\n",
      "Epoch 839/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5789 - accuracy: 0.7908\n",
      "Epoch 839: val_loss did not improve from 0.47219\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5787 - accuracy: 0.7909 - val_loss: 0.5008 - val_accuracy: 0.8175\n",
      "Epoch 840/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5856 - accuracy: 0.7889\n",
      "Epoch 840: val_loss did not improve from 0.47219\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5857 - accuracy: 0.7888 - val_loss: 0.5341 - val_accuracy: 0.8105\n",
      "Epoch 841/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5771 - accuracy: 0.7913\n",
      "Epoch 841: val_loss did not improve from 0.47219\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5771 - accuracy: 0.7913 - val_loss: 0.5530 - val_accuracy: 0.8025\n",
      "Epoch 842/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5718 - accuracy: 0.7919\n",
      "Epoch 842: val_loss did not improve from 0.47219\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5717 - accuracy: 0.7918 - val_loss: 0.4949 - val_accuracy: 0.8276\n",
      "Epoch 843/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6075 - accuracy: 0.7830\n",
      "Epoch 843: val_loss did not improve from 0.47219\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6070 - accuracy: 0.7831 - val_loss: 0.4760 - val_accuracy: 0.8238\n",
      "Epoch 844/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5808 - accuracy: 0.7901\n",
      "Epoch 844: val_loss did not improve from 0.47219\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5813 - accuracy: 0.7899 - val_loss: 0.5199 - val_accuracy: 0.8126\n",
      "Epoch 845/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6228 - accuracy: 0.7756\n",
      "Epoch 845: val_loss did not improve from 0.47219\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6227 - accuracy: 0.7756 - val_loss: 0.5161 - val_accuracy: 0.8161\n",
      "Epoch 846/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5811 - accuracy: 0.7903\n",
      "Epoch 846: val_loss did not improve from 0.47219\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5811 - accuracy: 0.7903 - val_loss: 0.5617 - val_accuracy: 0.8018\n",
      "Epoch 847/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5737 - accuracy: 0.7913\n",
      "Epoch 847: val_loss did not improve from 0.47219\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5738 - accuracy: 0.7912 - val_loss: 0.4877 - val_accuracy: 0.8206\n",
      "Epoch 848/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5873 - accuracy: 0.7891\n",
      "Epoch 848: val_loss did not improve from 0.47219\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5871 - accuracy: 0.7892 - val_loss: 0.5060 - val_accuracy: 0.8211\n",
      "Epoch 849/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5890 - accuracy: 0.7875\n",
      "Epoch 849: val_loss did not improve from 0.47219\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5895 - accuracy: 0.7874 - val_loss: 0.5453 - val_accuracy: 0.8067\n",
      "Epoch 850/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5758 - accuracy: 0.7915\n",
      "Epoch 850: val_loss did not improve from 0.47219\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5758 - accuracy: 0.7915 - val_loss: 0.4858 - val_accuracy: 0.8260\n",
      "Epoch 851/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5780 - accuracy: 0.7906\n",
      "Epoch 851: val_loss did not improve from 0.47219\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5782 - accuracy: 0.7906 - val_loss: 0.4852 - val_accuracy: 0.8232\n",
      "Epoch 852/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6020 - accuracy: 0.7849\n",
      "Epoch 852: val_loss did not improve from 0.47219\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6022 - accuracy: 0.7848 - val_loss: 0.5645 - val_accuracy: 0.8026\n",
      "Epoch 853/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5790 - accuracy: 0.7888\n",
      "Epoch 853: val_loss did not improve from 0.47219\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5791 - accuracy: 0.7888 - val_loss: 0.5067 - val_accuracy: 0.8221\n",
      "Epoch 854/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5927 - accuracy: 0.7864\n",
      "Epoch 854: val_loss did not improve from 0.47219\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5924 - accuracy: 0.7865 - val_loss: 0.5076 - val_accuracy: 0.8157\n",
      "Epoch 855/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6322 - accuracy: 0.7789\n",
      "Epoch 855: val_loss did not improve from 0.47219\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6317 - accuracy: 0.7790 - val_loss: 0.5094 - val_accuracy: 0.8165\n",
      "Epoch 856/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6056 - accuracy: 0.7808\n",
      "Epoch 856: val_loss did not improve from 0.47219\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6057 - accuracy: 0.7807 - val_loss: 0.4917 - val_accuracy: 0.8193\n",
      "Epoch 857/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5815 - accuracy: 0.7897\n",
      "Epoch 857: val_loss did not improve from 0.47219\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5814 - accuracy: 0.7898 - val_loss: 0.4899 - val_accuracy: 0.8248\n",
      "Epoch 858/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5848 - accuracy: 0.7894\n",
      "Epoch 858: val_loss did not improve from 0.47219\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5848 - accuracy: 0.7894 - val_loss: 0.4950 - val_accuracy: 0.8210\n",
      "Epoch 859/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5745 - accuracy: 0.7921\n",
      "Epoch 859: val_loss did not improve from 0.47219\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5742 - accuracy: 0.7922 - val_loss: 0.5082 - val_accuracy: 0.8233\n",
      "Epoch 860/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5744 - accuracy: 0.7925\n",
      "Epoch 860: val_loss improved from 0.47219 to 0.47066, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5744 - accuracy: 0.7925 - val_loss: 0.4707 - val_accuracy: 0.8264\n",
      "Epoch 861/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.7644 - accuracy: 0.7426\n",
      "Epoch 861: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7640 - accuracy: 0.7428 - val_loss: 0.5644 - val_accuracy: 0.8097\n",
      "Epoch 862/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6084 - accuracy: 0.7817\n",
      "Epoch 862: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6081 - accuracy: 0.7818 - val_loss: 0.4900 - val_accuracy: 0.8211\n",
      "Epoch 863/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5847 - accuracy: 0.7890\n",
      "Epoch 863: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5847 - accuracy: 0.7890 - val_loss: 0.5519 - val_accuracy: 0.8052\n",
      "Epoch 864/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5865 - accuracy: 0.7890\n",
      "Epoch 864: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5867 - accuracy: 0.7889 - val_loss: 0.5238 - val_accuracy: 0.8140\n",
      "Epoch 865/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5845 - accuracy: 0.7900\n",
      "Epoch 865: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5849 - accuracy: 0.7899 - val_loss: 0.4734 - val_accuracy: 0.8265\n",
      "Epoch 866/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5878 - accuracy: 0.7873\n",
      "Epoch 866: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5876 - accuracy: 0.7874 - val_loss: 0.4991 - val_accuracy: 0.8191\n",
      "Epoch 867/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5753 - accuracy: 0.7916\n",
      "Epoch 867: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5754 - accuracy: 0.7916 - val_loss: 0.4998 - val_accuracy: 0.8221\n",
      "Epoch 868/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5836 - accuracy: 0.7897\n",
      "Epoch 868: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5839 - accuracy: 0.7896 - val_loss: 0.4970 - val_accuracy: 0.8170\n",
      "Epoch 869/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5848 - accuracy: 0.7903\n",
      "Epoch 869: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5849 - accuracy: 0.7904 - val_loss: 0.5130 - val_accuracy: 0.8146\n",
      "Epoch 870/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5952 - accuracy: 0.7861\n",
      "Epoch 870: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5955 - accuracy: 0.7860 - val_loss: 0.5997 - val_accuracy: 0.7905\n",
      "Epoch 871/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5911 - accuracy: 0.7889\n",
      "Epoch 871: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5911 - accuracy: 0.7889 - val_loss: 0.5187 - val_accuracy: 0.8187\n",
      "Epoch 872/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5814 - accuracy: 0.7886\n",
      "Epoch 872: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5817 - accuracy: 0.7885 - val_loss: 0.5027 - val_accuracy: 0.8211\n",
      "Epoch 873/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5755 - accuracy: 0.7919\n",
      "Epoch 873: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5754 - accuracy: 0.7920 - val_loss: 0.5039 - val_accuracy: 0.8213\n",
      "Epoch 874/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5877 - accuracy: 0.7882\n",
      "Epoch 874: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5877 - accuracy: 0.7882 - val_loss: 0.4854 - val_accuracy: 0.8254\n",
      "Epoch 875/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5759 - accuracy: 0.7908\n",
      "Epoch 875: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5759 - accuracy: 0.7908 - val_loss: 0.4983 - val_accuracy: 0.8198\n",
      "Epoch 876/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5760 - accuracy: 0.7922\n",
      "Epoch 876: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5761 - accuracy: 0.7922 - val_loss: 0.5110 - val_accuracy: 0.8157\n",
      "Epoch 877/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5942 - accuracy: 0.7873\n",
      "Epoch 877: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5943 - accuracy: 0.7872 - val_loss: 0.5075 - val_accuracy: 0.8157\n",
      "Epoch 878/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5795 - accuracy: 0.7917\n",
      "Epoch 878: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5795 - accuracy: 0.7917 - val_loss: 0.5121 - val_accuracy: 0.8181\n",
      "Epoch 879/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5966 - accuracy: 0.7862\n",
      "Epoch 879: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5966 - accuracy: 0.7863 - val_loss: 0.5058 - val_accuracy: 0.8190\n",
      "Epoch 880/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5918 - accuracy: 0.7873\n",
      "Epoch 880: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5924 - accuracy: 0.7872 - val_loss: 0.6205 - val_accuracy: 0.7893\n",
      "Epoch 881/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5813 - accuracy: 0.7904\n",
      "Epoch 881: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5813 - accuracy: 0.7904 - val_loss: 0.4796 - val_accuracy: 0.8278\n",
      "Epoch 882/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5808 - accuracy: 0.7902\n",
      "Epoch 882: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5809 - accuracy: 0.7902 - val_loss: 0.4791 - val_accuracy: 0.8268\n",
      "Epoch 883/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5779 - accuracy: 0.7911\n",
      "Epoch 883: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5778 - accuracy: 0.7911 - val_loss: 0.5686 - val_accuracy: 0.7992\n",
      "Epoch 884/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5870 - accuracy: 0.7893\n",
      "Epoch 884: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5869 - accuracy: 0.7894 - val_loss: 0.4759 - val_accuracy: 0.8250\n",
      "Epoch 885/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5831 - accuracy: 0.7895\n",
      "Epoch 885: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5831 - accuracy: 0.7895 - val_loss: 0.4975 - val_accuracy: 0.8171\n",
      "Epoch 886/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5912 - accuracy: 0.7906\n",
      "Epoch 886: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5911 - accuracy: 0.7906 - val_loss: 0.5094 - val_accuracy: 0.8122\n",
      "Epoch 887/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6100 - accuracy: 0.7825\n",
      "Epoch 887: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6100 - accuracy: 0.7825 - val_loss: 0.4758 - val_accuracy: 0.8282\n",
      "Epoch 888/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5982 - accuracy: 0.7862\n",
      "Epoch 888: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5982 - accuracy: 0.7862 - val_loss: 0.5275 - val_accuracy: 0.8066\n",
      "Epoch 889/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5790 - accuracy: 0.7908\n",
      "Epoch 889: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5788 - accuracy: 0.7909 - val_loss: 0.5023 - val_accuracy: 0.8195\n",
      "Epoch 890/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5760 - accuracy: 0.7933\n",
      "Epoch 890: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5761 - accuracy: 0.7932 - val_loss: 0.5167 - val_accuracy: 0.8117\n",
      "Epoch 891/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5845 - accuracy: 0.7887\n",
      "Epoch 891: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5845 - accuracy: 0.7887 - val_loss: 0.5045 - val_accuracy: 0.8211\n",
      "Epoch 892/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5883 - accuracy: 0.7893\n",
      "Epoch 892: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5883 - accuracy: 0.7893 - val_loss: 0.5049 - val_accuracy: 0.8187\n",
      "Epoch 893/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5724 - accuracy: 0.7932\n",
      "Epoch 893: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5724 - accuracy: 0.7932 - val_loss: 0.4834 - val_accuracy: 0.8242\n",
      "Epoch 894/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5748 - accuracy: 0.7922\n",
      "Epoch 894: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5748 - accuracy: 0.7922 - val_loss: 0.5003 - val_accuracy: 0.8220\n",
      "Epoch 895/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6008 - accuracy: 0.7860\n",
      "Epoch 895: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6009 - accuracy: 0.7860 - val_loss: 0.4920 - val_accuracy: 0.8212\n",
      "Epoch 896/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5832 - accuracy: 0.7912\n",
      "Epoch 896: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5833 - accuracy: 0.7910 - val_loss: 0.5091 - val_accuracy: 0.8221\n",
      "Epoch 897/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5719 - accuracy: 0.7944\n",
      "Epoch 897: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5719 - accuracy: 0.7944 - val_loss: 0.5746 - val_accuracy: 0.8020\n",
      "Epoch 898/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5807 - accuracy: 0.7907\n",
      "Epoch 898: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5807 - accuracy: 0.7907 - val_loss: 0.5019 - val_accuracy: 0.8159\n",
      "Epoch 899/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5702 - accuracy: 0.7929\n",
      "Epoch 899: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5702 - accuracy: 0.7929 - val_loss: 0.4881 - val_accuracy: 0.8232\n",
      "Epoch 900/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5792 - accuracy: 0.7909\n",
      "Epoch 900: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5792 - accuracy: 0.7908 - val_loss: 0.4885 - val_accuracy: 0.8196\n",
      "Epoch 901/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6340 - accuracy: 0.7747\n",
      "Epoch 901: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6341 - accuracy: 0.7746 - val_loss: 0.5412 - val_accuracy: 0.8056\n",
      "Epoch 902/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5786 - accuracy: 0.7906\n",
      "Epoch 902: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5788 - accuracy: 0.7906 - val_loss: 0.4843 - val_accuracy: 0.8273\n",
      "Epoch 903/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5667 - accuracy: 0.7937\n",
      "Epoch 903: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5667 - accuracy: 0.7937 - val_loss: 0.5049 - val_accuracy: 0.8165\n",
      "Epoch 904/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6223 - accuracy: 0.7812\n",
      "Epoch 904: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6225 - accuracy: 0.7810 - val_loss: 0.4986 - val_accuracy: 0.8177\n",
      "Epoch 905/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5706 - accuracy: 0.7946\n",
      "Epoch 905: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5708 - accuracy: 0.7945 - val_loss: 0.4871 - val_accuracy: 0.8216\n",
      "Epoch 906/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5836 - accuracy: 0.7919\n",
      "Epoch 906: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5836 - accuracy: 0.7919 - val_loss: 0.5053 - val_accuracy: 0.8150\n",
      "Epoch 907/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5882 - accuracy: 0.7890\n",
      "Epoch 907: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5882 - accuracy: 0.7890 - val_loss: 0.5122 - val_accuracy: 0.8175\n",
      "Epoch 908/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5954 - accuracy: 0.7873\n",
      "Epoch 908: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5953 - accuracy: 0.7873 - val_loss: 0.5645 - val_accuracy: 0.7970\n",
      "Epoch 909/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5843 - accuracy: 0.7891\n",
      "Epoch 909: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5843 - accuracy: 0.7891 - val_loss: 0.5152 - val_accuracy: 0.8185\n",
      "Epoch 910/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5778 - accuracy: 0.7908\n",
      "Epoch 910: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5775 - accuracy: 0.7909 - val_loss: 0.4767 - val_accuracy: 0.8318\n",
      "Epoch 911/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6464 - accuracy: 0.7773\n",
      "Epoch 911: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6465 - accuracy: 0.7774 - val_loss: 0.5226 - val_accuracy: 0.8178\n",
      "Epoch 912/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6214 - accuracy: 0.7807\n",
      "Epoch 912: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6213 - accuracy: 0.7807 - val_loss: 0.5011 - val_accuracy: 0.8219\n",
      "Epoch 913/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5918 - accuracy: 0.7895\n",
      "Epoch 913: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5918 - accuracy: 0.7894 - val_loss: 0.5047 - val_accuracy: 0.8230\n",
      "Epoch 914/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5752 - accuracy: 0.7926\n",
      "Epoch 914: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5751 - accuracy: 0.7927 - val_loss: 0.5083 - val_accuracy: 0.8249\n",
      "Epoch 915/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5725 - accuracy: 0.7930\n",
      "Epoch 915: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5724 - accuracy: 0.7930 - val_loss: 0.4877 - val_accuracy: 0.8157\n",
      "Epoch 916/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5876 - accuracy: 0.7884\n",
      "Epoch 916: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5876 - accuracy: 0.7884 - val_loss: 0.4913 - val_accuracy: 0.8252\n",
      "Epoch 917/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6163 - accuracy: 0.7796\n",
      "Epoch 917: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6163 - accuracy: 0.7797 - val_loss: 0.4873 - val_accuracy: 0.8264\n",
      "Epoch 918/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5727 - accuracy: 0.7919\n",
      "Epoch 918: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5727 - accuracy: 0.7919 - val_loss: 0.5007 - val_accuracy: 0.8172\n",
      "Epoch 919/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5844 - accuracy: 0.7906\n",
      "Epoch 919: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5842 - accuracy: 0.7908 - val_loss: 0.5215 - val_accuracy: 0.8181\n",
      "Epoch 920/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.7332 - accuracy: 0.7523\n",
      "Epoch 920: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7325 - accuracy: 0.7525 - val_loss: 0.5113 - val_accuracy: 0.8176\n",
      "Epoch 921/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5775 - accuracy: 0.7922\n",
      "Epoch 921: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5777 - accuracy: 0.7921 - val_loss: 0.4933 - val_accuracy: 0.8238\n",
      "Epoch 922/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5752 - accuracy: 0.7942\n",
      "Epoch 922: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5753 - accuracy: 0.7941 - val_loss: 0.5224 - val_accuracy: 0.8192\n",
      "Epoch 923/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6024 - accuracy: 0.7861\n",
      "Epoch 923: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6027 - accuracy: 0.7859 - val_loss: 0.5270 - val_accuracy: 0.8179\n",
      "Epoch 924/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5890 - accuracy: 0.7895\n",
      "Epoch 924: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5891 - accuracy: 0.7893 - val_loss: 0.5108 - val_accuracy: 0.8174\n",
      "Epoch 925/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5889 - accuracy: 0.7900\n",
      "Epoch 925: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5889 - accuracy: 0.7901 - val_loss: 0.4869 - val_accuracy: 0.8283\n",
      "Epoch 926/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5836 - accuracy: 0.7905\n",
      "Epoch 926: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5837 - accuracy: 0.7904 - val_loss: 0.5284 - val_accuracy: 0.8135\n",
      "Epoch 927/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5832 - accuracy: 0.7899\n",
      "Epoch 927: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5834 - accuracy: 0.7898 - val_loss: 0.4814 - val_accuracy: 0.8246\n",
      "Epoch 928/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5734 - accuracy: 0.7942\n",
      "Epoch 928: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5735 - accuracy: 0.7942 - val_loss: 0.5010 - val_accuracy: 0.8236\n",
      "Epoch 929/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5677 - accuracy: 0.7950\n",
      "Epoch 929: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5677 - accuracy: 0.7950 - val_loss: 0.4888 - val_accuracy: 0.8226\n",
      "Epoch 930/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5828 - accuracy: 0.7912\n",
      "Epoch 930: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5825 - accuracy: 0.7913 - val_loss: 0.4906 - val_accuracy: 0.8282\n",
      "Epoch 931/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5748 - accuracy: 0.7943\n",
      "Epoch 931: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5748 - accuracy: 0.7942 - val_loss: 0.4999 - val_accuracy: 0.8229\n",
      "Epoch 932/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5716 - accuracy: 0.7939\n",
      "Epoch 932: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5716 - accuracy: 0.7939 - val_loss: 0.5032 - val_accuracy: 0.8227\n",
      "Epoch 933/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5723 - accuracy: 0.7936\n",
      "Epoch 933: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5723 - accuracy: 0.7936 - val_loss: 0.4802 - val_accuracy: 0.8263\n",
      "Epoch 934/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5744 - accuracy: 0.7934\n",
      "Epoch 934: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5745 - accuracy: 0.7934 - val_loss: 0.4855 - val_accuracy: 0.8264\n",
      "Epoch 935/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5714 - accuracy: 0.7960\n",
      "Epoch 935: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5712 - accuracy: 0.7961 - val_loss: 0.4872 - val_accuracy: 0.8258\n",
      "Epoch 936/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5942 - accuracy: 0.7873\n",
      "Epoch 936: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5942 - accuracy: 0.7873 - val_loss: 0.5164 - val_accuracy: 0.8130\n",
      "Epoch 937/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5736 - accuracy: 0.7935\n",
      "Epoch 937: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5736 - accuracy: 0.7935 - val_loss: 0.5154 - val_accuracy: 0.8142\n",
      "Epoch 938/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5790 - accuracy: 0.7935\n",
      "Epoch 938: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5790 - accuracy: 0.7935 - val_loss: 0.4864 - val_accuracy: 0.8278\n",
      "Epoch 939/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6091 - accuracy: 0.7865\n",
      "Epoch 939: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6090 - accuracy: 0.7865 - val_loss: 0.4965 - val_accuracy: 0.8212\n",
      "Epoch 940/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5803 - accuracy: 0.7918\n",
      "Epoch 940: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5801 - accuracy: 0.7918 - val_loss: 0.5007 - val_accuracy: 0.8197\n",
      "Epoch 941/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5778 - accuracy: 0.7924\n",
      "Epoch 941: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5777 - accuracy: 0.7924 - val_loss: 0.4841 - val_accuracy: 0.8249\n",
      "Epoch 942/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5875 - accuracy: 0.7907\n",
      "Epoch 942: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5874 - accuracy: 0.7907 - val_loss: 0.5294 - val_accuracy: 0.8104\n",
      "Epoch 943/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5743 - accuracy: 0.7932\n",
      "Epoch 943: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5740 - accuracy: 0.7934 - val_loss: 0.4976 - val_accuracy: 0.8204\n",
      "Epoch 944/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6009 - accuracy: 0.7857\n",
      "Epoch 944: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6003 - accuracy: 0.7859 - val_loss: 0.4778 - val_accuracy: 0.8278\n",
      "Epoch 945/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5701 - accuracy: 0.7944\n",
      "Epoch 945: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5702 - accuracy: 0.7944 - val_loss: 0.5060 - val_accuracy: 0.8171\n",
      "Epoch 946/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5826 - accuracy: 0.7910\n",
      "Epoch 946: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5824 - accuracy: 0.7911 - val_loss: 0.4974 - val_accuracy: 0.8150\n",
      "Epoch 947/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5773 - accuracy: 0.7922\n",
      "Epoch 947: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5772 - accuracy: 0.7922 - val_loss: 0.5025 - val_accuracy: 0.8210\n",
      "Epoch 948/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5755 - accuracy: 0.7951\n",
      "Epoch 948: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5755 - accuracy: 0.7951 - val_loss: 0.5212 - val_accuracy: 0.8142\n",
      "Epoch 949/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5796 - accuracy: 0.7911\n",
      "Epoch 949: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5797 - accuracy: 0.7911 - val_loss: 0.5614 - val_accuracy: 0.7995\n",
      "Epoch 950/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5763 - accuracy: 0.7942\n",
      "Epoch 950: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5764 - accuracy: 0.7942 - val_loss: 0.4820 - val_accuracy: 0.8306\n",
      "Epoch 951/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5781 - accuracy: 0.7910\n",
      "Epoch 951: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5780 - accuracy: 0.7911 - val_loss: 0.5142 - val_accuracy: 0.8139\n",
      "Epoch 952/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6033 - accuracy: 0.7855\n",
      "Epoch 952: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6033 - accuracy: 0.7855 - val_loss: 0.5170 - val_accuracy: 0.8212\n",
      "Epoch 953/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5720 - accuracy: 0.7943\n",
      "Epoch 953: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5721 - accuracy: 0.7941 - val_loss: 0.5008 - val_accuracy: 0.8201\n",
      "Epoch 954/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5732 - accuracy: 0.7949\n",
      "Epoch 954: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5733 - accuracy: 0.7949 - val_loss: 0.5939 - val_accuracy: 0.7917\n",
      "Epoch 955/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5834 - accuracy: 0.7920\n",
      "Epoch 955: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5834 - accuracy: 0.7920 - val_loss: 0.4932 - val_accuracy: 0.8237\n",
      "Epoch 956/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5795 - accuracy: 0.7920\n",
      "Epoch 956: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5792 - accuracy: 0.7921 - val_loss: 0.4731 - val_accuracy: 0.8286\n",
      "Epoch 957/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5773 - accuracy: 0.7929\n",
      "Epoch 957: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5773 - accuracy: 0.7928 - val_loss: 0.5117 - val_accuracy: 0.8099\n",
      "Epoch 958/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5897 - accuracy: 0.7892\n",
      "Epoch 958: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5894 - accuracy: 0.7893 - val_loss: 0.4936 - val_accuracy: 0.8186\n",
      "Epoch 959/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5857 - accuracy: 0.7894\n",
      "Epoch 959: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5857 - accuracy: 0.7894 - val_loss: 0.4807 - val_accuracy: 0.8241\n",
      "Epoch 960/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5746 - accuracy: 0.7944\n",
      "Epoch 960: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5745 - accuracy: 0.7944 - val_loss: 0.4944 - val_accuracy: 0.8248\n",
      "Epoch 961/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5815 - accuracy: 0.7920\n",
      "Epoch 961: val_loss did not improve from 0.47066\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5814 - accuracy: 0.7921 - val_loss: 0.4791 - val_accuracy: 0.8266\n",
      "Epoch 962/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5696 - accuracy: 0.7944\n",
      "Epoch 962: val_loss improved from 0.47066 to 0.46483, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5696 - accuracy: 0.7944 - val_loss: 0.4648 - val_accuracy: 0.8303\n",
      "Epoch 963/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5753 - accuracy: 0.7944\n",
      "Epoch 963: val_loss did not improve from 0.46483\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5753 - accuracy: 0.7943 - val_loss: 0.5179 - val_accuracy: 0.8126\n",
      "Epoch 964/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5752 - accuracy: 0.7935\n",
      "Epoch 964: val_loss did not improve from 0.46483\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5752 - accuracy: 0.7935 - val_loss: 0.5439 - val_accuracy: 0.8076\n",
      "Epoch 965/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5924 - accuracy: 0.7897\n",
      "Epoch 965: val_loss did not improve from 0.46483\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5921 - accuracy: 0.7898 - val_loss: 0.4773 - val_accuracy: 0.8268\n",
      "Epoch 966/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5758 - accuracy: 0.7921\n",
      "Epoch 966: val_loss did not improve from 0.46483\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5758 - accuracy: 0.7921 - val_loss: 0.4871 - val_accuracy: 0.8286\n",
      "Epoch 967/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6015 - accuracy: 0.7848\n",
      "Epoch 967: val_loss did not improve from 0.46483\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6016 - accuracy: 0.7846 - val_loss: 0.5164 - val_accuracy: 0.8189\n",
      "Epoch 968/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5853 - accuracy: 0.7890\n",
      "Epoch 968: val_loss did not improve from 0.46483\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5853 - accuracy: 0.7890 - val_loss: 0.5136 - val_accuracy: 0.8137\n",
      "Epoch 969/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5757 - accuracy: 0.7917\n",
      "Epoch 969: val_loss did not improve from 0.46483\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5756 - accuracy: 0.7918 - val_loss: 0.4879 - val_accuracy: 0.8252\n",
      "Epoch 970/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6003 - accuracy: 0.7859\n",
      "Epoch 970: val_loss did not improve from 0.46483\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6001 - accuracy: 0.7860 - val_loss: 0.4852 - val_accuracy: 0.8237\n",
      "Epoch 971/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5658 - accuracy: 0.7963\n",
      "Epoch 971: val_loss did not improve from 0.46483\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5658 - accuracy: 0.7963 - val_loss: 0.4951 - val_accuracy: 0.8254\n",
      "Epoch 972/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5667 - accuracy: 0.7968\n",
      "Epoch 972: val_loss did not improve from 0.46483\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5668 - accuracy: 0.7967 - val_loss: 0.5693 - val_accuracy: 0.7996\n",
      "Epoch 973/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6003 - accuracy: 0.7852\n",
      "Epoch 973: val_loss did not improve from 0.46483\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6003 - accuracy: 0.7852 - val_loss: 0.4737 - val_accuracy: 0.8328\n",
      "Epoch 974/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5857 - accuracy: 0.7902\n",
      "Epoch 974: val_loss did not improve from 0.46483\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5855 - accuracy: 0.7902 - val_loss: 0.4802 - val_accuracy: 0.8272\n",
      "Epoch 975/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.7867 - accuracy: 0.7229\n",
      "Epoch 975: val_loss did not improve from 0.46483\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7853 - accuracy: 0.7234 - val_loss: 0.4877 - val_accuracy: 0.8241\n",
      "Epoch 976/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5747 - accuracy: 0.7944\n",
      "Epoch 976: val_loss did not improve from 0.46483\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5749 - accuracy: 0.7943 - val_loss: 0.4909 - val_accuracy: 0.8209\n",
      "Epoch 977/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5627 - accuracy: 0.7974\n",
      "Epoch 977: val_loss did not improve from 0.46483\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5626 - accuracy: 0.7974 - val_loss: 0.4824 - val_accuracy: 0.8260\n",
      "Epoch 978/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5765 - accuracy: 0.7944\n",
      "Epoch 978: val_loss did not improve from 0.46483\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5760 - accuracy: 0.7946 - val_loss: 0.5110 - val_accuracy: 0.8186\n",
      "Epoch 979/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5737 - accuracy: 0.7949\n",
      "Epoch 979: val_loss did not improve from 0.46483\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5737 - accuracy: 0.7949 - val_loss: 0.4973 - val_accuracy: 0.8178\n",
      "Epoch 980/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5890 - accuracy: 0.7903\n",
      "Epoch 980: val_loss did not improve from 0.46483\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5891 - accuracy: 0.7903 - val_loss: 0.4824 - val_accuracy: 0.8319\n",
      "Epoch 981/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6086 - accuracy: 0.7848\n",
      "Epoch 981: val_loss did not improve from 0.46483\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6082 - accuracy: 0.7849 - val_loss: 0.5296 - val_accuracy: 0.8134\n",
      "Epoch 982/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5826 - accuracy: 0.7917\n",
      "Epoch 982: val_loss did not improve from 0.46483\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5826 - accuracy: 0.7918 - val_loss: 0.4916 - val_accuracy: 0.8226\n",
      "Epoch 983/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5843 - accuracy: 0.7908\n",
      "Epoch 983: val_loss did not improve from 0.46483\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5842 - accuracy: 0.7909 - val_loss: 0.5092 - val_accuracy: 0.8188\n",
      "Epoch 984/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5752 - accuracy: 0.7930\n",
      "Epoch 984: val_loss did not improve from 0.46483\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5755 - accuracy: 0.7931 - val_loss: 0.5238 - val_accuracy: 0.8151\n",
      "Epoch 985/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5785 - accuracy: 0.7928\n",
      "Epoch 985: val_loss did not improve from 0.46483\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5785 - accuracy: 0.7928 - val_loss: 0.5984 - val_accuracy: 0.7880\n",
      "Epoch 986/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6153 - accuracy: 0.7805\n",
      "Epoch 986: val_loss did not improve from 0.46483\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6148 - accuracy: 0.7806 - val_loss: 0.4855 - val_accuracy: 0.8209\n",
      "Epoch 987/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5744 - accuracy: 0.7930\n",
      "Epoch 987: val_loss did not improve from 0.46483\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5745 - accuracy: 0.7930 - val_loss: 0.4794 - val_accuracy: 0.8282\n",
      "Epoch 988/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5774 - accuracy: 0.7933\n",
      "Epoch 988: val_loss did not improve from 0.46483\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5773 - accuracy: 0.7932 - val_loss: 0.4802 - val_accuracy: 0.8296\n",
      "Epoch 989/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5733 - accuracy: 0.7934\n",
      "Epoch 989: val_loss did not improve from 0.46483\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5737 - accuracy: 0.7933 - val_loss: 0.5285 - val_accuracy: 0.8098\n",
      "Epoch 990/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5971 - accuracy: 0.7876\n",
      "Epoch 990: val_loss did not improve from 0.46483\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5972 - accuracy: 0.7876 - val_loss: 0.5004 - val_accuracy: 0.8173\n",
      "Epoch 991/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5816 - accuracy: 0.7906\n",
      "Epoch 991: val_loss did not improve from 0.46483\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5814 - accuracy: 0.7907 - val_loss: 0.4988 - val_accuracy: 0.8239\n",
      "Epoch 992/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5743 - accuracy: 0.7935\n",
      "Epoch 992: val_loss did not improve from 0.46483\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5743 - accuracy: 0.7936 - val_loss: 0.5039 - val_accuracy: 0.8189\n",
      "Epoch 993/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5851 - accuracy: 0.7924\n",
      "Epoch 993: val_loss did not improve from 0.46483\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5851 - accuracy: 0.7924 - val_loss: 0.5541 - val_accuracy: 0.8005\n",
      "Epoch 994/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5794 - accuracy: 0.7920\n",
      "Epoch 994: val_loss did not improve from 0.46483\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5791 - accuracy: 0.7921 - val_loss: 0.5125 - val_accuracy: 0.8161\n",
      "Epoch 995/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5682 - accuracy: 0.7968\n",
      "Epoch 995: val_loss did not improve from 0.46483\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5682 - accuracy: 0.7968 - val_loss: 0.4940 - val_accuracy: 0.8232\n",
      "Epoch 996/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5602 - accuracy: 0.8004\n",
      "Epoch 996: val_loss did not improve from 0.46483\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5604 - accuracy: 0.8004 - val_loss: 0.4833 - val_accuracy: 0.8287\n",
      "Epoch 997/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5762 - accuracy: 0.7925\n",
      "Epoch 997: val_loss did not improve from 0.46483\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5761 - accuracy: 0.7926 - val_loss: 0.4707 - val_accuracy: 0.8335\n",
      "Epoch 998/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5730 - accuracy: 0.7960\n",
      "Epoch 998: val_loss did not improve from 0.46483\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5732 - accuracy: 0.7959 - val_loss: 0.5043 - val_accuracy: 0.8193\n",
      "Epoch 999/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6206 - accuracy: 0.7781\n",
      "Epoch 999: val_loss did not improve from 0.46483\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6206 - accuracy: 0.7781 - val_loss: 0.4922 - val_accuracy: 0.8244\n",
      "Epoch 1000/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5735 - accuracy: 0.7947\n",
      "Epoch 1000: val_loss did not improve from 0.46483\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5734 - accuracy: 0.7947 - val_loss: 0.4773 - val_accuracy: 0.8334\n",
      "model saved locally\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, CSVLogger\n",
    "\n",
    "current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "model_path = \"/home/jupyter/trained_models_srikar/\"\n",
    "model_name = f'model_{current_time}.h5'\n",
    "\n",
    "# Define a callback to save the best model based on validation loss\n",
    "checkpoint = ModelCheckpoint(model_path + \"best_epoch_\" + model_name, \n",
    "                             monitor='val_loss', \n",
    "                             verbose=1, \n",
    "                             save_best_only=True, \n",
    "                             mode='min')\n",
    "\n",
    "# TensorBoard callback to log training metrics\n",
    "log_dir = \"/home/jupyter/logs/\" + current_time\n",
    "tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# CSVLogger callback to save training logs in CSV format\n",
    "csv_logger = CSVLogger(model_path + \"training_logs.csv\", separator=',', append=False)\n",
    "\n",
    "callbacks = [checkpoint, tensorboard, csv_logger]\n",
    "\n",
    "model_history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=1000,\n",
    "    batch_size=32,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "model.save(model_path + model_name)\n",
    "print(\"model saved locally\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00438a5a-10ac-4b09-a731-67577913014f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5690 - accuracy: 0.7961\n",
      "Epoch 1: val_loss improved from inf to 0.51358, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_05-39-21.h5\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5689 - accuracy: 0.7962 - val_loss: 0.5136 - val_accuracy: 0.8247\n",
      "Epoch 2/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5735 - accuracy: 0.7931\n",
      "Epoch 2: val_loss improved from 0.51358 to 0.47157, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_05-39-21.h5\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5735 - accuracy: 0.7931 - val_loss: 0.4716 - val_accuracy: 0.8308\n",
      "Epoch 3/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5790 - accuracy: 0.7934\n",
      "Epoch 3: val_loss did not improve from 0.47157\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5790 - accuracy: 0.7934 - val_loss: 0.5199 - val_accuracy: 0.8143\n",
      "Epoch 4/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5794 - accuracy: 0.7921\n",
      "Epoch 4: val_loss did not improve from 0.47157\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5795 - accuracy: 0.7920 - val_loss: 0.4758 - val_accuracy: 0.8302\n",
      "Epoch 5/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5691 - accuracy: 0.7937\n",
      "Epoch 5: val_loss did not improve from 0.47157\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5686 - accuracy: 0.7939 - val_loss: 0.4971 - val_accuracy: 0.8195\n",
      "Epoch 6/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5865 - accuracy: 0.7902\n",
      "Epoch 6: val_loss did not improve from 0.47157\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5864 - accuracy: 0.7903 - val_loss: 0.5216 - val_accuracy: 0.8115\n",
      "Epoch 7/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6906 - accuracy: 0.7518\n",
      "Epoch 7: val_loss did not improve from 0.47157\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6903 - accuracy: 0.7519 - val_loss: 0.5397 - val_accuracy: 0.8089\n",
      "Epoch 8/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5827 - accuracy: 0.7904\n",
      "Epoch 8: val_loss did not improve from 0.47157\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5827 - accuracy: 0.7904 - val_loss: 0.5042 - val_accuracy: 0.8182\n",
      "Epoch 9/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5597 - accuracy: 0.7964\n",
      "Epoch 9: val_loss did not improve from 0.47157\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5597 - accuracy: 0.7964 - val_loss: 0.5170 - val_accuracy: 0.8127\n",
      "Epoch 10/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5633 - accuracy: 0.7960\n",
      "Epoch 10: val_loss did not improve from 0.47157\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5633 - accuracy: 0.7960 - val_loss: 0.4847 - val_accuracy: 0.8259\n",
      "Epoch 11/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5907 - accuracy: 0.7915\n",
      "Epoch 11: val_loss did not improve from 0.47157\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5910 - accuracy: 0.7914 - val_loss: 0.5085 - val_accuracy: 0.8208\n",
      "Epoch 12/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6304 - accuracy: 0.7825\n",
      "Epoch 12: val_loss did not improve from 0.47157\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6304 - accuracy: 0.7824 - val_loss: 0.5980 - val_accuracy: 0.7977\n",
      "Epoch 13/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5838 - accuracy: 0.7914\n",
      "Epoch 13: val_loss did not improve from 0.47157\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5838 - accuracy: 0.7914 - val_loss: 0.4740 - val_accuracy: 0.8319\n",
      "Epoch 14/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5698 - accuracy: 0.7965\n",
      "Epoch 14: val_loss did not improve from 0.47157\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5699 - accuracy: 0.7965 - val_loss: 0.5188 - val_accuracy: 0.8178\n",
      "Epoch 15/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5646 - accuracy: 0.7969\n",
      "Epoch 15: val_loss did not improve from 0.47157\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5645 - accuracy: 0.7970 - val_loss: 0.4802 - val_accuracy: 0.8237\n",
      "Epoch 16/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6355 - accuracy: 0.7755\n",
      "Epoch 16: val_loss did not improve from 0.47157\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6355 - accuracy: 0.7755 - val_loss: 0.4907 - val_accuracy: 0.8273\n",
      "Epoch 17/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6556 - accuracy: 0.7658\n",
      "Epoch 17: val_loss did not improve from 0.47157\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6556 - accuracy: 0.7658 - val_loss: 0.5514 - val_accuracy: 0.8081\n",
      "Epoch 18/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5854 - accuracy: 0.7915\n",
      "Epoch 18: val_loss did not improve from 0.47157\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5857 - accuracy: 0.7915 - val_loss: 0.5034 - val_accuracy: 0.8196\n",
      "Epoch 19/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5880 - accuracy: 0.7902\n",
      "Epoch 19: val_loss did not improve from 0.47157\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5882 - accuracy: 0.7901 - val_loss: 0.5068 - val_accuracy: 0.8179\n",
      "Epoch 20/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5757 - accuracy: 0.7934\n",
      "Epoch 20: val_loss did not improve from 0.47157\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5756 - accuracy: 0.7934 - val_loss: 0.4887 - val_accuracy: 0.8216\n",
      "Epoch 21/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5759 - accuracy: 0.7948\n",
      "Epoch 21: val_loss did not improve from 0.47157\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5758 - accuracy: 0.7948 - val_loss: 0.5084 - val_accuracy: 0.8141\n",
      "Epoch 22/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5947 - accuracy: 0.7888\n",
      "Epoch 22: val_loss did not improve from 0.47157\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5947 - accuracy: 0.7888 - val_loss: 0.5155 - val_accuracy: 0.8171\n",
      "Epoch 23/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5774 - accuracy: 0.7931\n",
      "Epoch 23: val_loss did not improve from 0.47157\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5774 - accuracy: 0.7931 - val_loss: 0.4965 - val_accuracy: 0.8214\n",
      "Epoch 24/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6038 - accuracy: 0.7855\n",
      "Epoch 24: val_loss did not improve from 0.47157\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6038 - accuracy: 0.7854 - val_loss: 0.4758 - val_accuracy: 0.8308\n",
      "Epoch 25/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5630 - accuracy: 0.7967\n",
      "Epoch 25: val_loss did not improve from 0.47157\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5628 - accuracy: 0.7967 - val_loss: 0.4886 - val_accuracy: 0.8209\n",
      "Epoch 26/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5651 - accuracy: 0.7948\n",
      "Epoch 26: val_loss did not improve from 0.47157\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5651 - accuracy: 0.7948 - val_loss: 0.4972 - val_accuracy: 0.8245\n",
      "Epoch 27/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6064 - accuracy: 0.7855\n",
      "Epoch 27: val_loss did not improve from 0.47157\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6064 - accuracy: 0.7855 - val_loss: 0.5126 - val_accuracy: 0.8174\n",
      "Epoch 28/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5786 - accuracy: 0.7909\n",
      "Epoch 28: val_loss did not improve from 0.47157\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5786 - accuracy: 0.7909 - val_loss: 0.5073 - val_accuracy: 0.8169\n",
      "Epoch 29/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5696 - accuracy: 0.7967\n",
      "Epoch 29: val_loss did not improve from 0.47157\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5699 - accuracy: 0.7966 - val_loss: 0.4760 - val_accuracy: 0.8267\n",
      "Epoch 30/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5663 - accuracy: 0.7958\n",
      "Epoch 30: val_loss did not improve from 0.47157\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5663 - accuracy: 0.7958 - val_loss: 0.4737 - val_accuracy: 0.8331\n",
      "Epoch 31/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5830 - accuracy: 0.7915\n",
      "Epoch 31: val_loss did not improve from 0.47157\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5832 - accuracy: 0.7915 - val_loss: 0.5128 - val_accuracy: 0.8161\n",
      "Epoch 32/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5769 - accuracy: 0.7924\n",
      "Epoch 32: val_loss did not improve from 0.47157\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5768 - accuracy: 0.7924 - val_loss: 0.4795 - val_accuracy: 0.8273\n",
      "Epoch 33/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5668 - accuracy: 0.7972\n",
      "Epoch 33: val_loss did not improve from 0.47157\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5668 - accuracy: 0.7972 - val_loss: 0.4836 - val_accuracy: 0.8252\n",
      "Epoch 34/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5801 - accuracy: 0.7912\n",
      "Epoch 34: val_loss did not improve from 0.47157\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5803 - accuracy: 0.7911 - val_loss: 0.5378 - val_accuracy: 0.8081\n",
      "Epoch 35/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5670 - accuracy: 0.7967\n",
      "Epoch 35: val_loss did not improve from 0.47157\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5671 - accuracy: 0.7966 - val_loss: 0.5090 - val_accuracy: 0.8176\n",
      "Epoch 36/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5753 - accuracy: 0.7959\n",
      "Epoch 36: val_loss did not improve from 0.47157\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5753 - accuracy: 0.7959 - val_loss: 0.4963 - val_accuracy: 0.8191\n",
      "Epoch 37/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5882 - accuracy: 0.7922\n",
      "Epoch 37: val_loss did not improve from 0.47157\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5880 - accuracy: 0.7923 - val_loss: 0.5251 - val_accuracy: 0.8126\n",
      "Epoch 38/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5879 - accuracy: 0.7905\n",
      "Epoch 38: val_loss did not improve from 0.47157\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5878 - accuracy: 0.7906 - val_loss: 0.4911 - val_accuracy: 0.8221\n",
      "Epoch 39/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5838 - accuracy: 0.7935\n",
      "Epoch 39: val_loss did not improve from 0.47157\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5837 - accuracy: 0.7935 - val_loss: 0.4788 - val_accuracy: 0.8277\n",
      "Epoch 40/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5805 - accuracy: 0.7923\n",
      "Epoch 40: val_loss did not improve from 0.47157\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5803 - accuracy: 0.7924 - val_loss: 0.4870 - val_accuracy: 0.8251\n",
      "Epoch 41/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5789 - accuracy: 0.7941\n",
      "Epoch 41: val_loss did not improve from 0.47157\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5788 - accuracy: 0.7941 - val_loss: 0.5056 - val_accuracy: 0.8185\n",
      "Epoch 42/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5743 - accuracy: 0.7959\n",
      "Epoch 42: val_loss did not improve from 0.47157\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5742 - accuracy: 0.7959 - val_loss: 0.4748 - val_accuracy: 0.8258\n",
      "Epoch 43/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5756 - accuracy: 0.7940\n",
      "Epoch 43: val_loss did not improve from 0.47157\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5757 - accuracy: 0.7941 - val_loss: 0.4989 - val_accuracy: 0.8254\n",
      "Epoch 44/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5827 - accuracy: 0.7924\n",
      "Epoch 44: val_loss did not improve from 0.47157\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5835 - accuracy: 0.7922 - val_loss: 0.7341 - val_accuracy: 0.7338\n",
      "Epoch 45/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5929 - accuracy: 0.7882\n",
      "Epoch 45: val_loss did not improve from 0.47157\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5928 - accuracy: 0.7882 - val_loss: 0.5063 - val_accuracy: 0.8206\n",
      "Epoch 46/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5730 - accuracy: 0.7944\n",
      "Epoch 46: val_loss did not improve from 0.47157\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5730 - accuracy: 0.7944 - val_loss: 0.4748 - val_accuracy: 0.8310\n",
      "Epoch 47/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6026 - accuracy: 0.7843\n",
      "Epoch 47: val_loss did not improve from 0.47157\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6028 - accuracy: 0.7843 - val_loss: 0.4968 - val_accuracy: 0.8192\n",
      "Epoch 48/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5689 - accuracy: 0.7971\n",
      "Epoch 48: val_loss improved from 0.47157 to 0.46802, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_05-39-21.h5\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5691 - accuracy: 0.7969 - val_loss: 0.4680 - val_accuracy: 0.8321\n",
      "Epoch 49/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5612 - accuracy: 0.7977\n",
      "Epoch 49: val_loss did not improve from 0.46802\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5612 - accuracy: 0.7977 - val_loss: 0.4906 - val_accuracy: 0.8213\n",
      "Epoch 50/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6118 - accuracy: 0.7838\n",
      "Epoch 50: val_loss did not improve from 0.46802\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6118 - accuracy: 0.7838 - val_loss: 0.5807 - val_accuracy: 0.7970\n",
      "Epoch 51/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6057 - accuracy: 0.7878\n",
      "Epoch 51: val_loss did not improve from 0.46802\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6057 - accuracy: 0.7878 - val_loss: 0.5637 - val_accuracy: 0.8053\n",
      "Epoch 52/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5931 - accuracy: 0.7896\n",
      "Epoch 52: val_loss did not improve from 0.46802\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5930 - accuracy: 0.7896 - val_loss: 0.5376 - val_accuracy: 0.8160\n",
      "Epoch 53/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5970 - accuracy: 0.7875\n",
      "Epoch 53: val_loss did not improve from 0.46802\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5972 - accuracy: 0.7875 - val_loss: 0.5316 - val_accuracy: 0.8160\n",
      "Epoch 54/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5839 - accuracy: 0.7910\n",
      "Epoch 54: val_loss did not improve from 0.46802\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5839 - accuracy: 0.7910 - val_loss: 0.4896 - val_accuracy: 0.8258\n",
      "Epoch 55/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5677 - accuracy: 0.7966\n",
      "Epoch 55: val_loss did not improve from 0.46802\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5677 - accuracy: 0.7966 - val_loss: 0.4778 - val_accuracy: 0.8283\n",
      "Epoch 56/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5724 - accuracy: 0.7939\n",
      "Epoch 56: val_loss did not improve from 0.46802\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5723 - accuracy: 0.7939 - val_loss: 0.5110 - val_accuracy: 0.8190\n",
      "Epoch 57/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5751 - accuracy: 0.7947\n",
      "Epoch 57: val_loss did not improve from 0.46802\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5751 - accuracy: 0.7947 - val_loss: 0.4810 - val_accuracy: 0.8261\n",
      "Epoch 58/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5764 - accuracy: 0.7930\n",
      "Epoch 58: val_loss improved from 0.46802 to 0.46189, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_05-39-21.h5\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5763 - accuracy: 0.7930 - val_loss: 0.4619 - val_accuracy: 0.8310\n",
      "Epoch 59/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6040 - accuracy: 0.7834\n",
      "Epoch 59: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6040 - accuracy: 0.7834 - val_loss: 0.6114 - val_accuracy: 0.7847\n",
      "Epoch 60/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6118 - accuracy: 0.7844\n",
      "Epoch 60: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6118 - accuracy: 0.7844 - val_loss: 0.4949 - val_accuracy: 0.8273\n",
      "Epoch 61/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5921 - accuracy: 0.7906\n",
      "Epoch 61: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5920 - accuracy: 0.7907 - val_loss: 0.4868 - val_accuracy: 0.8236\n",
      "Epoch 62/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5790 - accuracy: 0.7935\n",
      "Epoch 62: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5789 - accuracy: 0.7935 - val_loss: 0.5135 - val_accuracy: 0.8189\n",
      "Epoch 63/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5670 - accuracy: 0.7976\n",
      "Epoch 63: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5670 - accuracy: 0.7975 - val_loss: 0.4902 - val_accuracy: 0.8275\n",
      "Epoch 64/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5742 - accuracy: 0.7937\n",
      "Epoch 64: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5743 - accuracy: 0.7937 - val_loss: 0.4936 - val_accuracy: 0.8234\n",
      "Epoch 65/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5845 - accuracy: 0.7945\n",
      "Epoch 65: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5850 - accuracy: 0.7944 - val_loss: 1.0742 - val_accuracy: 0.6338\n",
      "Epoch 66/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6801 - accuracy: 0.7613\n",
      "Epoch 66: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6800 - accuracy: 0.7613 - val_loss: 0.5359 - val_accuracy: 0.8114\n",
      "Epoch 67/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6393 - accuracy: 0.7747\n",
      "Epoch 67: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6392 - accuracy: 0.7747 - val_loss: 0.5303 - val_accuracy: 0.8119\n",
      "Epoch 68/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6038 - accuracy: 0.7874\n",
      "Epoch 68: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6039 - accuracy: 0.7873 - val_loss: 0.6059 - val_accuracy: 0.7923\n",
      "Epoch 69/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5790 - accuracy: 0.7927\n",
      "Epoch 69: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5790 - accuracy: 0.7926 - val_loss: 0.4907 - val_accuracy: 0.8256\n",
      "Epoch 70/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5877 - accuracy: 0.7893\n",
      "Epoch 70: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5882 - accuracy: 0.7892 - val_loss: 0.4908 - val_accuracy: 0.8220\n",
      "Epoch 71/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5652 - accuracy: 0.7964\n",
      "Epoch 71: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5653 - accuracy: 0.7965 - val_loss: 0.5582 - val_accuracy: 0.8027\n",
      "Epoch 72/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6000 - accuracy: 0.7861\n",
      "Epoch 72: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5998 - accuracy: 0.7861 - val_loss: 0.5051 - val_accuracy: 0.8220\n",
      "Epoch 73/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5943 - accuracy: 0.7889\n",
      "Epoch 73: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5944 - accuracy: 0.7888 - val_loss: 0.4886 - val_accuracy: 0.8243\n",
      "Epoch 74/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5676 - accuracy: 0.7945\n",
      "Epoch 74: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5677 - accuracy: 0.7945 - val_loss: 0.4762 - val_accuracy: 0.8281\n",
      "Epoch 75/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5942 - accuracy: 0.7887\n",
      "Epoch 75: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5940 - accuracy: 0.7888 - val_loss: 0.4961 - val_accuracy: 0.8228\n",
      "Epoch 76/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5702 - accuracy: 0.7947\n",
      "Epoch 76: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5702 - accuracy: 0.7947 - val_loss: 0.4986 - val_accuracy: 0.8244\n",
      "Epoch 77/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5817 - accuracy: 0.7934\n",
      "Epoch 77: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5818 - accuracy: 0.7934 - val_loss: 0.5058 - val_accuracy: 0.8242\n",
      "Epoch 78/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5895 - accuracy: 0.7896\n",
      "Epoch 78: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5895 - accuracy: 0.7896 - val_loss: 0.5221 - val_accuracy: 0.8136\n",
      "Epoch 79/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5695 - accuracy: 0.7961\n",
      "Epoch 79: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5696 - accuracy: 0.7961 - val_loss: 0.4977 - val_accuracy: 0.8216\n",
      "Epoch 80/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6048 - accuracy: 0.7862\n",
      "Epoch 80: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6048 - accuracy: 0.7862 - val_loss: 0.5135 - val_accuracy: 0.8165\n",
      "Epoch 81/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5958 - accuracy: 0.7895\n",
      "Epoch 81: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5958 - accuracy: 0.7895 - val_loss: 0.5171 - val_accuracy: 0.8164\n",
      "Epoch 82/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5680 - accuracy: 0.7967\n",
      "Epoch 82: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5680 - accuracy: 0.7967 - val_loss: 0.5034 - val_accuracy: 0.8172\n",
      "Epoch 83/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5783 - accuracy: 0.7912\n",
      "Epoch 83: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5786 - accuracy: 0.7911 - val_loss: 0.4968 - val_accuracy: 0.8224\n",
      "Epoch 84/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5930 - accuracy: 0.7893\n",
      "Epoch 84: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5930 - accuracy: 0.7892 - val_loss: 0.4922 - val_accuracy: 0.8211\n",
      "Epoch 85/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5673 - accuracy: 0.7948\n",
      "Epoch 85: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5671 - accuracy: 0.7948 - val_loss: 0.4925 - val_accuracy: 0.8203\n",
      "Epoch 86/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6135 - accuracy: 0.7829\n",
      "Epoch 86: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6135 - accuracy: 0.7829 - val_loss: 0.5126 - val_accuracy: 0.8216\n",
      "Epoch 87/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5878 - accuracy: 0.7894\n",
      "Epoch 87: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5878 - accuracy: 0.7895 - val_loss: 0.5483 - val_accuracy: 0.8076\n",
      "Epoch 88/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5905 - accuracy: 0.7907\n",
      "Epoch 88: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5904 - accuracy: 0.7906 - val_loss: 0.5136 - val_accuracy: 0.8171\n",
      "Epoch 89/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5795 - accuracy: 0.7931\n",
      "Epoch 89: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5795 - accuracy: 0.7930 - val_loss: 0.5422 - val_accuracy: 0.8066\n",
      "Epoch 90/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5881 - accuracy: 0.7906\n",
      "Epoch 90: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5881 - accuracy: 0.7907 - val_loss: 0.5114 - val_accuracy: 0.8108\n",
      "Epoch 91/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5901 - accuracy: 0.7898\n",
      "Epoch 91: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5899 - accuracy: 0.7898 - val_loss: 0.5427 - val_accuracy: 0.8028\n",
      "Epoch 92/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5711 - accuracy: 0.7953\n",
      "Epoch 92: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5709 - accuracy: 0.7954 - val_loss: 0.5144 - val_accuracy: 0.8180\n",
      "Epoch 93/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5679 - accuracy: 0.7969\n",
      "Epoch 93: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5680 - accuracy: 0.7968 - val_loss: 0.5109 - val_accuracy: 0.8163\n",
      "Epoch 94/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5701 - accuracy: 0.7939\n",
      "Epoch 94: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5701 - accuracy: 0.7938 - val_loss: 0.5215 - val_accuracy: 0.8207\n",
      "Epoch 95/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5768 - accuracy: 0.7944\n",
      "Epoch 95: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5767 - accuracy: 0.7944 - val_loss: 0.4857 - val_accuracy: 0.8239\n",
      "Epoch 96/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5846 - accuracy: 0.7906\n",
      "Epoch 96: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5846 - accuracy: 0.7906 - val_loss: 0.5307 - val_accuracy: 0.8190\n",
      "Epoch 97/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5741 - accuracy: 0.7931\n",
      "Epoch 97: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5747 - accuracy: 0.7929 - val_loss: 0.7257 - val_accuracy: 0.7634\n",
      "Epoch 98/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5931 - accuracy: 0.7901\n",
      "Epoch 98: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5930 - accuracy: 0.7901 - val_loss: 0.4987 - val_accuracy: 0.8233\n",
      "Epoch 99/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5895 - accuracy: 0.7917\n",
      "Epoch 99: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5895 - accuracy: 0.7917 - val_loss: 0.4882 - val_accuracy: 0.8231\n",
      "Epoch 100/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5798 - accuracy: 0.7928\n",
      "Epoch 100: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5799 - accuracy: 0.7927 - val_loss: 0.4938 - val_accuracy: 0.8224\n",
      "Epoch 101/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5813 - accuracy: 0.7924\n",
      "Epoch 101: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5813 - accuracy: 0.7924 - val_loss: 0.5218 - val_accuracy: 0.8103\n",
      "Epoch 102/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5808 - accuracy: 0.7914\n",
      "Epoch 102: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5808 - accuracy: 0.7914 - val_loss: 0.5047 - val_accuracy: 0.8113\n",
      "Epoch 103/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6930 - accuracy: 0.7781\n",
      "Epoch 103: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6928 - accuracy: 0.7782 - val_loss: 0.5292 - val_accuracy: 0.8137\n",
      "Epoch 104/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6647 - accuracy: 0.7651\n",
      "Epoch 104: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6647 - accuracy: 0.7651 - val_loss: 1.0126 - val_accuracy: 0.6444\n",
      "Epoch 105/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7627 - accuracy: 0.7319\n",
      "Epoch 105: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7627 - accuracy: 0.7319 - val_loss: 0.6056 - val_accuracy: 0.7788\n",
      "Epoch 106/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5960 - accuracy: 0.7886\n",
      "Epoch 106: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5960 - accuracy: 0.7887 - val_loss: 0.5113 - val_accuracy: 0.8156\n",
      "Epoch 107/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5799 - accuracy: 0.7924\n",
      "Epoch 107: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5798 - accuracy: 0.7924 - val_loss: 0.5032 - val_accuracy: 0.8238\n",
      "Epoch 108/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5748 - accuracy: 0.7923\n",
      "Epoch 108: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5749 - accuracy: 0.7922 - val_loss: 0.5218 - val_accuracy: 0.8120\n",
      "Epoch 109/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5902 - accuracy: 0.7881\n",
      "Epoch 109: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5903 - accuracy: 0.7881 - val_loss: 0.4840 - val_accuracy: 0.8284\n",
      "Epoch 110/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5768 - accuracy: 0.7921\n",
      "Epoch 110: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5769 - accuracy: 0.7921 - val_loss: 0.4889 - val_accuracy: 0.8219\n",
      "Epoch 111/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5825 - accuracy: 0.7927\n",
      "Epoch 111: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5824 - accuracy: 0.7927 - val_loss: 0.5057 - val_accuracy: 0.8173\n",
      "Epoch 112/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5751 - accuracy: 0.7943\n",
      "Epoch 112: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5751 - accuracy: 0.7943 - val_loss: 0.5206 - val_accuracy: 0.8136\n",
      "Epoch 113/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6321 - accuracy: 0.7775\n",
      "Epoch 113: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6322 - accuracy: 0.7775 - val_loss: 0.5184 - val_accuracy: 0.8193\n",
      "Epoch 114/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5781 - accuracy: 0.7940\n",
      "Epoch 114: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5781 - accuracy: 0.7940 - val_loss: 0.5166 - val_accuracy: 0.8149\n",
      "Epoch 115/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5810 - accuracy: 0.7924\n",
      "Epoch 115: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5816 - accuracy: 0.7923 - val_loss: 0.5064 - val_accuracy: 0.8235\n",
      "Epoch 116/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6237 - accuracy: 0.7780\n",
      "Epoch 116: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6237 - accuracy: 0.7781 - val_loss: 0.4862 - val_accuracy: 0.8218\n",
      "Epoch 117/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5696 - accuracy: 0.7948\n",
      "Epoch 117: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5698 - accuracy: 0.7948 - val_loss: 0.4877 - val_accuracy: 0.8261\n",
      "Epoch 118/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5877 - accuracy: 0.7910\n",
      "Epoch 118: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5877 - accuracy: 0.7910 - val_loss: 0.4954 - val_accuracy: 0.8220\n",
      "Epoch 119/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5699 - accuracy: 0.7943\n",
      "Epoch 119: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5699 - accuracy: 0.7943 - val_loss: 0.5344 - val_accuracy: 0.8099\n",
      "Epoch 120/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5974 - accuracy: 0.7870\n",
      "Epoch 120: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5974 - accuracy: 0.7869 - val_loss: 0.5095 - val_accuracy: 0.8157\n",
      "Epoch 121/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5625 - accuracy: 0.7975\n",
      "Epoch 121: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5624 - accuracy: 0.7976 - val_loss: 0.5037 - val_accuracy: 0.8234\n",
      "Epoch 122/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5806 - accuracy: 0.7944\n",
      "Epoch 122: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5807 - accuracy: 0.7943 - val_loss: 0.4891 - val_accuracy: 0.8279\n",
      "Epoch 123/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5839 - accuracy: 0.7912\n",
      "Epoch 123: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5841 - accuracy: 0.7912 - val_loss: 0.5002 - val_accuracy: 0.8196\n",
      "Epoch 124/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5694 - accuracy: 0.7955\n",
      "Epoch 124: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5692 - accuracy: 0.7955 - val_loss: 0.4936 - val_accuracy: 0.8253\n",
      "Epoch 125/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5795 - accuracy: 0.7940\n",
      "Epoch 125: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5796 - accuracy: 0.7940 - val_loss: 0.5156 - val_accuracy: 0.8190\n",
      "Epoch 126/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6127 - accuracy: 0.7833\n",
      "Epoch 126: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6126 - accuracy: 0.7833 - val_loss: 0.5237 - val_accuracy: 0.8206\n",
      "Epoch 127/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5840 - accuracy: 0.7914\n",
      "Epoch 127: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5839 - accuracy: 0.7914 - val_loss: 0.4966 - val_accuracy: 0.8205\n",
      "Epoch 128/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5687 - accuracy: 0.7968\n",
      "Epoch 128: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5686 - accuracy: 0.7968 - val_loss: 0.4892 - val_accuracy: 0.8251\n",
      "Epoch 129/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5711 - accuracy: 0.7950\n",
      "Epoch 129: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5711 - accuracy: 0.7950 - val_loss: 0.6061 - val_accuracy: 0.7933\n",
      "Epoch 130/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5835 - accuracy: 0.7917\n",
      "Epoch 130: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5834 - accuracy: 0.7917 - val_loss: 0.5548 - val_accuracy: 0.8044\n",
      "Epoch 131/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5802 - accuracy: 0.7929\n",
      "Epoch 131: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5804 - accuracy: 0.7928 - val_loss: 0.5311 - val_accuracy: 0.8111\n",
      "Epoch 132/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6115 - accuracy: 0.7837\n",
      "Epoch 132: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6117 - accuracy: 0.7835 - val_loss: 0.6114 - val_accuracy: 0.7934\n",
      "Epoch 133/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5794 - accuracy: 0.7917\n",
      "Epoch 133: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5793 - accuracy: 0.7917 - val_loss: 0.4817 - val_accuracy: 0.8294\n",
      "Epoch 134/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5719 - accuracy: 0.7975\n",
      "Epoch 134: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5721 - accuracy: 0.7975 - val_loss: 0.5224 - val_accuracy: 0.8209\n",
      "Epoch 135/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5774 - accuracy: 0.7945\n",
      "Epoch 135: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5774 - accuracy: 0.7945 - val_loss: 0.5000 - val_accuracy: 0.8240\n",
      "Epoch 136/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5665 - accuracy: 0.7965\n",
      "Epoch 136: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5666 - accuracy: 0.7965 - val_loss: 0.4956 - val_accuracy: 0.8210\n",
      "Epoch 137/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5611 - accuracy: 0.7989\n",
      "Epoch 137: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5611 - accuracy: 0.7990 - val_loss: 0.4773 - val_accuracy: 0.8255\n",
      "Epoch 138/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5842 - accuracy: 0.7913\n",
      "Epoch 138: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5839 - accuracy: 0.7914 - val_loss: 0.4930 - val_accuracy: 0.8218\n",
      "Epoch 139/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5929 - accuracy: 0.7896\n",
      "Epoch 139: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5930 - accuracy: 0.7896 - val_loss: 0.6186 - val_accuracy: 0.7880\n",
      "Epoch 140/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6883 - accuracy: 0.7645\n",
      "Epoch 140: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6884 - accuracy: 0.7645 - val_loss: 0.6608 - val_accuracy: 0.7758\n",
      "Epoch 141/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6670 - accuracy: 0.7685\n",
      "Epoch 141: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6669 - accuracy: 0.7685 - val_loss: 0.5416 - val_accuracy: 0.8094\n",
      "Epoch 142/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6320 - accuracy: 0.7798\n",
      "Epoch 142: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6324 - accuracy: 0.7798 - val_loss: 0.5375 - val_accuracy: 0.8071\n",
      "Epoch 143/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6788 - accuracy: 0.7634\n",
      "Epoch 143: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6785 - accuracy: 0.7634 - val_loss: 0.5172 - val_accuracy: 0.8162\n",
      "Epoch 144/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5825 - accuracy: 0.7906\n",
      "Epoch 144: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5823 - accuracy: 0.7906 - val_loss: 0.4891 - val_accuracy: 0.8239\n",
      "Epoch 145/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7412 - accuracy: 0.7453\n",
      "Epoch 145: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7412 - accuracy: 0.7453 - val_loss: 0.5555 - val_accuracy: 0.8073\n",
      "Epoch 146/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6364 - accuracy: 0.7756\n",
      "Epoch 146: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6363 - accuracy: 0.7757 - val_loss: 0.5690 - val_accuracy: 0.8028\n",
      "Epoch 147/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5964 - accuracy: 0.7892\n",
      "Epoch 147: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5965 - accuracy: 0.7891 - val_loss: 0.5432 - val_accuracy: 0.8084\n",
      "Epoch 148/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6262 - accuracy: 0.7795\n",
      "Epoch 148: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6261 - accuracy: 0.7795 - val_loss: 0.5507 - val_accuracy: 0.8093\n",
      "Epoch 149/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5886 - accuracy: 0.7888\n",
      "Epoch 149: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5885 - accuracy: 0.7888 - val_loss: 0.4938 - val_accuracy: 0.8225\n",
      "Epoch 150/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5686 - accuracy: 0.7957\n",
      "Epoch 150: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5686 - accuracy: 0.7957 - val_loss: 0.4969 - val_accuracy: 0.8224\n",
      "Epoch 151/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5755 - accuracy: 0.7951\n",
      "Epoch 151: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5754 - accuracy: 0.7952 - val_loss: 0.4901 - val_accuracy: 0.8273\n",
      "Epoch 152/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6259 - accuracy: 0.7777\n",
      "Epoch 152: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6263 - accuracy: 0.7776 - val_loss: 0.5895 - val_accuracy: 0.7911\n",
      "Epoch 153/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6739 - accuracy: 0.7636\n",
      "Epoch 153: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6737 - accuracy: 0.7637 - val_loss: 0.5486 - val_accuracy: 0.8077\n",
      "Epoch 154/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6350 - accuracy: 0.7761\n",
      "Epoch 154: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6349 - accuracy: 0.7762 - val_loss: 0.5647 - val_accuracy: 0.8081\n",
      "Epoch 155/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6051 - accuracy: 0.7853\n",
      "Epoch 155: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6049 - accuracy: 0.7853 - val_loss: 0.5808 - val_accuracy: 0.7971\n",
      "Epoch 156/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5821 - accuracy: 0.7925\n",
      "Epoch 156: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5821 - accuracy: 0.7925 - val_loss: 0.4934 - val_accuracy: 0.8254\n",
      "Epoch 157/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6052 - accuracy: 0.7855\n",
      "Epoch 157: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6051 - accuracy: 0.7855 - val_loss: 0.5143 - val_accuracy: 0.8198\n",
      "Epoch 158/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6088 - accuracy: 0.7833\n",
      "Epoch 158: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6089 - accuracy: 0.7833 - val_loss: 0.5139 - val_accuracy: 0.8148\n",
      "Epoch 159/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5714 - accuracy: 0.7955\n",
      "Epoch 159: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5714 - accuracy: 0.7955 - val_loss: 0.5105 - val_accuracy: 0.8207\n",
      "Epoch 160/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6260 - accuracy: 0.7829\n",
      "Epoch 160: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6259 - accuracy: 0.7829 - val_loss: 0.5002 - val_accuracy: 0.8196\n",
      "Epoch 161/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6677 - accuracy: 0.7681\n",
      "Epoch 161: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6675 - accuracy: 0.7682 - val_loss: 0.5153 - val_accuracy: 0.8149\n",
      "Epoch 162/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5870 - accuracy: 0.7892\n",
      "Epoch 162: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5871 - accuracy: 0.7893 - val_loss: 0.5049 - val_accuracy: 0.8205\n",
      "Epoch 163/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5943 - accuracy: 0.7870\n",
      "Epoch 163: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5943 - accuracy: 0.7870 - val_loss: 0.5396 - val_accuracy: 0.8090\n",
      "Epoch 164/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5977 - accuracy: 0.7874\n",
      "Epoch 164: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5976 - accuracy: 0.7874 - val_loss: 0.4933 - val_accuracy: 0.8210\n",
      "Epoch 165/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5919 - accuracy: 0.7886\n",
      "Epoch 165: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5922 - accuracy: 0.7885 - val_loss: 0.5421 - val_accuracy: 0.8048\n",
      "Epoch 166/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6054 - accuracy: 0.7842\n",
      "Epoch 166: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6053 - accuracy: 0.7842 - val_loss: 0.5026 - val_accuracy: 0.8193\n",
      "Epoch 167/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5776 - accuracy: 0.7923\n",
      "Epoch 167: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5776 - accuracy: 0.7922 - val_loss: 0.4885 - val_accuracy: 0.8223\n",
      "Epoch 168/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5944 - accuracy: 0.7911\n",
      "Epoch 168: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5943 - accuracy: 0.7911 - val_loss: 0.5250 - val_accuracy: 0.8134\n",
      "Epoch 169/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5986 - accuracy: 0.7868\n",
      "Epoch 169: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5986 - accuracy: 0.7868 - val_loss: 0.5329 - val_accuracy: 0.8119\n",
      "Epoch 170/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5810 - accuracy: 0.7925\n",
      "Epoch 170: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5809 - accuracy: 0.7925 - val_loss: 0.4744 - val_accuracy: 0.8299\n",
      "Epoch 171/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6342 - accuracy: 0.7782\n",
      "Epoch 171: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6342 - accuracy: 0.7782 - val_loss: 0.6665 - val_accuracy: 0.7718\n",
      "Epoch 172/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5956 - accuracy: 0.7885\n",
      "Epoch 172: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5954 - accuracy: 0.7886 - val_loss: 0.4788 - val_accuracy: 0.8281\n",
      "Epoch 173/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5719 - accuracy: 0.7963\n",
      "Epoch 173: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 10s 13ms/step - loss: 0.5722 - accuracy: 0.7963 - val_loss: 0.5107 - val_accuracy: 0.8228\n",
      "Epoch 174/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5722 - accuracy: 0.7944\n",
      "Epoch 174: val_loss did not improve from 0.46189\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6155 - accuracy: 0.7831\n",
      "Epoch 175: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6154 - accuracy: 0.7832 - val_loss: 0.5175 - val_accuracy: 0.8158\n",
      "Epoch 176/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5778 - accuracy: 0.7955\n",
      "Epoch 176: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5778 - accuracy: 0.7955 - val_loss: 0.4728 - val_accuracy: 0.8282\n",
      "Epoch 177/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8428 - accuracy: 0.7097\n",
      "Epoch 177: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8427 - accuracy: 0.7097 - val_loss: 0.6337 - val_accuracy: 0.7825\n",
      "Epoch 178/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6442 - accuracy: 0.7730\n",
      "Epoch 178: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6442 - accuracy: 0.7730 - val_loss: 0.4949 - val_accuracy: 0.8232\n",
      "Epoch 179/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6431 - accuracy: 0.7705\n",
      "Epoch 179: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6431 - accuracy: 0.7705 - val_loss: 0.5423 - val_accuracy: 0.8052\n",
      "Epoch 180/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5746 - accuracy: 0.7950\n",
      "Epoch 180: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5747 - accuracy: 0.7950 - val_loss: 0.4980 - val_accuracy: 0.8212\n",
      "Epoch 181/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5939 - accuracy: 0.7901\n",
      "Epoch 181: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5939 - accuracy: 0.7901 - val_loss: 0.4862 - val_accuracy: 0.8283\n",
      "Epoch 182/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5817 - accuracy: 0.7928\n",
      "Epoch 182: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5817 - accuracy: 0.7928 - val_loss: 0.4925 - val_accuracy: 0.8217\n",
      "Epoch 183/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.9438 - accuracy: 0.6741\n",
      "Epoch 183: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9441 - accuracy: 0.6741 - val_loss: 0.7646 - val_accuracy: 0.7396\n",
      "Epoch 184/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7379 - accuracy: 0.7454\n",
      "Epoch 184: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7377 - accuracy: 0.7455 - val_loss: 0.5458 - val_accuracy: 0.8110\n",
      "Epoch 185/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6060 - accuracy: 0.7864\n",
      "Epoch 185: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6060 - accuracy: 0.7864 - val_loss: 0.5437 - val_accuracy: 0.8096\n",
      "Epoch 186/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5880 - accuracy: 0.7900\n",
      "Epoch 186: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 10s 12ms/step - loss: 0.5880 - accuracy: 0.7900 - val_loss: 0.5090 - val_accuracy: 0.8187\n",
      "Epoch 187/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6371 - accuracy: 0.7786\n",
      "Epoch 187: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6372 - accuracy: 0.7787 - val_loss: 0.5129 - val_accuracy: 0.8116\n",
      "Epoch 188/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5880 - accuracy: 0.7901\n",
      "Epoch 188: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5880 - accuracy: 0.7901 - val_loss: 0.5311 - val_accuracy: 0.8059\n",
      "Epoch 189/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5877 - accuracy: 0.7924\n",
      "Epoch 189: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5877 - accuracy: 0.7924 - val_loss: 0.4959 - val_accuracy: 0.8249\n",
      "Epoch 190/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6157 - accuracy: 0.7807\n",
      "Epoch 190: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6157 - accuracy: 0.7807 - val_loss: 0.5210 - val_accuracy: 0.8192\n",
      "Epoch 191/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7572 - accuracy: 0.7342\n",
      "Epoch 191: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7571 - accuracy: 0.7343 - val_loss: 0.5981 - val_accuracy: 0.7935\n",
      "Epoch 192/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6032 - accuracy: 0.7851\n",
      "Epoch 192: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6031 - accuracy: 0.7851 - val_loss: 0.5140 - val_accuracy: 0.8146\n",
      "Epoch 193/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5957 - accuracy: 0.7877\n",
      "Epoch 193: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5958 - accuracy: 0.7877 - val_loss: 0.5070 - val_accuracy: 0.8196\n",
      "Epoch 194/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6120 - accuracy: 0.7839\n",
      "Epoch 194: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6119 - accuracy: 0.7840 - val_loss: 0.5438 - val_accuracy: 0.8136\n",
      "Epoch 195/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5849 - accuracy: 0.7910\n",
      "Epoch 195: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5849 - accuracy: 0.7910 - val_loss: 0.5011 - val_accuracy: 0.8263\n",
      "Epoch 196/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6035 - accuracy: 0.7875\n",
      "Epoch 196: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6036 - accuracy: 0.7874 - val_loss: 0.6746 - val_accuracy: 0.7775\n",
      "Epoch 197/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5918 - accuracy: 0.7890\n",
      "Epoch 197: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5919 - accuracy: 0.7890 - val_loss: 0.5182 - val_accuracy: 0.8199\n",
      "Epoch 198/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5941 - accuracy: 0.7899\n",
      "Epoch 198: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5941 - accuracy: 0.7899 - val_loss: 0.5812 - val_accuracy: 0.8024\n",
      "Epoch 199/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5935 - accuracy: 0.7893\n",
      "Epoch 199: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5932 - accuracy: 0.7896 - val_loss: 0.5151 - val_accuracy: 0.8202\n",
      "Epoch 200/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5804 - accuracy: 0.7936\n",
      "Epoch 200: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5805 - accuracy: 0.7936 - val_loss: 0.5097 - val_accuracy: 0.8202\n",
      "Epoch 201/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6020 - accuracy: 0.7870\n",
      "Epoch 201: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6021 - accuracy: 0.7870 - val_loss: 0.6460 - val_accuracy: 0.7797\n",
      "Epoch 202/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5890 - accuracy: 0.7919\n",
      "Epoch 202: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5890 - accuracy: 0.7919 - val_loss: 0.4877 - val_accuracy: 0.8254\n",
      "Epoch 203/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6025 - accuracy: 0.7859\n",
      "Epoch 203: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6025 - accuracy: 0.7858 - val_loss: 0.5206 - val_accuracy: 0.8136\n",
      "Epoch 204/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5988 - accuracy: 0.7902\n",
      "Epoch 204: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5987 - accuracy: 0.7903 - val_loss: 0.5109 - val_accuracy: 0.8177\n",
      "Epoch 205/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5724 - accuracy: 0.7958\n",
      "Epoch 205: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5725 - accuracy: 0.7959 - val_loss: 0.4973 - val_accuracy: 0.8302\n",
      "Epoch 206/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5873 - accuracy: 0.7925\n",
      "Epoch 206: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5876 - accuracy: 0.7924 - val_loss: 0.5093 - val_accuracy: 0.8203\n",
      "Epoch 207/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5796 - accuracy: 0.7933\n",
      "Epoch 207: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5800 - accuracy: 0.7932 - val_loss: 0.5307 - val_accuracy: 0.8118\n",
      "Epoch 208/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5977 - accuracy: 0.7884\n",
      "Epoch 208: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5976 - accuracy: 0.7884 - val_loss: 0.5105 - val_accuracy: 0.8219\n",
      "Epoch 209/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5806 - accuracy: 0.7928\n",
      "Epoch 209: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5808 - accuracy: 0.7927 - val_loss: 0.5224 - val_accuracy: 0.8148\n",
      "Epoch 210/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5947 - accuracy: 0.7894\n",
      "Epoch 210: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5946 - accuracy: 0.7894 - val_loss: 0.4866 - val_accuracy: 0.8260\n",
      "Epoch 211/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5842 - accuracy: 0.7923\n",
      "Epoch 211: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5842 - accuracy: 0.7923 - val_loss: 0.5571 - val_accuracy: 0.8068\n",
      "Epoch 212/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5971 - accuracy: 0.7884\n",
      "Epoch 212: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5970 - accuracy: 0.7885 - val_loss: 0.4964 - val_accuracy: 0.8194\n",
      "Epoch 213/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5855 - accuracy: 0.7926\n",
      "Epoch 213: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5855 - accuracy: 0.7926 - val_loss: 0.5231 - val_accuracy: 0.8172\n",
      "Epoch 214/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6148 - accuracy: 0.7818\n",
      "Epoch 214: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6148 - accuracy: 0.7818 - val_loss: 0.5349 - val_accuracy: 0.8139\n",
      "Epoch 215/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5602 - accuracy: 0.7991\n",
      "Epoch 215: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5601 - accuracy: 0.7991 - val_loss: 0.4931 - val_accuracy: 0.8225\n",
      "Epoch 216/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5719 - accuracy: 0.7974\n",
      "Epoch 216: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5719 - accuracy: 0.7974 - val_loss: 0.5124 - val_accuracy: 0.8185\n",
      "Epoch 217/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5875 - accuracy: 0.7920\n",
      "Epoch 217: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5873 - accuracy: 0.7920 - val_loss: 0.4850 - val_accuracy: 0.8306\n",
      "Epoch 218/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5913 - accuracy: 0.7912\n",
      "Epoch 218: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5916 - accuracy: 0.7912 - val_loss: 0.4738 - val_accuracy: 0.8327\n",
      "Epoch 219/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6077 - accuracy: 0.7848\n",
      "Epoch 219: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6079 - accuracy: 0.7849 - val_loss: 0.5636 - val_accuracy: 0.8192\n",
      "Epoch 220/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5905 - accuracy: 0.7907\n",
      "Epoch 220: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5904 - accuracy: 0.7907 - val_loss: 0.4789 - val_accuracy: 0.8253\n",
      "Epoch 221/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5908 - accuracy: 0.7928\n",
      "Epoch 221: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5908 - accuracy: 0.7928 - val_loss: 0.4953 - val_accuracy: 0.8238\n",
      "Epoch 222/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6396 - accuracy: 0.7790\n",
      "Epoch 222: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6397 - accuracy: 0.7789 - val_loss: 0.5645 - val_accuracy: 0.8059\n",
      "Epoch 223/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6073 - accuracy: 0.7842\n",
      "Epoch 223: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6074 - accuracy: 0.7842 - val_loss: 0.5119 - val_accuracy: 0.8210\n",
      "Epoch 224/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5746 - accuracy: 0.7943\n",
      "Epoch 224: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5749 - accuracy: 0.7941 - val_loss: 0.5209 - val_accuracy: 0.8180\n",
      "Epoch 225/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5634 - accuracy: 0.7970\n",
      "Epoch 225: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5636 - accuracy: 0.7969 - val_loss: 0.4764 - val_accuracy: 0.8293\n",
      "Epoch 226/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5917 - accuracy: 0.7901\n",
      "Epoch 226: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5916 - accuracy: 0.7902 - val_loss: 0.5018 - val_accuracy: 0.8223\n",
      "Epoch 227/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5997 - accuracy: 0.7877\n",
      "Epoch 227: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5998 - accuracy: 0.7877 - val_loss: 0.5556 - val_accuracy: 0.7965\n",
      "Epoch 228/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5660 - accuracy: 0.7976\n",
      "Epoch 228: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5659 - accuracy: 0.7977 - val_loss: 0.4932 - val_accuracy: 0.8228\n",
      "Epoch 229/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5845 - accuracy: 0.7921\n",
      "Epoch 229: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5850 - accuracy: 0.7920 - val_loss: 1.0141 - val_accuracy: 0.6718\n",
      "Epoch 230/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6556 - accuracy: 0.7721\n",
      "Epoch 230: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6551 - accuracy: 0.7724 - val_loss: 0.4903 - val_accuracy: 0.8255\n",
      "Epoch 231/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5687 - accuracy: 0.7956\n",
      "Epoch 231: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5686 - accuracy: 0.7956 - val_loss: 0.4999 - val_accuracy: 0.8211\n",
      "Epoch 232/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5837 - accuracy: 0.7921\n",
      "Epoch 232: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5837 - accuracy: 0.7921 - val_loss: 0.5175 - val_accuracy: 0.8220\n",
      "Epoch 233/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5631 - accuracy: 0.7996\n",
      "Epoch 233: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5631 - accuracy: 0.7996 - val_loss: 0.4866 - val_accuracy: 0.8274\n",
      "Epoch 234/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5607 - accuracy: 0.8002\n",
      "Epoch 234: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5607 - accuracy: 0.8002 - val_loss: 0.4680 - val_accuracy: 0.8303\n",
      "Epoch 235/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6711 - accuracy: 0.7642\n",
      "Epoch 235: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6715 - accuracy: 0.7641 - val_loss: 0.7760 - val_accuracy: 0.7307\n",
      "Epoch 236/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6598 - accuracy: 0.7652\n",
      "Epoch 236: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6597 - accuracy: 0.7653 - val_loss: 0.5516 - val_accuracy: 0.7970\n",
      "Epoch 237/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6106 - accuracy: 0.7829\n",
      "Epoch 237: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6106 - accuracy: 0.7829 - val_loss: 0.4866 - val_accuracy: 0.8260\n",
      "Epoch 238/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5785 - accuracy: 0.7955\n",
      "Epoch 238: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5785 - accuracy: 0.7955 - val_loss: 0.7871 - val_accuracy: 0.7476\n",
      "Epoch 239/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5924 - accuracy: 0.7904\n",
      "Epoch 239: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5927 - accuracy: 0.7903 - val_loss: 0.5028 - val_accuracy: 0.8187\n",
      "Epoch 240/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5803 - accuracy: 0.7939\n",
      "Epoch 240: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 10s 13ms/step - loss: 0.5807 - accuracy: 0.7938 - val_loss: 0.4947 - val_accuracy: 0.8222\n",
      "Epoch 241/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5744 - accuracy: 0.7956\n",
      "Epoch 241: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5746 - accuracy: 0.7956 - val_loss: 0.4912 - val_accuracy: 0.8229\n",
      "Epoch 242/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6170 - accuracy: 0.7824\n",
      "Epoch 242: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6171 - accuracy: 0.7824 - val_loss: 0.4851 - val_accuracy: 0.8280\n",
      "Epoch 243/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5815 - accuracy: 0.7934\n",
      "Epoch 243: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5815 - accuracy: 0.7933 - val_loss: 0.5636 - val_accuracy: 0.7987\n",
      "Epoch 244/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5856 - accuracy: 0.7925\n",
      "Epoch 244: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5853 - accuracy: 0.7926 - val_loss: 0.5132 - val_accuracy: 0.8144\n",
      "Epoch 245/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5826 - accuracy: 0.7927\n",
      "Epoch 245: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5826 - accuracy: 0.7927 - val_loss: 0.4900 - val_accuracy: 0.8280\n",
      "Epoch 246/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5715 - accuracy: 0.7964\n",
      "Epoch 246: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5714 - accuracy: 0.7965 - val_loss: 0.5220 - val_accuracy: 0.8146\n",
      "Epoch 247/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5636 - accuracy: 0.7989\n",
      "Epoch 247: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5636 - accuracy: 0.7988 - val_loss: 0.4994 - val_accuracy: 0.8221\n",
      "Epoch 248/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5699 - accuracy: 0.7968\n",
      "Epoch 248: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5698 - accuracy: 0.7968 - val_loss: 0.5351 - val_accuracy: 0.8139\n",
      "Epoch 249/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6131 - accuracy: 0.7845\n",
      "Epoch 249: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6130 - accuracy: 0.7845 - val_loss: 0.5001 - val_accuracy: 0.8239\n",
      "Epoch 250/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5617 - accuracy: 0.7988\n",
      "Epoch 250: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5621 - accuracy: 0.7988 - val_loss: 0.4920 - val_accuracy: 0.8190\n",
      "Epoch 251/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6765 - accuracy: 0.7635\n",
      "Epoch 251: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6765 - accuracy: 0.7635 - val_loss: 0.5148 - val_accuracy: 0.8200\n",
      "Epoch 252/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5912 - accuracy: 0.7884\n",
      "Epoch 252: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5910 - accuracy: 0.7886 - val_loss: 0.5068 - val_accuracy: 0.8209\n",
      "Epoch 253/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5770 - accuracy: 0.7943\n",
      "Epoch 253: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5770 - accuracy: 0.7943 - val_loss: 0.5207 - val_accuracy: 0.8136\n",
      "Epoch 254/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5733 - accuracy: 0.7954\n",
      "Epoch 254: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5734 - accuracy: 0.7954 - val_loss: 0.5784 - val_accuracy: 0.7903\n",
      "Epoch 255/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6372 - accuracy: 0.7785\n",
      "Epoch 255: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6371 - accuracy: 0.7785 - val_loss: 0.4948 - val_accuracy: 0.8220\n",
      "Epoch 256/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5855 - accuracy: 0.7902\n",
      "Epoch 256: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5856 - accuracy: 0.7901 - val_loss: 0.4997 - val_accuracy: 0.8231\n",
      "Epoch 257/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6032 - accuracy: 0.7887\n",
      "Epoch 257: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6032 - accuracy: 0.7887 - val_loss: 0.4849 - val_accuracy: 0.8279\n",
      "Epoch 258/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5776 - accuracy: 0.7944\n",
      "Epoch 258: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5775 - accuracy: 0.7945 - val_loss: 0.5195 - val_accuracy: 0.8173\n",
      "Epoch 259/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5910 - accuracy: 0.7908\n",
      "Epoch 259: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5912 - accuracy: 0.7907 - val_loss: 0.5479 - val_accuracy: 0.8119\n",
      "Epoch 260/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5844 - accuracy: 0.7918\n",
      "Epoch 260: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5843 - accuracy: 0.7919 - val_loss: 0.5124 - val_accuracy: 0.8187\n",
      "Epoch 261/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5908 - accuracy: 0.7905\n",
      "Epoch 261: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5908 - accuracy: 0.7906 - val_loss: 0.5503 - val_accuracy: 0.8020\n",
      "Epoch 262/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5970 - accuracy: 0.7878\n",
      "Epoch 262: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5968 - accuracy: 0.7879 - val_loss: 0.4988 - val_accuracy: 0.8206\n",
      "Epoch 263/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5703 - accuracy: 0.7976\n",
      "Epoch 263: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5705 - accuracy: 0.7977 - val_loss: 0.4939 - val_accuracy: 0.8277\n",
      "Epoch 264/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6120 - accuracy: 0.7846\n",
      "Epoch 264: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6120 - accuracy: 0.7846 - val_loss: 0.5132 - val_accuracy: 0.8094\n",
      "Epoch 265/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6129 - accuracy: 0.7838\n",
      "Epoch 265: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6128 - accuracy: 0.7838 - val_loss: 0.5160 - val_accuracy: 0.8196\n",
      "Epoch 266/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5982 - accuracy: 0.7893\n",
      "Epoch 266: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5985 - accuracy: 0.7892 - val_loss: 0.5124 - val_accuracy: 0.8180\n",
      "Epoch 267/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6386 - accuracy: 0.7746\n",
      "Epoch 267: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6384 - accuracy: 0.7747 - val_loss: 0.6969 - val_accuracy: 0.7591\n",
      "Epoch 268/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7305 - accuracy: 0.7459\n",
      "Epoch 268: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7305 - accuracy: 0.7459 - val_loss: 0.5099 - val_accuracy: 0.8186\n",
      "Epoch 269/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5850 - accuracy: 0.7931\n",
      "Epoch 269: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5848 - accuracy: 0.7931 - val_loss: 0.5002 - val_accuracy: 0.8251\n",
      "Epoch 270/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5669 - accuracy: 0.7969\n",
      "Epoch 270: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5670 - accuracy: 0.7968 - val_loss: 0.5051 - val_accuracy: 0.8250\n",
      "Epoch 271/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5708 - accuracy: 0.7958\n",
      "Epoch 271: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5708 - accuracy: 0.7957 - val_loss: 0.4992 - val_accuracy: 0.8233\n",
      "Epoch 272/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5877 - accuracy: 0.7918\n",
      "Epoch 272: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5879 - accuracy: 0.7918 - val_loss: 0.5138 - val_accuracy: 0.8210\n",
      "Epoch 273/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5959 - accuracy: 0.7906\n",
      "Epoch 273: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5958 - accuracy: 0.7906 - val_loss: 0.5121 - val_accuracy: 0.8188\n",
      "Epoch 274/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5801 - accuracy: 0.7940\n",
      "Epoch 274: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5804 - accuracy: 0.7938 - val_loss: 0.6331 - val_accuracy: 0.7874\n",
      "Epoch 275/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5782 - accuracy: 0.7932\n",
      "Epoch 275: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5782 - accuracy: 0.7932 - val_loss: 0.4911 - val_accuracy: 0.8255\n",
      "Epoch 276/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6051 - accuracy: 0.7873\n",
      "Epoch 276: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6050 - accuracy: 0.7873 - val_loss: 0.5662 - val_accuracy: 0.7987\n",
      "Epoch 277/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5763 - accuracy: 0.7952\n",
      "Epoch 277: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5760 - accuracy: 0.7953 - val_loss: 0.4728 - val_accuracy: 0.8310\n",
      "Epoch 278/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6422 - accuracy: 0.7747\n",
      "Epoch 278: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6420 - accuracy: 0.7746 - val_loss: 0.4953 - val_accuracy: 0.8216\n",
      "Epoch 279/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5702 - accuracy: 0.7953\n",
      "Epoch 279: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5702 - accuracy: 0.7953 - val_loss: 0.5064 - val_accuracy: 0.8184\n",
      "Epoch 280/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5936 - accuracy: 0.7900\n",
      "Epoch 280: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5936 - accuracy: 0.7899 - val_loss: 0.5937 - val_accuracy: 0.7862\n",
      "Epoch 281/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6036 - accuracy: 0.7899\n",
      "Epoch 281: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6036 - accuracy: 0.7899 - val_loss: 0.5315 - val_accuracy: 0.8147\n",
      "Epoch 282/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6199 - accuracy: 0.7825\n",
      "Epoch 282: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6199 - accuracy: 0.7825 - val_loss: 0.5292 - val_accuracy: 0.8113\n",
      "Epoch 283/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5861 - accuracy: 0.7926\n",
      "Epoch 283: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5862 - accuracy: 0.7926 - val_loss: 0.5651 - val_accuracy: 0.7999\n",
      "Epoch 284/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6068 - accuracy: 0.7857\n",
      "Epoch 284: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6068 - accuracy: 0.7857 - val_loss: 0.4975 - val_accuracy: 0.8204\n",
      "Epoch 285/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5837 - accuracy: 0.7938\n",
      "Epoch 285: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5843 - accuracy: 0.7936 - val_loss: 0.5314 - val_accuracy: 0.8134\n",
      "Epoch 286/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5828 - accuracy: 0.7927\n",
      "Epoch 286: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5828 - accuracy: 0.7928 - val_loss: 0.5187 - val_accuracy: 0.8133\n",
      "Epoch 287/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5761 - accuracy: 0.7939\n",
      "Epoch 287: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5762 - accuracy: 0.7939 - val_loss: 0.5057 - val_accuracy: 0.8188\n",
      "Epoch 288/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7716 - accuracy: 0.7390\n",
      "Epoch 288: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7714 - accuracy: 0.7391 - val_loss: 0.6082 - val_accuracy: 0.7887\n",
      "Epoch 289/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6320 - accuracy: 0.7784\n",
      "Epoch 289: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6320 - accuracy: 0.7783 - val_loss: 0.5223 - val_accuracy: 0.8152\n",
      "Epoch 290/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6037 - accuracy: 0.7865\n",
      "Epoch 290: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6037 - accuracy: 0.7866 - val_loss: 0.5944 - val_accuracy: 0.7869\n",
      "Epoch 291/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5978 - accuracy: 0.7869\n",
      "Epoch 291: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5980 - accuracy: 0.7868 - val_loss: 0.6594 - val_accuracy: 0.7787\n",
      "Epoch 292/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5993 - accuracy: 0.7862\n",
      "Epoch 292: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5993 - accuracy: 0.7862 - val_loss: 0.4798 - val_accuracy: 0.8301\n",
      "Epoch 293/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5791 - accuracy: 0.7943\n",
      "Epoch 293: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5791 - accuracy: 0.7943 - val_loss: 0.4882 - val_accuracy: 0.8283\n",
      "Epoch 294/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.5693 - accuracy: 0.7980\n",
      "Epoch 294: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5692 - accuracy: 0.7980 - val_loss: 0.4938 - val_accuracy: 0.8258\n",
      "Epoch 295/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5953 - accuracy: 0.7905\n",
      "Epoch 295: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5953 - accuracy: 0.7905 - val_loss: 0.5981 - val_accuracy: 0.7973\n",
      "Epoch 296/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7280 - accuracy: 0.7473\n",
      "Epoch 296: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7280 - accuracy: 0.7473 - val_loss: 0.5531 - val_accuracy: 0.8100\n",
      "Epoch 297/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6003 - accuracy: 0.7886\n",
      "Epoch 297: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6005 - accuracy: 0.7886 - val_loss: 0.5096 - val_accuracy: 0.8244\n",
      "Epoch 298/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6952 - accuracy: 0.7599\n",
      "Epoch 298: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6953 - accuracy: 0.7598 - val_loss: 0.7576 - val_accuracy: 0.7439\n",
      "Epoch 299/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6533 - accuracy: 0.7703\n",
      "Epoch 299: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6536 - accuracy: 0.7702 - val_loss: 0.5492 - val_accuracy: 0.8066\n",
      "Epoch 300/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6249 - accuracy: 0.7825\n",
      "Epoch 300: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6249 - accuracy: 0.7825 - val_loss: 0.5757 - val_accuracy: 0.7963\n",
      "Epoch 301/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5784 - accuracy: 0.7947\n",
      "Epoch 301: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5783 - accuracy: 0.7948 - val_loss: 0.5381 - val_accuracy: 0.8114\n",
      "Epoch 302/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6064 - accuracy: 0.7849\n",
      "Epoch 302: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6064 - accuracy: 0.7849 - val_loss: 0.4948 - val_accuracy: 0.8239\n",
      "Epoch 303/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5846 - accuracy: 0.7932\n",
      "Epoch 303: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5845 - accuracy: 0.7932 - val_loss: 0.4831 - val_accuracy: 0.8260\n",
      "Epoch 304/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5843 - accuracy: 0.7932\n",
      "Epoch 304: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5844 - accuracy: 0.7931 - val_loss: 0.5096 - val_accuracy: 0.8196\n",
      "Epoch 305/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6019 - accuracy: 0.7869\n",
      "Epoch 305: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6020 - accuracy: 0.7869 - val_loss: 0.5867 - val_accuracy: 0.7960\n",
      "Epoch 306/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6283 - accuracy: 0.7794\n",
      "Epoch 306: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6287 - accuracy: 0.7792 - val_loss: 1.0282 - val_accuracy: 0.6385\n",
      "Epoch 307/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8417 - accuracy: 0.7020\n",
      "Epoch 307: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8416 - accuracy: 0.7020 - val_loss: 0.6024 - val_accuracy: 0.7924\n",
      "Epoch 308/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6203 - accuracy: 0.7821\n",
      "Epoch 308: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6203 - accuracy: 0.7821 - val_loss: 0.5152 - val_accuracy: 0.8230\n",
      "Epoch 309/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5818 - accuracy: 0.7926\n",
      "Epoch 309: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5818 - accuracy: 0.7926 - val_loss: 0.4908 - val_accuracy: 0.8256\n",
      "Epoch 310/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6227 - accuracy: 0.7834\n",
      "Epoch 310: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6227 - accuracy: 0.7834 - val_loss: 0.5443 - val_accuracy: 0.8126\n",
      "Epoch 311/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5916 - accuracy: 0.7904\n",
      "Epoch 311: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5916 - accuracy: 0.7904 - val_loss: 0.5199 - val_accuracy: 0.8154\n",
      "Epoch 312/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5899 - accuracy: 0.7919\n",
      "Epoch 312: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5899 - accuracy: 0.7919 - val_loss: 0.5091 - val_accuracy: 0.8214\n",
      "Epoch 313/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.5804 - accuracy: 0.7943\n",
      "Epoch 313: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5804 - accuracy: 0.7945 - val_loss: 0.4872 - val_accuracy: 0.8309\n",
      "Epoch 314/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6022 - accuracy: 0.7891\n",
      "Epoch 314: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6021 - accuracy: 0.7891 - val_loss: 0.4980 - val_accuracy: 0.8252\n",
      "Epoch 315/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6290 - accuracy: 0.7779\n",
      "Epoch 315: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6290 - accuracy: 0.7779 - val_loss: 0.5253 - val_accuracy: 0.8157\n",
      "Epoch 316/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5890 - accuracy: 0.7898\n",
      "Epoch 316: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5890 - accuracy: 0.7898 - val_loss: 0.4750 - val_accuracy: 0.8308\n",
      "Epoch 317/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6049 - accuracy: 0.7858\n",
      "Epoch 317: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6049 - accuracy: 0.7858 - val_loss: 0.5025 - val_accuracy: 0.8207\n",
      "Epoch 318/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5662 - accuracy: 0.7954\n",
      "Epoch 318: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5661 - accuracy: 0.7955 - val_loss: 0.5054 - val_accuracy: 0.8192\n",
      "Epoch 319/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6046 - accuracy: 0.7864\n",
      "Epoch 319: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6046 - accuracy: 0.7864 - val_loss: 0.4979 - val_accuracy: 0.8235\n",
      "Epoch 320/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8350 - accuracy: 0.7184\n",
      "Epoch 323: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8347 - accuracy: 0.7184 - val_loss: 0.6091 - val_accuracy: 0.7855\n",
      "Epoch 324/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6575 - accuracy: 0.7674\n",
      "Epoch 324: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6576 - accuracy: 0.7674 - val_loss: 0.5437 - val_accuracy: 0.8071\n",
      "Epoch 325/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6178 - accuracy: 0.7812\n",
      "Epoch 325: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6179 - accuracy: 0.7811 - val_loss: 0.5233 - val_accuracy: 0.8193\n",
      "Epoch 326/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6073 - accuracy: 0.7845\n",
      "Epoch 326: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6074 - accuracy: 0.7845 - val_loss: 0.5414 - val_accuracy: 0.8111\n",
      "Epoch 327/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6028 - accuracy: 0.7879\n",
      "Epoch 327: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 10s 12ms/step - loss: 0.6029 - accuracy: 0.7879 - val_loss: 0.5263 - val_accuracy: 0.8112\n",
      "Epoch 328/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6068 - accuracy: 0.7859\n",
      "Epoch 328: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6068 - accuracy: 0.7858 - val_loss: 0.5395 - val_accuracy: 0.8198\n",
      "Epoch 329/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6076 - accuracy: 0.7854\n",
      "Epoch 329: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6077 - accuracy: 0.7854 - val_loss: 0.5561 - val_accuracy: 0.8078\n",
      "Epoch 330/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5965 - accuracy: 0.7899\n",
      "Epoch 330: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5964 - accuracy: 0.7899 - val_loss: 0.5114 - val_accuracy: 0.8210\n",
      "Epoch 331/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5791 - accuracy: 0.7934\n",
      "Epoch 331: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5791 - accuracy: 0.7934 - val_loss: 0.4994 - val_accuracy: 0.8204\n",
      "Epoch 332/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6026 - accuracy: 0.7873\n",
      "Epoch 332: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6027 - accuracy: 0.7872 - val_loss: 0.5107 - val_accuracy: 0.8214\n",
      "Epoch 333/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6048 - accuracy: 0.7859\n",
      "Epoch 333: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6049 - accuracy: 0.7859 - val_loss: 0.6304 - val_accuracy: 0.7791\n",
      "Epoch 334/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6050 - accuracy: 0.7868\n",
      "Epoch 334: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6049 - accuracy: 0.7868 - val_loss: 0.5531 - val_accuracy: 0.7990\n",
      "Epoch 335/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6205 - accuracy: 0.7812\n",
      "Epoch 335: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6204 - accuracy: 0.7812 - val_loss: 0.4920 - val_accuracy: 0.8247\n",
      "Epoch 336/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6639 - accuracy: 0.7684\n",
      "Epoch 336: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6637 - accuracy: 0.7684 - val_loss: 0.5105 - val_accuracy: 0.8184\n",
      "Epoch 337/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5841 - accuracy: 0.7923\n",
      "Epoch 337: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5843 - accuracy: 0.7922 - val_loss: 0.4950 - val_accuracy: 0.8271\n",
      "Epoch 338/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5911 - accuracy: 0.7926\n",
      "Epoch 338: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5911 - accuracy: 0.7926 - val_loss: 0.5079 - val_accuracy: 0.8199\n",
      "Epoch 339/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5757 - accuracy: 0.7964\n",
      "Epoch 339: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5757 - accuracy: 0.7964 - val_loss: 0.4891 - val_accuracy: 0.8283\n",
      "Epoch 340/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.5832 - accuracy: 0.7948\n",
      "Epoch 340: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 10s 12ms/step - loss: 0.5828 - accuracy: 0.7951 - val_loss: 0.5180 - val_accuracy: 0.8190\n",
      "Epoch 341/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6100 - accuracy: 0.7857\n",
      "Epoch 341: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6102 - accuracy: 0.7857 - val_loss: 0.5356 - val_accuracy: 0.8065\n",
      "Epoch 342/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.5906 - accuracy: 0.7903\n",
      "Epoch 342: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5906 - accuracy: 0.7903 - val_loss: 0.5525 - val_accuracy: 0.8083\n",
      "Epoch 343/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5799 - accuracy: 0.7933\n",
      "Epoch 343: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5798 - accuracy: 0.7934 - val_loss: 0.5107 - val_accuracy: 0.8183\n",
      "Epoch 344/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5945 - accuracy: 0.7902\n",
      "Epoch 344: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5944 - accuracy: 0.7901 - val_loss: 0.5014 - val_accuracy: 0.8217\n",
      "Epoch 345/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7284 - accuracy: 0.7487\n",
      "Epoch 345: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7283 - accuracy: 0.7487 - val_loss: 0.5762 - val_accuracy: 0.8015\n",
      "Epoch 346/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6604 - accuracy: 0.7731\n",
      "Epoch 346: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6605 - accuracy: 0.7730 - val_loss: 0.5905 - val_accuracy: 0.7883\n",
      "Epoch 347/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6306 - accuracy: 0.7785\n",
      "Epoch 347: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6302 - accuracy: 0.7786 - val_loss: 0.5304 - val_accuracy: 0.8156\n",
      "Epoch 348/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6168 - accuracy: 0.7847\n",
      "Epoch 348: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6168 - accuracy: 0.7847 - val_loss: 0.5505 - val_accuracy: 0.8124\n",
      "Epoch 349/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5848 - accuracy: 0.7930\n",
      "Epoch 349: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5848 - accuracy: 0.7930 - val_loss: 0.4943 - val_accuracy: 0.8224\n",
      "Epoch 350/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.5953 - accuracy: 0.7897\n",
      "Epoch 350: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.5954 - accuracy: 0.7896 - val_loss: 0.5705 - val_accuracy: 0.8007\n",
      "Epoch 351/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6634 - accuracy: 0.7715\n",
      "Epoch 351: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6633 - accuracy: 0.7716 - val_loss: 0.5326 - val_accuracy: 0.8143\n",
      "Epoch 352/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6310 - accuracy: 0.7795\n",
      "Epoch 352: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6308 - accuracy: 0.7795 - val_loss: 0.4896 - val_accuracy: 0.8212\n",
      "Epoch 353/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6995 - accuracy: 0.7627\n",
      "Epoch 353: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 10s 12ms/step - loss: 0.7002 - accuracy: 0.7624 - val_loss: 0.9411 - val_accuracy: 0.7029\n",
      "Epoch 354/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7906 - accuracy: 0.7323\n",
      "Epoch 354: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7907 - accuracy: 0.7323 - val_loss: 0.7699 - val_accuracy: 0.7459\n",
      "Epoch 355/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7164 - accuracy: 0.7554\n",
      "Epoch 355: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7162 - accuracy: 0.7554 - val_loss: 0.5491 - val_accuracy: 0.8059\n",
      "Epoch 356/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6229 - accuracy: 0.7839\n",
      "Epoch 356: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6232 - accuracy: 0.7838 - val_loss: 0.5809 - val_accuracy: 0.8030\n",
      "Epoch 357/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6581 - accuracy: 0.7713\n",
      "Epoch 357: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 10s 13ms/step - loss: 0.6583 - accuracy: 0.7711 - val_loss: 0.5586 - val_accuracy: 0.8023\n",
      "Epoch 358/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6651 - accuracy: 0.7683\n",
      "Epoch 358: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6649 - accuracy: 0.7684 - val_loss: 0.5565 - val_accuracy: 0.8056\n",
      "Epoch 359/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6765 - accuracy: 0.7617\n",
      "Epoch 359: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6767 - accuracy: 0.7616 - val_loss: 0.6963 - val_accuracy: 0.7506\n",
      "Epoch 360/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6292 - accuracy: 0.7776\n",
      "Epoch 360: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6292 - accuracy: 0.7776 - val_loss: 0.5359 - val_accuracy: 0.8098\n",
      "Epoch 361/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6483 - accuracy: 0.7746\n",
      "Epoch 361: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6484 - accuracy: 0.7747 - val_loss: 0.5914 - val_accuracy: 0.8004\n",
      "Epoch 362/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6511 - accuracy: 0.7720\n",
      "Epoch 362: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6511 - accuracy: 0.7720 - val_loss: 0.5536 - val_accuracy: 0.8016\n",
      "Epoch 363/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7193 - accuracy: 0.7510\n",
      "Epoch 363: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7194 - accuracy: 0.7510 - val_loss: 0.6484 - val_accuracy: 0.7928\n",
      "Epoch 364/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6467 - accuracy: 0.7768\n",
      "Epoch 364: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6465 - accuracy: 0.7768 - val_loss: 0.5378 - val_accuracy: 0.8079\n",
      "Epoch 365/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6153 - accuracy: 0.7842\n",
      "Epoch 365: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6152 - accuracy: 0.7843 - val_loss: 0.5686 - val_accuracy: 0.8050\n",
      "Epoch 366/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.6797 - accuracy: 0.7645\n",
      "Epoch 366: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 10s 13ms/step - loss: 0.6806 - accuracy: 0.7641 - val_loss: 0.7546 - val_accuracy: 0.7437\n",
      "Epoch 367/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6838 - accuracy: 0.7624\n",
      "Epoch 367: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6836 - accuracy: 0.7624 - val_loss: 0.5865 - val_accuracy: 0.7891\n",
      "Epoch 368/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6350 - accuracy: 0.7765\n",
      "Epoch 368: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6349 - accuracy: 0.7765 - val_loss: 0.5056 - val_accuracy: 0.8208\n",
      "Epoch 369/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6111 - accuracy: 0.7860\n",
      "Epoch 369: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6113 - accuracy: 0.7860 - val_loss: 0.5559 - val_accuracy: 0.8113\n",
      "Epoch 370/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6200 - accuracy: 0.7826\n",
      "Epoch 370: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6200 - accuracy: 0.7826 - val_loss: 0.5566 - val_accuracy: 0.8064\n",
      "Epoch 371/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6360 - accuracy: 0.7762\n",
      "Epoch 371: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6360 - accuracy: 0.7762 - val_loss: 0.6650 - val_accuracy: 0.7704\n",
      "Epoch 372/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6293 - accuracy: 0.7804\n",
      "Epoch 372: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 10s 13ms/step - loss: 0.6292 - accuracy: 0.7805 - val_loss: 0.5919 - val_accuracy: 0.7970\n",
      "Epoch 373/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6956 - accuracy: 0.7597\n",
      "Epoch 373: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 10s 12ms/step - loss: 0.6957 - accuracy: 0.7597 - val_loss: 0.6357 - val_accuracy: 0.7856\n",
      "Epoch 374/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6550 - accuracy: 0.7723\n",
      "Epoch 374: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6549 - accuracy: 0.7724 - val_loss: 0.6291 - val_accuracy: 0.7872\n",
      "Epoch 375/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6708 - accuracy: 0.7674\n",
      "Epoch 375: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6706 - accuracy: 0.7674 - val_loss: 0.5191 - val_accuracy: 0.8162\n",
      "Epoch 376/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6762 - accuracy: 0.7648\n",
      "Epoch 376: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6763 - accuracy: 0.7648 - val_loss: 0.7667 - val_accuracy: 0.7305\n",
      "Epoch 377/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7115 - accuracy: 0.7526\n",
      "Epoch 377: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7115 - accuracy: 0.7526 - val_loss: 0.6369 - val_accuracy: 0.7905\n",
      "Epoch 378/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6367 - accuracy: 0.7773\n",
      "Epoch 378: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6367 - accuracy: 0.7773 - val_loss: 0.5530 - val_accuracy: 0.8097\n",
      "Epoch 379/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6410 - accuracy: 0.7764\n",
      "Epoch 379: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 10s 13ms/step - loss: 0.6409 - accuracy: 0.7765 - val_loss: 0.5467 - val_accuracy: 0.8097\n",
      "Epoch 380/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6504 - accuracy: 0.7737\n",
      "Epoch 380: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6505 - accuracy: 0.7737 - val_loss: 0.5811 - val_accuracy: 0.8043\n",
      "Epoch 381/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6336 - accuracy: 0.7791\n",
      "Epoch 381: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6334 - accuracy: 0.7791 - val_loss: 0.5287 - val_accuracy: 0.8101\n",
      "Epoch 382/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8770 - accuracy: 0.6966\n",
      "Epoch 382: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8769 - accuracy: 0.6966 - val_loss: 0.6438 - val_accuracy: 0.7832\n",
      "Epoch 383/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6993 - accuracy: 0.7600\n",
      "Epoch 383: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6993 - accuracy: 0.7600 - val_loss: 0.6491 - val_accuracy: 0.7800\n",
      "Epoch 384/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6513 - accuracy: 0.7725\n",
      "Epoch 384: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6513 - accuracy: 0.7725 - val_loss: 0.5702 - val_accuracy: 0.8032\n",
      "Epoch 385/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6344 - accuracy: 0.7792\n",
      "Epoch 385: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 10s 12ms/step - loss: 0.6346 - accuracy: 0.7792 - val_loss: 0.5333 - val_accuracy: 0.8134\n",
      "Epoch 386/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6689 - accuracy: 0.7717\n",
      "Epoch 386: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6692 - accuracy: 0.7715 - val_loss: 0.6588 - val_accuracy: 0.7772\n",
      "Epoch 387/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.6608 - accuracy: 0.7715\n",
      "Epoch 387: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6608 - accuracy: 0.7714 - val_loss: 0.5444 - val_accuracy: 0.8094\n",
      "Epoch 388/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.9102 - accuracy: 0.6854\n",
      "Epoch 388: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9102 - accuracy: 0.6854 - val_loss: 0.7321 - val_accuracy: 0.7392\n",
      "Epoch 389/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8975 - accuracy: 0.6743\n",
      "Epoch 389: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8972 - accuracy: 0.6744 - val_loss: 0.7330 - val_accuracy: 0.7457\n",
      "Epoch 390/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7196 - accuracy: 0.7444\n",
      "Epoch 390: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7196 - accuracy: 0.7444 - val_loss: 0.5584 - val_accuracy: 0.8046\n",
      "Epoch 391/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.6890 - accuracy: 0.7568\n",
      "Epoch 391: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6891 - accuracy: 0.7567 - val_loss: 0.6185 - val_accuracy: 0.7794\n",
      "Epoch 392/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7276 - accuracy: 0.7485\n",
      "Epoch 392: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7275 - accuracy: 0.7485 - val_loss: 0.5995 - val_accuracy: 0.7945\n",
      "Epoch 393/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6845 - accuracy: 0.7605\n",
      "Epoch 393: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6845 - accuracy: 0.7605 - val_loss: 0.5717 - val_accuracy: 0.8038\n",
      "Epoch 394/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7431 - accuracy: 0.7402\n",
      "Epoch 394: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7434 - accuracy: 0.7401 - val_loss: 0.9983 - val_accuracy: 0.6400\n",
      "Epoch 395/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.9585 - accuracy: 0.6588\n",
      "Epoch 395: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9584 - accuracy: 0.6588 - val_loss: 0.9251 - val_accuracy: 0.6796\n",
      "Epoch 396/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8723 - accuracy: 0.6896\n",
      "Epoch 396: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8723 - accuracy: 0.6897 - val_loss: 0.7013 - val_accuracy: 0.7515\n",
      "Epoch 397/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7223 - accuracy: 0.7443\n",
      "Epoch 397: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7225 - accuracy: 0.7441 - val_loss: 0.6815 - val_accuracy: 0.7669\n",
      "Epoch 398/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.6552 - accuracy: 0.7681\n",
      "Epoch 398: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6552 - accuracy: 0.7681 - val_loss: 0.5392 - val_accuracy: 0.8115\n",
      "Epoch 399/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.6647 - accuracy: 0.7669\n",
      "Epoch 399: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.6646 - accuracy: 0.7669 - val_loss: 0.6068 - val_accuracy: 0.7991\n",
      "Epoch 400/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7880 - accuracy: 0.7237\n",
      "Epoch 400: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7886 - accuracy: 0.7237 - val_loss: 1.1650 - val_accuracy: 0.5820\n",
      "Epoch 401/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0207 - accuracy: 0.6324\n",
      "Epoch 401: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0208 - accuracy: 0.6324 - val_loss: 0.8608 - val_accuracy: 0.6952\n",
      "Epoch 402/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7907 - accuracy: 0.7236\n",
      "Epoch 402: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7906 - accuracy: 0.7236 - val_loss: 0.6977 - val_accuracy: 0.7522\n",
      "Epoch 403/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.7734 - accuracy: 0.7295\n",
      "Epoch 403: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7740 - accuracy: 0.7293 - val_loss: 0.8958 - val_accuracy: 0.7089\n",
      "Epoch 404/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.8121 - accuracy: 0.7172\n",
      "Epoch 404: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8126 - accuracy: 0.7171 - val_loss: 0.6995 - val_accuracy: 0.7617\n",
      "Epoch 405/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.7874 - accuracy: 0.7251\n",
      "Epoch 405: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7871 - accuracy: 0.7253 - val_loss: 0.6498 - val_accuracy: 0.7783\n",
      "Epoch 406/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7342 - accuracy: 0.7424\n",
      "Epoch 406: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7341 - accuracy: 0.7424 - val_loss: 0.6605 - val_accuracy: 0.7697\n",
      "Epoch 407/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8474 - accuracy: 0.7052\n",
      "Epoch 407: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8474 - accuracy: 0.7053 - val_loss: 0.9990 - val_accuracy: 0.6488\n",
      "Epoch 408/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8308 - accuracy: 0.7102\n",
      "Epoch 408: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8309 - accuracy: 0.7101 - val_loss: 0.7159 - val_accuracy: 0.7575\n",
      "Epoch 409/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0735 - accuracy: 0.6829\n",
      "Epoch 409: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0736 - accuracy: 0.6828 - val_loss: 1.1146 - val_accuracy: 0.5976\n",
      "Epoch 410/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.1085 - accuracy: 0.5915\n",
      "Epoch 410: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.1084 - accuracy: 0.5916 - val_loss: 1.0858 - val_accuracy: 0.5971\n",
      "Epoch 411/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 1.0126 - accuracy: 0.6289\n",
      "Epoch 411: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 10s 12ms/step - loss: 1.0122 - accuracy: 0.6291 - val_loss: 0.8099 - val_accuracy: 0.7262\n",
      "Epoch 412/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8724 - accuracy: 0.6992\n",
      "Epoch 412: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8725 - accuracy: 0.6991 - val_loss: 0.9677 - val_accuracy: 0.6690\n",
      "Epoch 413/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0332 - accuracy: 0.6345\n",
      "Epoch 413: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0334 - accuracy: 0.6345 - val_loss: 1.1144 - val_accuracy: 0.6062\n",
      "Epoch 414/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0858 - accuracy: 0.6015\n",
      "Epoch 414: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0859 - accuracy: 0.6015 - val_loss: 1.0523 - val_accuracy: 0.6188\n",
      "Epoch 415/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0794 - accuracy: 0.6104\n",
      "Epoch 415: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0795 - accuracy: 0.6104 - val_loss: 1.0266 - val_accuracy: 0.6434\n",
      "Epoch 416/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 1.0165 - accuracy: 0.6375\n",
      "Epoch 416: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0165 - accuracy: 0.6374 - val_loss: 0.9706 - val_accuracy: 0.6607\n",
      "Epoch 417/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.9816 - accuracy: 0.6459\n",
      "Epoch 417: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9816 - accuracy: 0.6459 - val_loss: 0.9260 - val_accuracy: 0.6764\n",
      "Epoch 418/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.1224 - accuracy: 0.5957\n",
      "Epoch 418: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.1226 - accuracy: 0.5957 - val_loss: 1.1448 - val_accuracy: 0.5920\n",
      "Epoch 419/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.1027 - accuracy: 0.5896\n",
      "Epoch 419: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.1026 - accuracy: 0.5896 - val_loss: 1.1017 - val_accuracy: 0.6007\n",
      "Epoch 420/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0924 - accuracy: 0.5920\n",
      "Epoch 420: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0924 - accuracy: 0.5920 - val_loss: 1.1541 - val_accuracy: 0.5818\n",
      "Epoch 421/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 1.0936 - accuracy: 0.5915\n",
      "Epoch 421: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0940 - accuracy: 0.5913 - val_loss: 1.1241 - val_accuracy: 0.5874\n",
      "Epoch 422/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.5920\n",
      "Epoch 422: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0987 - accuracy: 0.5920 - val_loss: 1.1526 - val_accuracy: 0.5937\n",
      "Epoch 423/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 1.0992 - accuracy: 0.5918\n",
      "Epoch 423: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0992 - accuracy: 0.5918 - val_loss: 1.1252 - val_accuracy: 0.5931\n",
      "Epoch 424/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 1.0860 - accuracy: 0.5944\n",
      "Epoch 424: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0858 - accuracy: 0.5945 - val_loss: 1.1104 - val_accuracy: 0.5898\n",
      "Epoch 425/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0714 - accuracy: 0.5934\n",
      "Epoch 425: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0713 - accuracy: 0.5935 - val_loss: 1.0680 - val_accuracy: 0.6006\n",
      "Epoch 426/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0578 - accuracy: 0.5983\n",
      "Epoch 426: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0580 - accuracy: 0.5982 - val_loss: 1.0684 - val_accuracy: 0.5966\n",
      "Epoch 427/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0656 - accuracy: 0.5978\n",
      "Epoch 427: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0657 - accuracy: 0.5978 - val_loss: 1.0767 - val_accuracy: 0.6024\n",
      "Epoch 428/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0881 - accuracy: 0.5962\n",
      "Epoch 428: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0879 - accuracy: 0.5962 - val_loss: 1.1065 - val_accuracy: 0.5971\n",
      "Epoch 429/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0728 - accuracy: 0.5973\n",
      "Epoch 429: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0727 - accuracy: 0.5973 - val_loss: 1.1354 - val_accuracy: 0.5922\n",
      "Epoch 430/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 1.0784 - accuracy: 0.5957\n",
      "Epoch 430: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0781 - accuracy: 0.5959 - val_loss: 1.0722 - val_accuracy: 0.6059\n",
      "Epoch 431/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0556 - accuracy: 0.6022\n",
      "Epoch 431: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0555 - accuracy: 0.6022 - val_loss: 1.0673 - val_accuracy: 0.6061\n",
      "Epoch 432/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0256 - accuracy: 0.6094\n",
      "Epoch 432: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0256 - accuracy: 0.6093 - val_loss: 0.9815 - val_accuracy: 0.6329\n",
      "Epoch 433/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0189 - accuracy: 0.6160\n",
      "Epoch 433: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0188 - accuracy: 0.6161 - val_loss: 0.9956 - val_accuracy: 0.6311\n",
      "Epoch 434/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0216 - accuracy: 0.6129\n",
      "Epoch 434: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0218 - accuracy: 0.6128 - val_loss: 1.0465 - val_accuracy: 0.5988\n",
      "Epoch 435/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 1.0234 - accuracy: 0.6070\n",
      "Epoch 435: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0234 - accuracy: 0.6070 - val_loss: 1.0184 - val_accuracy: 0.6115\n",
      "Epoch 436/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.9931 - accuracy: 0.6216\n",
      "Epoch 436: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9941 - accuracy: 0.6215 - val_loss: 1.1478 - val_accuracy: 0.6038\n",
      "Epoch 437/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0685 - accuracy: 0.5997\n",
      "Epoch 437: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0689 - accuracy: 0.5996 - val_loss: 1.0340 - val_accuracy: 0.6160\n",
      "Epoch 438/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 1.0069 - accuracy: 0.6121\n",
      "Epoch 438: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0069 - accuracy: 0.6121 - val_loss: 0.9857 - val_accuracy: 0.6234\n",
      "Epoch 439/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0004 - accuracy: 0.6150\n",
      "Epoch 439: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 10s 12ms/step - loss: 1.0005 - accuracy: 0.6150 - val_loss: 1.0307 - val_accuracy: 0.6057\n",
      "Epoch 440/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0101 - accuracy: 0.6091\n",
      "Epoch 440: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0101 - accuracy: 0.6091 - val_loss: 0.9603 - val_accuracy: 0.6319\n",
      "Epoch 441/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 1.0191 - accuracy: 0.6068\n",
      "Epoch 441: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0191 - accuracy: 0.6068 - val_loss: 0.9884 - val_accuracy: 0.6166\n",
      "Epoch 442/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.9823 - accuracy: 0.6174\n",
      "Epoch 442: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9822 - accuracy: 0.6175 - val_loss: 0.9710 - val_accuracy: 0.6261\n",
      "Epoch 443/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.9801 - accuracy: 0.6204\n",
      "Epoch 443: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 10s 13ms/step - loss: 0.9804 - accuracy: 0.6204 - val_loss: 0.9463 - val_accuracy: 0.6426\n",
      "Epoch 444/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.9955 - accuracy: 0.6221\n",
      "Epoch 444: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9955 - accuracy: 0.6221 - val_loss: 0.9566 - val_accuracy: 0.6356\n",
      "Epoch 445/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0517 - accuracy: 0.6040\n",
      "Epoch 445: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0515 - accuracy: 0.6040 - val_loss: 0.9681 - val_accuracy: 0.6337\n",
      "Epoch 446/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0245 - accuracy: 0.6111\n",
      "Epoch 446: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0244 - accuracy: 0.6111 - val_loss: 1.0815 - val_accuracy: 0.6022\n",
      "Epoch 447/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0937 - accuracy: 0.5927\n",
      "Epoch 447: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0935 - accuracy: 0.5927 - val_loss: 1.0588 - val_accuracy: 0.5936\n",
      "Epoch 448/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 1.0606 - accuracy: 0.6006\n",
      "Epoch 448: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0606 - accuracy: 0.6006 - val_loss: 1.0717 - val_accuracy: 0.6031\n",
      "Epoch 449/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0577 - accuracy: 0.5969\n",
      "Epoch 449: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0576 - accuracy: 0.5971 - val_loss: 1.0034 - val_accuracy: 0.6224\n",
      "Epoch 450/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 1.0115 - accuracy: 0.6112\n",
      "Epoch 450: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0115 - accuracy: 0.6112 - val_loss: 0.9802 - val_accuracy: 0.6071\n",
      "Epoch 451/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0581 - accuracy: 0.6010\n",
      "Epoch 451: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0581 - accuracy: 0.6011 - val_loss: 1.0445 - val_accuracy: 0.6072\n",
      "Epoch 452/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 1.0661 - accuracy: 0.5961\n",
      "Epoch 452: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0662 - accuracy: 0.5962 - val_loss: 1.0748 - val_accuracy: 0.5990\n",
      "Epoch 453/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0480 - accuracy: 0.5939\n",
      "Epoch 453: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0479 - accuracy: 0.5939 - val_loss: 1.0180 - val_accuracy: 0.5996\n",
      "Epoch 454/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0094 - accuracy: 0.5990\n",
      "Epoch 454: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0094 - accuracy: 0.5990 - val_loss: 0.9746 - val_accuracy: 0.6168\n",
      "Epoch 455/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0185 - accuracy: 0.6015\n",
      "Epoch 455: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0183 - accuracy: 0.6016 - val_loss: 0.9967 - val_accuracy: 0.6031\n",
      "Epoch 456/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 1.0024 - accuracy: 0.6092\n",
      "Epoch 456: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0023 - accuracy: 0.6093 - val_loss: 0.9680 - val_accuracy: 0.6238\n",
      "Epoch 457/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0050 - accuracy: 0.6128\n",
      "Epoch 457: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0053 - accuracy: 0.6126 - val_loss: 1.0049 - val_accuracy: 0.6139\n",
      "Epoch 458/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0312 - accuracy: 0.6039\n",
      "Epoch 458: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0312 - accuracy: 0.6038 - val_loss: 1.0409 - val_accuracy: 0.5952\n",
      "Epoch 459/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.9944 - accuracy: 0.6066\n",
      "Epoch 459: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9944 - accuracy: 0.6066 - val_loss: 0.9951 - val_accuracy: 0.5940\n",
      "Epoch 460/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0047 - accuracy: 0.6092\n",
      "Epoch 460: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0049 - accuracy: 0.6091 - val_loss: 1.0035 - val_accuracy: 0.6045\n",
      "Epoch 461/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.9782 - accuracy: 0.6115\n",
      "Epoch 461: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9782 - accuracy: 0.6115 - val_loss: 0.9538 - val_accuracy: 0.6217\n",
      "Epoch 462/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 1.0128 - accuracy: 0.6049\n",
      "Epoch 462: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0126 - accuracy: 0.6050 - val_loss: 1.0184 - val_accuracy: 0.6044\n",
      "Epoch 463/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0297 - accuracy: 0.6011\n",
      "Epoch 463: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0298 - accuracy: 0.6011 - val_loss: 1.0118 - val_accuracy: 0.5996\n",
      "Epoch 464/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.9873 - accuracy: 0.6070\n",
      "Epoch 464: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9876 - accuracy: 0.6070 - val_loss: 0.9678 - val_accuracy: 0.6233\n",
      "Epoch 465/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 1.0062 - accuracy: 0.6075\n",
      "Epoch 465: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0066 - accuracy: 0.6074 - val_loss: 1.1245 - val_accuracy: 0.5898\n",
      "Epoch 466/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 1.0697 - accuracy: 0.5969\n",
      "Epoch 466: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0698 - accuracy: 0.5969 - val_loss: 1.0460 - val_accuracy: 0.6086\n",
      "Epoch 467/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 1.0369 - accuracy: 0.5987\n",
      "Epoch 467: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0371 - accuracy: 0.5988 - val_loss: 1.0531 - val_accuracy: 0.5991\n",
      "Epoch 468/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 1.0318 - accuracy: 0.5989\n",
      "Epoch 468: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0318 - accuracy: 0.5989 - val_loss: 1.0104 - val_accuracy: 0.6016\n",
      "Epoch 469/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 1.0236 - accuracy: 0.5973\n",
      "Epoch 469: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0236 - accuracy: 0.5973 - val_loss: 1.0354 - val_accuracy: 0.5926\n",
      "Epoch 470/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 1.0973 - accuracy: 0.5923\n",
      "Epoch 470: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0972 - accuracy: 0.5925 - val_loss: 1.0726 - val_accuracy: 0.5978\n",
      "Epoch 471/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 1.0644 - accuracy: 0.5960\n",
      "Epoch 471: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0644 - accuracy: 0.5960 - val_loss: 1.0516 - val_accuracy: 0.5996\n",
      "Epoch 472/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 1.0136 - accuracy: 0.6012\n",
      "Epoch 472: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 10s 12ms/step - loss: 1.0139 - accuracy: 0.6011 - val_loss: 0.9824 - val_accuracy: 0.6052\n",
      "Epoch 473/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0228 - accuracy: 0.5998\n",
      "Epoch 473: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0227 - accuracy: 0.5998 - val_loss: 0.9854 - val_accuracy: 0.6119\n",
      "Epoch 474/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0343 - accuracy: 0.5985\n",
      "Epoch 474: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0341 - accuracy: 0.5986 - val_loss: 1.0498 - val_accuracy: 0.5981\n",
      "Epoch 475/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 1.0119 - accuracy: 0.6012\n",
      "Epoch 475: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0121 - accuracy: 0.6010 - val_loss: 0.9864 - val_accuracy: 0.6006\n",
      "Epoch 476/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0168 - accuracy: 0.6023\n",
      "Epoch 476: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0168 - accuracy: 0.6023 - val_loss: 1.0056 - val_accuracy: 0.6048\n",
      "Epoch 477/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0008 - accuracy: 0.6052\n",
      "Epoch 477: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0007 - accuracy: 0.6052 - val_loss: 0.9557 - val_accuracy: 0.6144\n",
      "Epoch 478/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0021 - accuracy: 0.6028\n",
      "Epoch 478: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0022 - accuracy: 0.6027 - val_loss: 1.0192 - val_accuracy: 0.5984\n",
      "Epoch 479/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0516 - accuracy: 0.5940\n",
      "Epoch 479: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0515 - accuracy: 0.5941 - val_loss: 1.0234 - val_accuracy: 0.5927\n",
      "Epoch 480/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0180 - accuracy: 0.5975\n",
      "Epoch 480: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0178 - accuracy: 0.5976 - val_loss: 0.9890 - val_accuracy: 0.6014\n",
      "Epoch 481/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.9773 - accuracy: 0.6066\n",
      "Epoch 481: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9778 - accuracy: 0.6064 - val_loss: 0.9672 - val_accuracy: 0.6037\n",
      "Epoch 482/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.9642 - accuracy: 0.6094\n",
      "Epoch 482: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9641 - accuracy: 0.6095 - val_loss: 0.9582 - val_accuracy: 0.6094\n",
      "Epoch 483/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.9984 - accuracy: 0.6050\n",
      "Epoch 483: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9984 - accuracy: 0.6050 - val_loss: 0.9585 - val_accuracy: 0.6185\n",
      "Epoch 484/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0607 - accuracy: 0.5955\n",
      "Epoch 484: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0605 - accuracy: 0.5955 - val_loss: 1.1554 - val_accuracy: 0.5831\n",
      "Epoch 485/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 1.1325 - accuracy: 0.5871\n",
      "Epoch 485: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.1320 - accuracy: 0.5872 - val_loss: 1.1045 - val_accuracy: 0.5904\n",
      "Epoch 486/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.1082 - accuracy: 0.5860\n",
      "Epoch 486: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.1079 - accuracy: 0.5861 - val_loss: 1.0408 - val_accuracy: 0.6049\n",
      "Epoch 487/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 1.0329 - accuracy: 0.5975\n",
      "Epoch 487: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0322 - accuracy: 0.5978 - val_loss: 1.0407 - val_accuracy: 0.5944\n",
      "Epoch 488/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0517 - accuracy: 0.5963\n",
      "Epoch 488: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0517 - accuracy: 0.5963 - val_loss: 1.0228 - val_accuracy: 0.5944\n",
      "Epoch 489/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0369 - accuracy: 0.5966\n",
      "Epoch 489: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0368 - accuracy: 0.5967 - val_loss: 1.0340 - val_accuracy: 0.5897\n",
      "Epoch 490/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 1.0449 - accuracy: 0.5944\n",
      "Epoch 490: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0444 - accuracy: 0.5945 - val_loss: 1.0730 - val_accuracy: 0.5912\n",
      "Epoch 491/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 1.1211 - accuracy: 0.5836\n",
      "Epoch 491: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.1211 - accuracy: 0.5836 - val_loss: 1.1054 - val_accuracy: 0.5937\n",
      "Epoch 492/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.1110 - accuracy: 0.5869\n",
      "Epoch 492: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.1114 - accuracy: 0.5867 - val_loss: 1.1011 - val_accuracy: 0.5881\n",
      "Epoch 493/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 1.0723 - accuracy: 0.5878\n",
      "Epoch 493: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0725 - accuracy: 0.5877 - val_loss: 1.0964 - val_accuracy: 0.5884\n",
      "Epoch 494/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0750 - accuracy: 0.5888\n",
      "Epoch 494: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0750 - accuracy: 0.5888 - val_loss: 1.0824 - val_accuracy: 0.5895\n",
      "Epoch 495/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 1.0535 - accuracy: 0.5930\n",
      "Epoch 495: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0535 - accuracy: 0.5930 - val_loss: 1.0040 - val_accuracy: 0.6054\n",
      "Epoch 496/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0235 - accuracy: 0.5954\n",
      "Epoch 496: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0236 - accuracy: 0.5954 - val_loss: 0.9927 - val_accuracy: 0.6049\n",
      "Epoch 497/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0180 - accuracy: 0.5944\n",
      "Epoch 497: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0178 - accuracy: 0.5943 - val_loss: 0.9905 - val_accuracy: 0.6065\n",
      "Epoch 498/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0692 - accuracy: 0.5923\n",
      "Epoch 498: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0690 - accuracy: 0.5924 - val_loss: 1.0349 - val_accuracy: 0.5964\n",
      "Epoch 499/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0842 - accuracy: 0.5894\n",
      "Epoch 499: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0840 - accuracy: 0.5894 - val_loss: 1.0933 - val_accuracy: 0.5891\n",
      "Epoch 500/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 1.0962 - accuracy: 0.5897\n",
      "Epoch 500: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0962 - accuracy: 0.5897 - val_loss: 1.0716 - val_accuracy: 0.5903\n",
      "Epoch 501/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 1.0799 - accuracy: 0.5896\n",
      "Epoch 501: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0802 - accuracy: 0.5896 - val_loss: 1.0966 - val_accuracy: 0.5921\n",
      "Epoch 502/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 1.1572 - accuracy: 0.5838\n",
      "Epoch 502: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.1570 - accuracy: 0.5840 - val_loss: 1.1224 - val_accuracy: 0.5921\n",
      "Epoch 503/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.1406 - accuracy: 0.5851\n",
      "Epoch 503: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.1405 - accuracy: 0.5851 - val_loss: 1.1101 - val_accuracy: 0.5857\n",
      "Epoch 504/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.1098 - accuracy: 0.5855\n",
      "Epoch 504: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.1101 - accuracy: 0.5853 - val_loss: 1.0829 - val_accuracy: 0.5915\n",
      "Epoch 505/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0956 - accuracy: 0.5891\n",
      "Epoch 505: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0955 - accuracy: 0.5891 - val_loss: 1.0889 - val_accuracy: 0.5878\n",
      "Epoch 506/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0822 - accuracy: 0.5895\n",
      "Epoch 506: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0823 - accuracy: 0.5894 - val_loss: 1.0458 - val_accuracy: 0.5948\n",
      "Epoch 507/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0794 - accuracy: 0.5888\n",
      "Epoch 507: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0794 - accuracy: 0.5888 - val_loss: 1.0995 - val_accuracy: 0.5920\n",
      "Epoch 508/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0869 - accuracy: 0.5896\n",
      "Epoch 508: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0869 - accuracy: 0.5895 - val_loss: 1.0676 - val_accuracy: 0.5906\n",
      "Epoch 509/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 1.0643 - accuracy: 0.5909\n",
      "Epoch 509: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0643 - accuracy: 0.5909 - val_loss: 1.0567 - val_accuracy: 0.5888\n",
      "Epoch 510/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 1.0422 - accuracy: 0.5917\n",
      "Epoch 510: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0422 - accuracy: 0.5917 - val_loss: 1.0488 - val_accuracy: 0.5878\n",
      "Epoch 511/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0391 - accuracy: 0.5914\n",
      "Epoch 511: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0391 - accuracy: 0.5913 - val_loss: 1.0498 - val_accuracy: 0.5905\n",
      "Epoch 512/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 1.0395 - accuracy: 0.5942\n",
      "Epoch 512: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0399 - accuracy: 0.5939 - val_loss: 1.0487 - val_accuracy: 0.5960\n",
      "Epoch 513/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 1.0436 - accuracy: 0.5940\n",
      "Epoch 513: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0436 - accuracy: 0.5940 - val_loss: 1.0168 - val_accuracy: 0.5986\n",
      "Epoch 514/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 1.0506 - accuracy: 0.5920\n",
      "Epoch 514: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0506 - accuracy: 0.5920 - val_loss: 1.0325 - val_accuracy: 0.5958\n",
      "Epoch 515/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 1.0404 - accuracy: 0.5936\n",
      "Epoch 515: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0404 - accuracy: 0.5936 - val_loss: 1.0935 - val_accuracy: 0.5848\n",
      "Epoch 516/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0440 - accuracy: 0.5927\n",
      "Epoch 516: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0439 - accuracy: 0.5928 - val_loss: 1.0128 - val_accuracy: 0.5922\n",
      "Epoch 517/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 1.0572 - accuracy: 0.5930\n",
      "Epoch 517: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0569 - accuracy: 0.5930 - val_loss: 1.0380 - val_accuracy: 0.5953\n",
      "Epoch 518/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 1.0186 - accuracy: 0.5974\n",
      "Epoch 518: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0186 - accuracy: 0.5974 - val_loss: 1.0239 - val_accuracy: 0.5926\n",
      "Epoch 519/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 1.0267 - accuracy: 0.5953\n",
      "Epoch 519: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0266 - accuracy: 0.5952 - val_loss: 1.0260 - val_accuracy: 0.5953\n",
      "Epoch 520/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0073 - accuracy: 0.5965\n",
      "Epoch 520: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0075 - accuracy: 0.5965 - val_loss: 0.9782 - val_accuracy: 0.5995\n",
      "Epoch 521/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.9900 - accuracy: 0.6003\n",
      "Epoch 521: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9900 - accuracy: 0.6003 - val_loss: 1.0038 - val_accuracy: 0.5975\n",
      "Epoch 522/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 1.0046 - accuracy: 0.5983\n",
      "Epoch 522: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0046 - accuracy: 0.5983 - val_loss: 1.0310 - val_accuracy: 0.5947\n",
      "Epoch 523/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.9949 - accuracy: 0.6021\n",
      "Epoch 523: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9949 - accuracy: 0.6021 - val_loss: 1.0131 - val_accuracy: 0.5966\n",
      "Epoch 524/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0084 - accuracy: 0.5988\n",
      "Epoch 524: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0085 - accuracy: 0.5987 - val_loss: 1.0180 - val_accuracy: 0.5980\n",
      "Epoch 525/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.1118 - accuracy: 0.5859\n",
      "Epoch 525: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.1119 - accuracy: 0.5858 - val_loss: 1.0557 - val_accuracy: 0.5953\n",
      "Epoch 526/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0586 - accuracy: 0.5896\n",
      "Epoch 526: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0586 - accuracy: 0.5895 - val_loss: 1.1809 - val_accuracy: 0.5828\n",
      "Epoch 527/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 1.1182 - accuracy: 0.5859\n",
      "Epoch 527: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.1189 - accuracy: 0.5857 - val_loss: 1.1087 - val_accuracy: 0.5953\n",
      "Epoch 528/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 1.0943 - accuracy: 0.5835\n",
      "Epoch 528: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0939 - accuracy: 0.5835 - val_loss: 1.0842 - val_accuracy: 0.5864\n",
      "Epoch 529/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 1.0839 - accuracy: 0.5874\n",
      "Epoch 529: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0839 - accuracy: 0.5874 - val_loss: 1.1114 - val_accuracy: 0.5855\n",
      "Epoch 530/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0907 - accuracy: 0.5850\n",
      "Epoch 530: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0906 - accuracy: 0.5849 - val_loss: 1.0924 - val_accuracy: 0.5821\n",
      "Epoch 531/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0711 - accuracy: 0.5854\n",
      "Epoch 531: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0712 - accuracy: 0.5855 - val_loss: 1.0336 - val_accuracy: 0.6016\n",
      "Epoch 532/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 1.0623 - accuracy: 0.5887\n",
      "Epoch 532: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0623 - accuracy: 0.5887 - val_loss: 1.0587 - val_accuracy: 0.5894\n",
      "Epoch 533/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 1.0829 - accuracy: 0.5856\n",
      "Epoch 533: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0831 - accuracy: 0.5856 - val_loss: 1.0617 - val_accuracy: 0.5842\n",
      "Epoch 534/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 1.0795 - accuracy: 0.5886\n",
      "Epoch 534: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0795 - accuracy: 0.5886 - val_loss: 1.0676 - val_accuracy: 0.5813\n",
      "Epoch 535/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0760 - accuracy: 0.5895\n",
      "Epoch 535: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0759 - accuracy: 0.5896 - val_loss: 1.0492 - val_accuracy: 0.5905\n",
      "Epoch 536/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 1.0516 - accuracy: 0.5908\n",
      "Epoch 536: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0516 - accuracy: 0.5908 - val_loss: 1.0394 - val_accuracy: 0.5964\n",
      "Epoch 537/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 1.0437 - accuracy: 0.5901\n",
      "Epoch 537: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0437 - accuracy: 0.5901 - val_loss: 1.0271 - val_accuracy: 0.5926\n",
      "Epoch 538/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0402 - accuracy: 0.5912\n",
      "Epoch 538: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0401 - accuracy: 0.5913 - val_loss: 1.0355 - val_accuracy: 0.5815\n",
      "Epoch 539/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0438 - accuracy: 0.5913\n",
      "Epoch 539: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0438 - accuracy: 0.5913 - val_loss: 1.0456 - val_accuracy: 0.5924\n",
      "Epoch 540/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 1.0362 - accuracy: 0.5889\n",
      "Epoch 540: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0360 - accuracy: 0.5889 - val_loss: 1.0342 - val_accuracy: 0.5994\n",
      "Epoch 541/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 1.0166 - accuracy: 0.5922\n",
      "Epoch 541: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0168 - accuracy: 0.5920 - val_loss: 0.9934 - val_accuracy: 0.5925\n",
      "Epoch 542/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0708 - accuracy: 0.5891\n",
      "Epoch 542: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0708 - accuracy: 0.5890 - val_loss: 1.0497 - val_accuracy: 0.5901\n",
      "Epoch 543/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 1.0355 - accuracy: 0.5925\n",
      "Epoch 543: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0357 - accuracy: 0.5925 - val_loss: 1.0139 - val_accuracy: 0.5985\n",
      "Epoch 544/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0079 - accuracy: 0.5971\n",
      "Epoch 544: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0080 - accuracy: 0.5970 - val_loss: 0.9922 - val_accuracy: 0.5959\n",
      "Epoch 545/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0079 - accuracy: 0.5971\n",
      "Epoch 545: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0079 - accuracy: 0.5970 - val_loss: 1.0125 - val_accuracy: 0.5982\n",
      "Epoch 546/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 1.0082 - accuracy: 0.5946\n",
      "Epoch 546: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0080 - accuracy: 0.5947 - val_loss: 0.9938 - val_accuracy: 0.5981\n",
      "Epoch 547/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0440 - accuracy: 0.5933\n",
      "Epoch 547: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0439 - accuracy: 0.5933 - val_loss: 1.0549 - val_accuracy: 0.5885\n",
      "Epoch 548/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 1.0135 - accuracy: 0.5931\n",
      "Epoch 548: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0135 - accuracy: 0.5931 - val_loss: 0.9801 - val_accuracy: 0.5998\n",
      "Epoch 549/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.9830 - accuracy: 0.5997\n",
      "Epoch 549: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9831 - accuracy: 0.5998 - val_loss: 0.9737 - val_accuracy: 0.5976\n",
      "Epoch 550/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0145 - accuracy: 0.5966\n",
      "Epoch 550: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0144 - accuracy: 0.5966 - val_loss: 1.0090 - val_accuracy: 0.5873\n",
      "Epoch 551/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0079 - accuracy: 0.5985\n",
      "Epoch 551: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0078 - accuracy: 0.5985 - val_loss: 0.9924 - val_accuracy: 0.5990\n",
      "Epoch 552/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.9809 - accuracy: 0.6002\n",
      "Epoch 552: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9809 - accuracy: 0.6002 - val_loss: 0.9685 - val_accuracy: 0.6094\n",
      "Epoch 553/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.9824 - accuracy: 0.6003\n",
      "Epoch 553: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9824 - accuracy: 0.6003 - val_loss: 0.9863 - val_accuracy: 0.6062\n",
      "Epoch 554/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.9719 - accuracy: 0.6022\n",
      "Epoch 554: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9714 - accuracy: 0.6024 - val_loss: 0.9660 - val_accuracy: 0.6035\n",
      "Epoch 555/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.9649 - accuracy: 0.6048\n",
      "Epoch 555: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9649 - accuracy: 0.6048 - val_loss: 0.9589 - val_accuracy: 0.6051\n",
      "Epoch 556/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.9583 - accuracy: 0.6065\n",
      "Epoch 556: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9586 - accuracy: 0.6063 - val_loss: 0.9462 - val_accuracy: 0.6168\n",
      "Epoch 557/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.9522 - accuracy: 0.6077\n",
      "Epoch 557: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9524 - accuracy: 0.6077 - val_loss: 0.9459 - val_accuracy: 0.6046\n",
      "Epoch 558/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.9502 - accuracy: 0.6079\n",
      "Epoch 558: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9502 - accuracy: 0.6079 - val_loss: 0.9709 - val_accuracy: 0.5996\n",
      "Epoch 559/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.9906 - accuracy: 0.6036\n",
      "Epoch 559: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9911 - accuracy: 0.6035 - val_loss: 1.1473 - val_accuracy: 0.5872\n",
      "Epoch 560/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 1.0887 - accuracy: 0.5934\n",
      "Epoch 560: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0891 - accuracy: 0.5932 - val_loss: 1.0663 - val_accuracy: 0.5968\n",
      "Epoch 561/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0367 - accuracy: 0.5937\n",
      "Epoch 561: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0364 - accuracy: 0.5938 - val_loss: 1.0010 - val_accuracy: 0.6024\n",
      "Epoch 562/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 1.0064 - accuracy: 0.5977\n",
      "Epoch 562: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 1.0063 - accuracy: 0.5978 - val_loss: 0.9735 - val_accuracy: 0.6077\n",
      "Epoch 563/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.9732 - accuracy: 0.6024\n",
      "Epoch 563: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9732 - accuracy: 0.6024 - val_loss: 0.9684 - val_accuracy: 0.6033\n",
      "Epoch 564/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.9625 - accuracy: 0.6029\n",
      "Epoch 564: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9625 - accuracy: 0.6028 - val_loss: 0.9518 - val_accuracy: 0.6088\n",
      "Epoch 565/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.9590 - accuracy: 0.6067\n",
      "Epoch 565: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9590 - accuracy: 0.6067 - val_loss: 0.9311 - val_accuracy: 0.6074\n",
      "Epoch 566/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.9604 - accuracy: 0.6073\n",
      "Epoch 566: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9608 - accuracy: 0.6071 - val_loss: 0.9978 - val_accuracy: 0.5974\n",
      "Epoch 567/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.9471 - accuracy: 0.6096\n",
      "Epoch 567: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9471 - accuracy: 0.6096 - val_loss: 0.9469 - val_accuracy: 0.5999\n",
      "Epoch 568/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.9339 - accuracy: 0.6132\n",
      "Epoch 568: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9339 - accuracy: 0.6132 - val_loss: 0.9318 - val_accuracy: 0.6122\n",
      "Epoch 569/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.9378 - accuracy: 0.6096\n",
      "Epoch 569: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9378 - accuracy: 0.6096 - val_loss: 0.9162 - val_accuracy: 0.6224\n",
      "Epoch 570/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.9335 - accuracy: 0.6124\n",
      "Epoch 570: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9335 - accuracy: 0.6124 - val_loss: 0.9206 - val_accuracy: 0.6155\n",
      "Epoch 571/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.9369 - accuracy: 0.6102\n",
      "Epoch 571: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9366 - accuracy: 0.6103 - val_loss: 0.9279 - val_accuracy: 0.6114\n",
      "Epoch 572/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.9533 - accuracy: 0.6083\n",
      "Epoch 572: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9530 - accuracy: 0.6084 - val_loss: 0.9090 - val_accuracy: 0.6261\n",
      "Epoch 573/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.9426 - accuracy: 0.6130\n",
      "Epoch 573: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9429 - accuracy: 0.6129 - val_loss: 0.9100 - val_accuracy: 0.6123\n",
      "Epoch 574/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.9307 - accuracy: 0.6175\n",
      "Epoch 574: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9306 - accuracy: 0.6176 - val_loss: 0.9122 - val_accuracy: 0.6162\n",
      "Epoch 575/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.9256 - accuracy: 0.6144\n",
      "Epoch 575: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9257 - accuracy: 0.6143 - val_loss: 0.9102 - val_accuracy: 0.6121\n",
      "Epoch 576/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.9220 - accuracy: 0.6159\n",
      "Epoch 576: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9220 - accuracy: 0.6159 - val_loss: 0.9010 - val_accuracy: 0.6256\n",
      "Epoch 577/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.9217 - accuracy: 0.6173\n",
      "Epoch 577: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9217 - accuracy: 0.6172 - val_loss: 0.9175 - val_accuracy: 0.6199\n",
      "Epoch 578/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.9177 - accuracy: 0.6188\n",
      "Epoch 578: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9177 - accuracy: 0.6188 - val_loss: 0.9451 - val_accuracy: 0.6093\n",
      "Epoch 579/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.9148 - accuracy: 0.6210\n",
      "Epoch 579: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9146 - accuracy: 0.6210 - val_loss: 0.8918 - val_accuracy: 0.6321\n",
      "Epoch 580/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.9176 - accuracy: 0.6201\n",
      "Epoch 580: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9177 - accuracy: 0.6201 - val_loss: 0.9099 - val_accuracy: 0.6226\n",
      "Epoch 581/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.9134 - accuracy: 0.6203\n",
      "Epoch 581: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9129 - accuracy: 0.6205 - val_loss: 0.9225 - val_accuracy: 0.6102\n",
      "Epoch 582/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.9140 - accuracy: 0.6204\n",
      "Epoch 582: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9136 - accuracy: 0.6206 - val_loss: 0.9243 - val_accuracy: 0.6176\n",
      "Epoch 583/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.9110 - accuracy: 0.6214\n",
      "Epoch 583: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9110 - accuracy: 0.6214 - val_loss: 0.8936 - val_accuracy: 0.6322\n",
      "Epoch 584/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.9111 - accuracy: 0.6234\n",
      "Epoch 584: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9111 - accuracy: 0.6234 - val_loss: 0.8984 - val_accuracy: 0.6256\n",
      "Epoch 585/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.9086 - accuracy: 0.6225\n",
      "Epoch 585: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9082 - accuracy: 0.6227 - val_loss: 0.9327 - val_accuracy: 0.6222\n",
      "Epoch 586/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.9065 - accuracy: 0.6251\n",
      "Epoch 586: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9062 - accuracy: 0.6253 - val_loss: 0.9178 - val_accuracy: 0.6235\n",
      "Epoch 587/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.9082 - accuracy: 0.6225\n",
      "Epoch 587: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9081 - accuracy: 0.6226 - val_loss: 0.9195 - val_accuracy: 0.6205\n",
      "Epoch 588/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.9059 - accuracy: 0.6243\n",
      "Epoch 588: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9061 - accuracy: 0.6242 - val_loss: 0.9279 - val_accuracy: 0.6250\n",
      "Epoch 589/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.9059 - accuracy: 0.6239\n",
      "Epoch 589: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9060 - accuracy: 0.6238 - val_loss: 0.9471 - val_accuracy: 0.6103\n",
      "Epoch 590/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.9048 - accuracy: 0.6263\n",
      "Epoch 590: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9047 - accuracy: 0.6264 - val_loss: 0.8919 - val_accuracy: 0.6252\n",
      "Epoch 591/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.9007 - accuracy: 0.6276\n",
      "Epoch 591: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9006 - accuracy: 0.6277 - val_loss: 0.9072 - val_accuracy: 0.6248\n",
      "Epoch 592/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8985 - accuracy: 0.6283\n",
      "Epoch 592: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 10s 12ms/step - loss: 0.8986 - accuracy: 0.6283 - val_loss: 0.9002 - val_accuracy: 0.6317\n",
      "Epoch 593/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.8985 - accuracy: 0.6267\n",
      "Epoch 593: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8988 - accuracy: 0.6265 - val_loss: 0.9319 - val_accuracy: 0.6176\n",
      "Epoch 594/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.9009 - accuracy: 0.6248\n",
      "Epoch 594: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9009 - accuracy: 0.6247 - val_loss: 0.8924 - val_accuracy: 0.6222\n",
      "Epoch 595/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8923 - accuracy: 0.6293\n",
      "Epoch 595: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8923 - accuracy: 0.6292 - val_loss: 0.9216 - val_accuracy: 0.6257\n",
      "Epoch 596/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8917 - accuracy: 0.6304\n",
      "Epoch 596: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8914 - accuracy: 0.6305 - val_loss: 0.9148 - val_accuracy: 0.6306\n",
      "Epoch 597/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.8916 - accuracy: 0.6286\n",
      "Epoch 597: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8916 - accuracy: 0.6286 - val_loss: 0.9552 - val_accuracy: 0.6076\n",
      "Epoch 598/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.8868 - accuracy: 0.6320\n",
      "Epoch 598: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8865 - accuracy: 0.6322 - val_loss: 0.9188 - val_accuracy: 0.6174\n",
      "Epoch 599/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8940 - accuracy: 0.6313\n",
      "Epoch 599: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8939 - accuracy: 0.6313 - val_loss: 0.9049 - val_accuracy: 0.6276\n",
      "Epoch 600/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.8917 - accuracy: 0.6312\n",
      "Epoch 600: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8917 - accuracy: 0.6312 - val_loss: 0.9261 - val_accuracy: 0.6234\n",
      "Epoch 601/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8886 - accuracy: 0.6301\n",
      "Epoch 601: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8886 - accuracy: 0.6302 - val_loss: 0.8807 - val_accuracy: 0.6421\n",
      "Epoch 602/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.8860 - accuracy: 0.6327\n",
      "Epoch 602: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8860 - accuracy: 0.6327 - val_loss: 0.8909 - val_accuracy: 0.6240\n",
      "Epoch 603/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.8889 - accuracy: 0.6325\n",
      "Epoch 603: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8889 - accuracy: 0.6325 - val_loss: 0.8671 - val_accuracy: 0.6380\n",
      "Epoch 604/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.8821 - accuracy: 0.6342\n",
      "Epoch 604: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8821 - accuracy: 0.6342 - val_loss: 0.9263 - val_accuracy: 0.6256\n",
      "Epoch 605/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8880 - accuracy: 0.6304\n",
      "Epoch 605: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8882 - accuracy: 0.6303 - val_loss: 0.8852 - val_accuracy: 0.6298\n",
      "Epoch 606/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8846 - accuracy: 0.6354\n",
      "Epoch 606: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8847 - accuracy: 0.6353 - val_loss: 0.8957 - val_accuracy: 0.6310\n",
      "Epoch 607/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8848 - accuracy: 0.6348\n",
      "Epoch 607: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8849 - accuracy: 0.6347 - val_loss: 0.8881 - val_accuracy: 0.6291\n",
      "Epoch 608/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8997 - accuracy: 0.6337\n",
      "Epoch 608: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8998 - accuracy: 0.6336 - val_loss: 0.9074 - val_accuracy: 0.6312\n",
      "Epoch 609/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8856 - accuracy: 0.6327\n",
      "Epoch 609: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8854 - accuracy: 0.6328 - val_loss: 0.8935 - val_accuracy: 0.6345\n",
      "Epoch 610/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.8791 - accuracy: 0.6331\n",
      "Epoch 610: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8791 - accuracy: 0.6330 - val_loss: 0.9122 - val_accuracy: 0.6285\n",
      "Epoch 611/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.8751 - accuracy: 0.6378\n",
      "Epoch 611: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8751 - accuracy: 0.6379 - val_loss: 0.8678 - val_accuracy: 0.6277\n",
      "Epoch 612/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8760 - accuracy: 0.6360\n",
      "Epoch 612: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8760 - accuracy: 0.6360 - val_loss: 0.8714 - val_accuracy: 0.6354\n",
      "Epoch 613/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.8794 - accuracy: 0.6368\n",
      "Epoch 613: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8794 - accuracy: 0.6368 - val_loss: 0.8810 - val_accuracy: 0.6300\n",
      "Epoch 614/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8728 - accuracy: 0.6404\n",
      "Epoch 614: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8726 - accuracy: 0.6405 - val_loss: 0.8613 - val_accuracy: 0.6362\n",
      "Epoch 615/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.8754 - accuracy: 0.6368\n",
      "Epoch 615: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8758 - accuracy: 0.6365 - val_loss: 0.8830 - val_accuracy: 0.6405\n",
      "Epoch 616/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.8757 - accuracy: 0.6397\n",
      "Epoch 616: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8760 - accuracy: 0.6396 - val_loss: 0.8730 - val_accuracy: 0.6444\n",
      "Epoch 617/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8737 - accuracy: 0.6389\n",
      "Epoch 617: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8737 - accuracy: 0.6389 - val_loss: 0.8650 - val_accuracy: 0.6430\n",
      "Epoch 618/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.8772 - accuracy: 0.6378\n",
      "Epoch 618: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8772 - accuracy: 0.6378 - val_loss: 0.8517 - val_accuracy: 0.6498\n",
      "Epoch 619/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.8712 - accuracy: 0.6404\n",
      "Epoch 619: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8713 - accuracy: 0.6403 - val_loss: 0.8641 - val_accuracy: 0.6342\n",
      "Epoch 620/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.8707 - accuracy: 0.6412\n",
      "Epoch 620: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8707 - accuracy: 0.6411 - val_loss: 0.8936 - val_accuracy: 0.6424\n",
      "Epoch 621/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8725 - accuracy: 0.6401\n",
      "Epoch 621: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8725 - accuracy: 0.6401 - val_loss: 0.8554 - val_accuracy: 0.6505\n",
      "Epoch 622/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.8692 - accuracy: 0.6424\n",
      "Epoch 622: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8692 - accuracy: 0.6424 - val_loss: 0.8600 - val_accuracy: 0.6414\n",
      "Epoch 623/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.8672 - accuracy: 0.6412\n",
      "Epoch 623: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8673 - accuracy: 0.6411 - val_loss: 0.8928 - val_accuracy: 0.6349\n",
      "Epoch 624/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.8697 - accuracy: 0.6406\n",
      "Epoch 624: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8695 - accuracy: 0.6406 - val_loss: 0.8505 - val_accuracy: 0.6468\n",
      "Epoch 625/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.8695 - accuracy: 0.6408\n",
      "Epoch 625: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8695 - accuracy: 0.6408 - val_loss: 0.8908 - val_accuracy: 0.6343\n",
      "Epoch 626/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.8627 - accuracy: 0.6434\n",
      "Epoch 626: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8627 - accuracy: 0.6434 - val_loss: 0.8566 - val_accuracy: 0.6534\n",
      "Epoch 627/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8698 - accuracy: 0.6409\n",
      "Epoch 627: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8697 - accuracy: 0.6410 - val_loss: 0.8615 - val_accuracy: 0.6460\n",
      "Epoch 628/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.8620 - accuracy: 0.6435\n",
      "Epoch 628: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8620 - accuracy: 0.6435 - val_loss: 0.8529 - val_accuracy: 0.6557\n",
      "Epoch 629/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.8645 - accuracy: 0.6441\n",
      "Epoch 629: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8645 - accuracy: 0.6441 - val_loss: 0.8941 - val_accuracy: 0.6389\n",
      "Epoch 630/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8665 - accuracy: 0.6416\n",
      "Epoch 630: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8664 - accuracy: 0.6417 - val_loss: 0.8406 - val_accuracy: 0.6479\n",
      "Epoch 631/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8634 - accuracy: 0.6438\n",
      "Epoch 631: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8637 - accuracy: 0.6437 - val_loss: 0.8838 - val_accuracy: 0.6439\n",
      "Epoch 632/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.8649 - accuracy: 0.6438\n",
      "Epoch 632: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8651 - accuracy: 0.6437 - val_loss: 0.8519 - val_accuracy: 0.6532\n",
      "Epoch 633/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.8579 - accuracy: 0.6464\n",
      "Epoch 633: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8580 - accuracy: 0.6464 - val_loss: 0.8577 - val_accuracy: 0.6481\n",
      "Epoch 634/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8588 - accuracy: 0.6446\n",
      "Epoch 634: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8589 - accuracy: 0.6446 - val_loss: 0.8780 - val_accuracy: 0.6449\n",
      "Epoch 635/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.8619 - accuracy: 0.6444\n",
      "Epoch 635: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8620 - accuracy: 0.6444 - val_loss: 0.8682 - val_accuracy: 0.6525\n",
      "Epoch 636/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.8549 - accuracy: 0.6480\n",
      "Epoch 636: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8549 - accuracy: 0.6480 - val_loss: 0.8848 - val_accuracy: 0.6366\n",
      "Epoch 637/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.8560 - accuracy: 0.6473\n",
      "Epoch 637: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 10s 12ms/step - loss: 0.8560 - accuracy: 0.6473 - val_loss: 0.8603 - val_accuracy: 0.6440\n",
      "Epoch 638/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.8585 - accuracy: 0.6462\n",
      "Epoch 638: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8585 - accuracy: 0.6462 - val_loss: 0.8743 - val_accuracy: 0.6471\n",
      "Epoch 639/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.8590 - accuracy: 0.6459\n",
      "Epoch 639: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8586 - accuracy: 0.6461 - val_loss: 0.8403 - val_accuracy: 0.6557\n",
      "Epoch 640/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.8526 - accuracy: 0.6475\n",
      "Epoch 640: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8526 - accuracy: 0.6475 - val_loss: 0.8643 - val_accuracy: 0.6466\n",
      "Epoch 641/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8554 - accuracy: 0.6484\n",
      "Epoch 641: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8555 - accuracy: 0.6484 - val_loss: 0.8746 - val_accuracy: 0.6441\n",
      "Epoch 642/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.8520 - accuracy: 0.6479\n",
      "Epoch 642: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8522 - accuracy: 0.6479 - val_loss: 0.8377 - val_accuracy: 0.6603\n",
      "Epoch 643/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8545 - accuracy: 0.6488\n",
      "Epoch 643: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8545 - accuracy: 0.6488 - val_loss: 0.8583 - val_accuracy: 0.6526\n",
      "Epoch 644/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8509 - accuracy: 0.6502\n",
      "Epoch 644: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8512 - accuracy: 0.6500 - val_loss: 0.8397 - val_accuracy: 0.6515\n",
      "Epoch 645/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.8541 - accuracy: 0.6486\n",
      "Epoch 645: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8541 - accuracy: 0.6487 - val_loss: 0.8791 - val_accuracy: 0.6530\n",
      "Epoch 646/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.8519 - accuracy: 0.6488\n",
      "Epoch 646: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8516 - accuracy: 0.6490 - val_loss: 0.8467 - val_accuracy: 0.6533\n",
      "Epoch 647/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.8541 - accuracy: 0.6477\n",
      "Epoch 647: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8537 - accuracy: 0.6479 - val_loss: 0.8766 - val_accuracy: 0.6416\n",
      "Epoch 648/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.8490 - accuracy: 0.6505\n",
      "Epoch 648: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8490 - accuracy: 0.6505 - val_loss: 0.8310 - val_accuracy: 0.6634\n",
      "Epoch 649/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.8532 - accuracy: 0.6494\n",
      "Epoch 649: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8531 - accuracy: 0.6493 - val_loss: 0.8751 - val_accuracy: 0.6399\n",
      "Epoch 650/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8528 - accuracy: 0.6496\n",
      "Epoch 650: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8526 - accuracy: 0.6497 - val_loss: 0.8172 - val_accuracy: 0.6634\n",
      "Epoch 651/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.8510 - accuracy: 0.6500\n",
      "Epoch 651: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8510 - accuracy: 0.6500 - val_loss: 0.8401 - val_accuracy: 0.6620\n",
      "Epoch 652/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.8436 - accuracy: 0.6516\n",
      "Epoch 652: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8436 - accuracy: 0.6516 - val_loss: 0.8663 - val_accuracy: 0.6550\n",
      "Epoch 653/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.8469 - accuracy: 0.6520\n",
      "Epoch 653: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8469 - accuracy: 0.6520 - val_loss: 0.8244 - val_accuracy: 0.6646\n",
      "Epoch 654/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.8446 - accuracy: 0.6518\n",
      "Epoch 654: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8445 - accuracy: 0.6519 - val_loss: 0.8446 - val_accuracy: 0.6530\n",
      "Epoch 655/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.8520 - accuracy: 0.6498\n",
      "Epoch 655: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8520 - accuracy: 0.6498 - val_loss: 0.8520 - val_accuracy: 0.6537\n",
      "Epoch 656/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.8420 - accuracy: 0.6543\n",
      "Epoch 656: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8420 - accuracy: 0.6543 - val_loss: 0.8388 - val_accuracy: 0.6633\n",
      "Epoch 657/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.8450 - accuracy: 0.6522\n",
      "Epoch 657: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8450 - accuracy: 0.6522 - val_loss: 0.8266 - val_accuracy: 0.6623\n",
      "Epoch 658/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.8449 - accuracy: 0.6527\n",
      "Epoch 658: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8451 - accuracy: 0.6525 - val_loss: 0.8661 - val_accuracy: 0.6534\n",
      "Epoch 659/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.8428 - accuracy: 0.6536\n",
      "Epoch 659: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8430 - accuracy: 0.6536 - val_loss: 0.8353 - val_accuracy: 0.6619\n",
      "Epoch 660/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.8466 - accuracy: 0.6515\n",
      "Epoch 660: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8468 - accuracy: 0.6514 - val_loss: 0.8302 - val_accuracy: 0.6611\n",
      "Epoch 661/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.8466 - accuracy: 0.6501\n",
      "Epoch 661: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8466 - accuracy: 0.6501 - val_loss: 0.8296 - val_accuracy: 0.6573\n",
      "Epoch 662/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.8434 - accuracy: 0.6508\n",
      "Epoch 662: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8434 - accuracy: 0.6508 - val_loss: 0.8244 - val_accuracy: 0.6589\n",
      "Epoch 663/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.8425 - accuracy: 0.6526\n",
      "Epoch 663: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8425 - accuracy: 0.6526 - val_loss: 0.8253 - val_accuracy: 0.6501\n",
      "Epoch 664/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.8393 - accuracy: 0.6559\n",
      "Epoch 664: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8391 - accuracy: 0.6561 - val_loss: 0.8366 - val_accuracy: 0.6619\n",
      "Epoch 665/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.8415 - accuracy: 0.6537\n",
      "Epoch 665: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8417 - accuracy: 0.6537 - val_loss: 0.8270 - val_accuracy: 0.6567\n",
      "Epoch 666/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.8423 - accuracy: 0.6549\n",
      "Epoch 666: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8423 - accuracy: 0.6549 - val_loss: 0.8498 - val_accuracy: 0.6603\n",
      "Epoch 667/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8416 - accuracy: 0.6542\n",
      "Epoch 667: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8415 - accuracy: 0.6542 - val_loss: 0.8593 - val_accuracy: 0.6373\n",
      "Epoch 668/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.8377 - accuracy: 0.6555\n",
      "Epoch 668: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8372 - accuracy: 0.6557 - val_loss: 0.8723 - val_accuracy: 0.6541\n",
      "Epoch 669/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.8391 - accuracy: 0.6554\n",
      "Epoch 669: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8391 - accuracy: 0.6554 - val_loss: 0.8148 - val_accuracy: 0.6592\n",
      "Epoch 670/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8383 - accuracy: 0.6546\n",
      "Epoch 670: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8382 - accuracy: 0.6547 - val_loss: 0.8312 - val_accuracy: 0.6615\n",
      "Epoch 671/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.8378 - accuracy: 0.6546\n",
      "Epoch 671: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8378 - accuracy: 0.6546 - val_loss: 0.8376 - val_accuracy: 0.6559\n",
      "Epoch 672/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.8370 - accuracy: 0.6556\n",
      "Epoch 672: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8369 - accuracy: 0.6557 - val_loss: 0.8104 - val_accuracy: 0.6729\n",
      "Epoch 673/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.8346 - accuracy: 0.6564\n",
      "Epoch 673: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8346 - accuracy: 0.6564 - val_loss: 0.8270 - val_accuracy: 0.6697\n",
      "Epoch 674/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8327 - accuracy: 0.6562\n",
      "Epoch 674: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8328 - accuracy: 0.6561 - val_loss: 0.8837 - val_accuracy: 0.6479\n",
      "Epoch 675/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8370 - accuracy: 0.6561\n",
      "Epoch 675: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8368 - accuracy: 0.6562 - val_loss: 0.8689 - val_accuracy: 0.6567\n",
      "Epoch 676/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.8470 - accuracy: 0.6527\n",
      "Epoch 676: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8470 - accuracy: 0.6526 - val_loss: 0.8448 - val_accuracy: 0.6644\n",
      "Epoch 677/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.8326 - accuracy: 0.6583\n",
      "Epoch 677: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8326 - accuracy: 0.6583 - val_loss: 0.8072 - val_accuracy: 0.6704\n",
      "Epoch 678/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8304 - accuracy: 0.6572\n",
      "Epoch 678: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8304 - accuracy: 0.6571 - val_loss: 0.7991 - val_accuracy: 0.6735\n",
      "Epoch 679/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.8323 - accuracy: 0.6590\n",
      "Epoch 679: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8323 - accuracy: 0.6589 - val_loss: 0.8594 - val_accuracy: 0.6525\n",
      "Epoch 680/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 2.8978 - accuracy: 0.6330\n",
      "Epoch 680: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 2.8978 - accuracy: 0.6330 - val_loss: 0.8850 - val_accuracy: 0.6488\n",
      "Epoch 681/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.8658 - accuracy: 0.6483\n",
      "Epoch 681: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8658 - accuracy: 0.6483 - val_loss: 0.8501 - val_accuracy: 0.6557\n",
      "Epoch 682/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8484 - accuracy: 0.6542\n",
      "Epoch 682: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8485 - accuracy: 0.6541 - val_loss: 0.8576 - val_accuracy: 0.6496\n",
      "Epoch 683/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.8448 - accuracy: 0.6553\n",
      "Epoch 683: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8448 - accuracy: 0.6553 - val_loss: 0.8683 - val_accuracy: 0.6540\n",
      "Epoch 684/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.8380 - accuracy: 0.6556\n",
      "Epoch 684: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8380 - accuracy: 0.6556 - val_loss: 0.8107 - val_accuracy: 0.6695\n",
      "Epoch 685/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.8370 - accuracy: 0.6577\n",
      "Epoch 685: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8368 - accuracy: 0.6579 - val_loss: 0.8555 - val_accuracy: 0.6578\n",
      "Epoch 686/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.8377 - accuracy: 0.6569\n",
      "Epoch 686: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8376 - accuracy: 0.6569 - val_loss: 0.8253 - val_accuracy: 0.6640\n",
      "Epoch 687/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.8347 - accuracy: 0.6582\n",
      "Epoch 687: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8347 - accuracy: 0.6582 - val_loss: 0.8209 - val_accuracy: 0.6678\n",
      "Epoch 688/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.8365 - accuracy: 0.6572\n",
      "Epoch 688: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8361 - accuracy: 0.6573 - val_loss: 0.8220 - val_accuracy: 0.6671\n",
      "Epoch 689/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.8388 - accuracy: 0.6581\n",
      "Epoch 689: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8387 - accuracy: 0.6581 - val_loss: 0.8524 - val_accuracy: 0.6617\n",
      "Epoch 690/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.8326 - accuracy: 0.6592\n",
      "Epoch 690: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8325 - accuracy: 0.6593 - val_loss: 0.8594 - val_accuracy: 0.6586\n",
      "Epoch 691/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.8314 - accuracy: 0.6612\n",
      "Epoch 691: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8312 - accuracy: 0.6612 - val_loss: 0.8160 - val_accuracy: 0.6681\n",
      "Epoch 692/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8300 - accuracy: 0.6602\n",
      "Epoch 692: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8300 - accuracy: 0.6602 - val_loss: 0.8003 - val_accuracy: 0.6700\n",
      "Epoch 693/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.8278 - accuracy: 0.6608\n",
      "Epoch 693: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8278 - accuracy: 0.6609 - val_loss: 0.8242 - val_accuracy: 0.6632\n",
      "Epoch 694/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.8275 - accuracy: 0.6611\n",
      "Epoch 694: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8276 - accuracy: 0.6609 - val_loss: 0.8231 - val_accuracy: 0.6662\n",
      "Epoch 695/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.8252 - accuracy: 0.6622\n",
      "Epoch 695: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8249 - accuracy: 0.6622 - val_loss: 0.8333 - val_accuracy: 0.6603\n",
      "Epoch 696/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.8265 - accuracy: 0.6613\n",
      "Epoch 696: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8265 - accuracy: 0.6613 - val_loss: 0.7975 - val_accuracy: 0.6758\n",
      "Epoch 697/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.8290 - accuracy: 0.6616\n",
      "Epoch 697: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8290 - accuracy: 0.6616 - val_loss: 0.8302 - val_accuracy: 0.6676\n",
      "Epoch 698/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.8274 - accuracy: 0.6604\n",
      "Epoch 698: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8274 - accuracy: 0.6604 - val_loss: 0.8071 - val_accuracy: 0.6657\n",
      "Epoch 699/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.8239 - accuracy: 0.6620\n",
      "Epoch 699: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8238 - accuracy: 0.6620 - val_loss: 0.8580 - val_accuracy: 0.6592\n",
      "Epoch 700/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.8268 - accuracy: 0.6582\n",
      "Epoch 700: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8265 - accuracy: 0.6582 - val_loss: 0.8006 - val_accuracy: 0.6712\n",
      "Epoch 701/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.8327 - accuracy: 0.6549\n",
      "Epoch 701: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8324 - accuracy: 0.6551 - val_loss: 0.8061 - val_accuracy: 0.6686\n",
      "Epoch 702/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.8278 - accuracy: 0.6586\n",
      "Epoch 702: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8278 - accuracy: 0.6585 - val_loss: 0.8159 - val_accuracy: 0.6690\n",
      "Epoch 703/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.8250 - accuracy: 0.6613\n",
      "Epoch 703: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8252 - accuracy: 0.6612 - val_loss: 0.8076 - val_accuracy: 0.6740\n",
      "Epoch 704/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.8213 - accuracy: 0.6625\n",
      "Epoch 704: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8213 - accuracy: 0.6625 - val_loss: 0.7983 - val_accuracy: 0.6711\n",
      "Epoch 705/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.8207 - accuracy: 0.6641\n",
      "Epoch 705: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8203 - accuracy: 0.6642 - val_loss: 0.7859 - val_accuracy: 0.6817\n",
      "Epoch 706/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8243 - accuracy: 0.6610\n",
      "Epoch 706: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8244 - accuracy: 0.6610 - val_loss: 0.8035 - val_accuracy: 0.6664\n",
      "Epoch 707/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.8231 - accuracy: 0.6626\n",
      "Epoch 707: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8232 - accuracy: 0.6626 - val_loss: 0.8084 - val_accuracy: 0.6788\n",
      "Epoch 708/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.8218 - accuracy: 0.6617\n",
      "Epoch 708: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8220 - accuracy: 0.6616 - val_loss: 0.7940 - val_accuracy: 0.6744\n",
      "Epoch 709/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.8209 - accuracy: 0.6640\n",
      "Epoch 709: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8209 - accuracy: 0.6640 - val_loss: 0.8077 - val_accuracy: 0.6708\n",
      "Epoch 710/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8198 - accuracy: 0.6642\n",
      "Epoch 710: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8197 - accuracy: 0.6643 - val_loss: 0.7961 - val_accuracy: 0.6768\n",
      "Epoch 711/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.8170 - accuracy: 0.6650\n",
      "Epoch 711: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8168 - accuracy: 0.6649 - val_loss: 0.7981 - val_accuracy: 0.6714\n",
      "Epoch 712/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.8184 - accuracy: 0.6646\n",
      "Epoch 712: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8182 - accuracy: 0.6646 - val_loss: 0.8303 - val_accuracy: 0.6566\n",
      "Epoch 713/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.8194 - accuracy: 0.6637\n",
      "Epoch 713: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8192 - accuracy: 0.6637 - val_loss: 0.8028 - val_accuracy: 0.6788\n",
      "Epoch 714/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8144 - accuracy: 0.6663\n",
      "Epoch 714: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8145 - accuracy: 0.6662 - val_loss: 0.8492 - val_accuracy: 0.6632\n",
      "Epoch 715/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.8200 - accuracy: 0.6650\n",
      "Epoch 715: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8200 - accuracy: 0.6649 - val_loss: 0.8007 - val_accuracy: 0.6739\n",
      "Epoch 716/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.8116 - accuracy: 0.6671\n",
      "Epoch 716: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8118 - accuracy: 0.6670 - val_loss: 0.7909 - val_accuracy: 0.6782\n",
      "Epoch 717/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.8182 - accuracy: 0.6649\n",
      "Epoch 717: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8182 - accuracy: 0.6648 - val_loss: 0.8039 - val_accuracy: 0.6703\n",
      "Epoch 718/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.8219 - accuracy: 0.6626\n",
      "Epoch 718: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8222 - accuracy: 0.6625 - val_loss: 0.8667 - val_accuracy: 0.6579\n",
      "Epoch 719/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.8190 - accuracy: 0.6653\n",
      "Epoch 719: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8188 - accuracy: 0.6653 - val_loss: 0.8164 - val_accuracy: 0.6722\n",
      "Epoch 720/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.8139 - accuracy: 0.6665\n",
      "Epoch 720: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8139 - accuracy: 0.6665 - val_loss: 0.7946 - val_accuracy: 0.6785\n",
      "Epoch 721/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.8161 - accuracy: 0.6659\n",
      "Epoch 721: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8163 - accuracy: 0.6660 - val_loss: 0.8014 - val_accuracy: 0.6796\n",
      "Epoch 722/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.8157 - accuracy: 0.6668\n",
      "Epoch 722: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8156 - accuracy: 0.6670 - val_loss: 0.8076 - val_accuracy: 0.6727\n",
      "Epoch 723/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.8134 - accuracy: 0.6669\n",
      "Epoch 723: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8129 - accuracy: 0.6670 - val_loss: 0.8029 - val_accuracy: 0.6694\n",
      "Epoch 724/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.8299 - accuracy: 0.6625\n",
      "Epoch 724: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8299 - accuracy: 0.6624 - val_loss: 0.8533 - val_accuracy: 0.6602\n",
      "Epoch 725/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.8167 - accuracy: 0.6650\n",
      "Epoch 725: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8168 - accuracy: 0.6649 - val_loss: 0.7982 - val_accuracy: 0.6726\n",
      "Epoch 726/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.8116 - accuracy: 0.6678\n",
      "Epoch 726: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8116 - accuracy: 0.6678 - val_loss: 0.7873 - val_accuracy: 0.6897\n",
      "Epoch 727/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.8272 - accuracy: 0.6617\n",
      "Epoch 727: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8272 - accuracy: 0.6616 - val_loss: 0.7826 - val_accuracy: 0.6779\n",
      "Epoch 728/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8158 - accuracy: 0.6653\n",
      "Epoch 728: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8158 - accuracy: 0.6654 - val_loss: 0.7855 - val_accuracy: 0.6827\n",
      "Epoch 729/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.8139 - accuracy: 0.6670\n",
      "Epoch 729: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8138 - accuracy: 0.6671 - val_loss: 0.7893 - val_accuracy: 0.6781\n",
      "Epoch 730/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.8115 - accuracy: 0.6679\n",
      "Epoch 730: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8114 - accuracy: 0.6678 - val_loss: 0.8366 - val_accuracy: 0.6680\n",
      "Epoch 731/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.8066 - accuracy: 0.6710\n",
      "Epoch 731: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8066 - accuracy: 0.6711 - val_loss: 0.8229 - val_accuracy: 0.6712\n",
      "Epoch 732/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.8128 - accuracy: 0.6667\n",
      "Epoch 732: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8126 - accuracy: 0.6667 - val_loss: 0.8011 - val_accuracy: 0.6760\n",
      "Epoch 733/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.8119 - accuracy: 0.6676\n",
      "Epoch 733: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8117 - accuracy: 0.6677 - val_loss: 0.7871 - val_accuracy: 0.6842\n",
      "Epoch 734/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.8068 - accuracy: 0.6706\n",
      "Epoch 734: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8068 - accuracy: 0.6706 - val_loss: 0.7976 - val_accuracy: 0.6815\n",
      "Epoch 735/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.8057 - accuracy: 0.6710\n",
      "Epoch 735: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8055 - accuracy: 0.6711 - val_loss: 0.8182 - val_accuracy: 0.6761\n",
      "Epoch 736/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.8030 - accuracy: 0.6699\n",
      "Epoch 736: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8025 - accuracy: 0.6701 - val_loss: 0.7840 - val_accuracy: 0.6756\n",
      "Epoch 737/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.8187 - accuracy: 0.6676\n",
      "Epoch 737: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8188 - accuracy: 0.6676 - val_loss: 0.8480 - val_accuracy: 0.6635\n",
      "Epoch 738/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.8076 - accuracy: 0.6700\n",
      "Epoch 738: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8076 - accuracy: 0.6699 - val_loss: 0.7820 - val_accuracy: 0.6847\n",
      "Epoch 739/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8038 - accuracy: 0.6714\n",
      "Epoch 739: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8038 - accuracy: 0.6715 - val_loss: 0.7942 - val_accuracy: 0.6761\n",
      "Epoch 740/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.8095 - accuracy: 0.6684\n",
      "Epoch 740: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8095 - accuracy: 0.6684 - val_loss: 0.8110 - val_accuracy: 0.6756\n",
      "Epoch 741/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8044 - accuracy: 0.6714\n",
      "Epoch 741: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8043 - accuracy: 0.6715 - val_loss: 0.8090 - val_accuracy: 0.6702\n",
      "Epoch 742/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.8067 - accuracy: 0.6704\n",
      "Epoch 742: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8067 - accuracy: 0.6704 - val_loss: 0.7912 - val_accuracy: 0.6780\n",
      "Epoch 743/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.8003 - accuracy: 0.6722\n",
      "Epoch 743: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8003 - accuracy: 0.6722 - val_loss: 0.7735 - val_accuracy: 0.6882\n",
      "Epoch 744/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.8066 - accuracy: 0.6692\n",
      "Epoch 744: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8065 - accuracy: 0.6693 - val_loss: 0.7743 - val_accuracy: 0.6834\n",
      "Epoch 745/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8051 - accuracy: 0.6695\n",
      "Epoch 745: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8052 - accuracy: 0.6694 - val_loss: 0.7522 - val_accuracy: 0.6959\n",
      "Epoch 746/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.8042 - accuracy: 0.6714\n",
      "Epoch 746: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8042 - accuracy: 0.6714 - val_loss: 0.7727 - val_accuracy: 0.6866\n",
      "Epoch 747/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.8037 - accuracy: 0.6710\n",
      "Epoch 747: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8038 - accuracy: 0.6709 - val_loss: 0.8083 - val_accuracy: 0.6764\n",
      "Epoch 748/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.8035 - accuracy: 0.6726\n",
      "Epoch 748: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8036 - accuracy: 0.6725 - val_loss: 0.8226 - val_accuracy: 0.6697\n",
      "Epoch 749/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8100 - accuracy: 0.6705\n",
      "Epoch 749: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8099 - accuracy: 0.6705 - val_loss: 0.8168 - val_accuracy: 0.6599\n",
      "Epoch 750/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.8030 - accuracy: 0.6715\n",
      "Epoch 750: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8030 - accuracy: 0.6715 - val_loss: 0.7836 - val_accuracy: 0.6838\n",
      "Epoch 751/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.8059 - accuracy: 0.6707\n",
      "Epoch 751: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8059 - accuracy: 0.6707 - val_loss: 0.8155 - val_accuracy: 0.6765\n",
      "Epoch 752/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8032 - accuracy: 0.6732\n",
      "Epoch 752: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8033 - accuracy: 0.6731 - val_loss: 0.7962 - val_accuracy: 0.6837\n",
      "Epoch 753/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8045 - accuracy: 0.6721\n",
      "Epoch 753: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8044 - accuracy: 0.6721 - val_loss: 0.7920 - val_accuracy: 0.6812\n",
      "Epoch 754/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8033 - accuracy: 0.6713\n",
      "Epoch 754: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8032 - accuracy: 0.6713 - val_loss: 0.7853 - val_accuracy: 0.6814\n",
      "Epoch 755/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.8099 - accuracy: 0.6705\n",
      "Epoch 755: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8098 - accuracy: 0.6706 - val_loss: 0.7868 - val_accuracy: 0.6827\n",
      "Epoch 756/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8042 - accuracy: 0.6713\n",
      "Epoch 756: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8044 - accuracy: 0.6713 - val_loss: 0.7976 - val_accuracy: 0.6738\n",
      "Epoch 757/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.7953 - accuracy: 0.6754\n",
      "Epoch 757: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7952 - accuracy: 0.6754 - val_loss: 0.7724 - val_accuracy: 0.6917\n",
      "Epoch 758/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.8013 - accuracy: 0.6732\n",
      "Epoch 758: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8013 - accuracy: 0.6732 - val_loss: 0.7794 - val_accuracy: 0.6926\n",
      "Epoch 759/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.8008 - accuracy: 0.6734\n",
      "Epoch 759: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8006 - accuracy: 0.6734 - val_loss: 0.7650 - val_accuracy: 0.6878\n",
      "Epoch 760/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7993 - accuracy: 0.6738\n",
      "Epoch 760: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7993 - accuracy: 0.6738 - val_loss: 0.7950 - val_accuracy: 0.6790\n",
      "Epoch 761/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.8025 - accuracy: 0.6719\n",
      "Epoch 761: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8027 - accuracy: 0.6717 - val_loss: 0.7934 - val_accuracy: 0.6824\n",
      "Epoch 762/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.8034 - accuracy: 0.6690\n",
      "Epoch 762: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8035 - accuracy: 0.6690 - val_loss: 0.7572 - val_accuracy: 0.6998\n",
      "Epoch 763/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7995 - accuracy: 0.6740\n",
      "Epoch 763: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7995 - accuracy: 0.6740 - val_loss: 0.7997 - val_accuracy: 0.6793\n",
      "Epoch 764/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7937 - accuracy: 0.6750\n",
      "Epoch 764: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7937 - accuracy: 0.6750 - val_loss: 0.7717 - val_accuracy: 0.6840\n",
      "Epoch 765/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7951 - accuracy: 0.6755\n",
      "Epoch 765: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7951 - accuracy: 0.6755 - val_loss: 0.8260 - val_accuracy: 0.6639\n",
      "Epoch 766/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.7941 - accuracy: 0.6758\n",
      "Epoch 766: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7940 - accuracy: 0.6758 - val_loss: 0.8273 - val_accuracy: 0.6725\n",
      "Epoch 767/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7958 - accuracy: 0.6757\n",
      "Epoch 767: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7957 - accuracy: 0.6757 - val_loss: 0.7877 - val_accuracy: 0.6882\n",
      "Epoch 768/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7917 - accuracy: 0.6781\n",
      "Epoch 768: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7917 - accuracy: 0.6781 - val_loss: 0.7689 - val_accuracy: 0.6946\n",
      "Epoch 769/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7952 - accuracy: 0.6778\n",
      "Epoch 769: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7953 - accuracy: 0.6777 - val_loss: 0.7605 - val_accuracy: 0.6991\n",
      "Epoch 770/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.8035 - accuracy: 0.6725\n",
      "Epoch 770: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8033 - accuracy: 0.6725 - val_loss: 0.7976 - val_accuracy: 0.6813\n",
      "Epoch 771/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7947 - accuracy: 0.6767\n",
      "Epoch 771: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7947 - accuracy: 0.6767 - val_loss: 0.7596 - val_accuracy: 0.6885\n",
      "Epoch 772/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7957 - accuracy: 0.6772\n",
      "Epoch 772: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7958 - accuracy: 0.6772 - val_loss: 0.8079 - val_accuracy: 0.6789\n",
      "Epoch 773/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.8006 - accuracy: 0.6734\n",
      "Epoch 773: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8006 - accuracy: 0.6734 - val_loss: 0.8123 - val_accuracy: 0.6814\n",
      "Epoch 774/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.7934 - accuracy: 0.6757\n",
      "Epoch 774: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7937 - accuracy: 0.6757 - val_loss: 0.7766 - val_accuracy: 0.6856\n",
      "Epoch 775/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.7929 - accuracy: 0.6781\n",
      "Epoch 775: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7926 - accuracy: 0.6783 - val_loss: 0.7906 - val_accuracy: 0.6808\n",
      "Epoch 776/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7967 - accuracy: 0.6772\n",
      "Epoch 776: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7969 - accuracy: 0.6772 - val_loss: 0.8021 - val_accuracy: 0.6823\n",
      "Epoch 777/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.7940 - accuracy: 0.6774\n",
      "Epoch 777: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7942 - accuracy: 0.6773 - val_loss: 0.7798 - val_accuracy: 0.6906\n",
      "Epoch 778/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7915 - accuracy: 0.6786\n",
      "Epoch 778: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7914 - accuracy: 0.6785 - val_loss: 0.7918 - val_accuracy: 0.6838\n",
      "Epoch 779/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7961 - accuracy: 0.6746\n",
      "Epoch 779: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7961 - accuracy: 0.6746 - val_loss: 0.7893 - val_accuracy: 0.6855\n",
      "Epoch 780/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7923 - accuracy: 0.6768\n",
      "Epoch 780: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7923 - accuracy: 0.6768 - val_loss: 0.7739 - val_accuracy: 0.6922\n",
      "Epoch 781/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.7997 - accuracy: 0.6758\n",
      "Epoch 781: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7998 - accuracy: 0.6757 - val_loss: 0.7886 - val_accuracy: 0.6749\n",
      "Epoch 782/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.7941 - accuracy: 0.6779\n",
      "Epoch 782: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7939 - accuracy: 0.6781 - val_loss: 0.8318 - val_accuracy: 0.6675\n",
      "Epoch 783/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.7956 - accuracy: 0.6764\n",
      "Epoch 783: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7955 - accuracy: 0.6765 - val_loss: 0.7484 - val_accuracy: 0.7022\n",
      "Epoch 784/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7893 - accuracy: 0.6792\n",
      "Epoch 784: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7893 - accuracy: 0.6792 - val_loss: 0.7612 - val_accuracy: 0.6982\n",
      "Epoch 785/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7937 - accuracy: 0.6767\n",
      "Epoch 785: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7937 - accuracy: 0.6767 - val_loss: 0.7706 - val_accuracy: 0.6912\n",
      "Epoch 786/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7942 - accuracy: 0.6780\n",
      "Epoch 786: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7942 - accuracy: 0.6779 - val_loss: 0.8480 - val_accuracy: 0.6764\n",
      "Epoch 787/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.7914 - accuracy: 0.6779\n",
      "Epoch 787: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7915 - accuracy: 0.6779 - val_loss: 0.7962 - val_accuracy: 0.6811\n",
      "Epoch 788/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7883 - accuracy: 0.6797\n",
      "Epoch 788: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7881 - accuracy: 0.6798 - val_loss: 0.7876 - val_accuracy: 0.6890\n",
      "Epoch 789/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.7870 - accuracy: 0.6794\n",
      "Epoch 789: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7870 - accuracy: 0.6794 - val_loss: 0.7352 - val_accuracy: 0.7078\n",
      "Epoch 790/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.7910 - accuracy: 0.6785\n",
      "Epoch 790: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7908 - accuracy: 0.6785 - val_loss: 0.7755 - val_accuracy: 0.6880\n",
      "Epoch 791/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7893 - accuracy: 0.6783\n",
      "Epoch 791: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7893 - accuracy: 0.6783 - val_loss: 0.8155 - val_accuracy: 0.6685\n",
      "Epoch 792/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7913 - accuracy: 0.6783\n",
      "Epoch 792: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7911 - accuracy: 0.6784 - val_loss: 0.7890 - val_accuracy: 0.6901\n",
      "Epoch 793/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.7871 - accuracy: 0.6792\n",
      "Epoch 793: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7870 - accuracy: 0.6793 - val_loss: 0.7795 - val_accuracy: 0.6902\n",
      "Epoch 794/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.7900 - accuracy: 0.6788\n",
      "Epoch 794: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7900 - accuracy: 0.6789 - val_loss: 0.7939 - val_accuracy: 0.6881\n",
      "Epoch 795/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.8150 - accuracy: 0.6749\n",
      "Epoch 795: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8159 - accuracy: 0.6746 - val_loss: 1.1504 - val_accuracy: 0.5912\n",
      "Epoch 796/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.9490 - accuracy: 0.6284\n",
      "Epoch 796: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.9489 - accuracy: 0.6285 - val_loss: 0.8561 - val_accuracy: 0.6607\n",
      "Epoch 797/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8541 - accuracy: 0.6545\n",
      "Epoch 797: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8543 - accuracy: 0.6544 - val_loss: 0.8300 - val_accuracy: 0.6676\n",
      "Epoch 798/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.8262 - accuracy: 0.6642\n",
      "Epoch 798: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8262 - accuracy: 0.6643 - val_loss: 0.8198 - val_accuracy: 0.6747\n",
      "Epoch 799/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.8118 - accuracy: 0.6720\n",
      "Epoch 799: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.8115 - accuracy: 0.6720 - val_loss: 0.7888 - val_accuracy: 0.6854\n",
      "Epoch 800/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7950 - accuracy: 0.6766\n",
      "Epoch 800: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7951 - accuracy: 0.6766 - val_loss: 0.7771 - val_accuracy: 0.6907\n",
      "Epoch 801/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7836 - accuracy: 0.6822\n",
      "Epoch 801: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7836 - accuracy: 0.6822 - val_loss: 0.7740 - val_accuracy: 0.6934\n",
      "Epoch 802/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7868 - accuracy: 0.6807\n",
      "Epoch 802: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7868 - accuracy: 0.6807 - val_loss: 0.7536 - val_accuracy: 0.6975\n",
      "Epoch 803/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7799 - accuracy: 0.6820\n",
      "Epoch 803: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7800 - accuracy: 0.6820 - val_loss: 0.7815 - val_accuracy: 0.6884\n",
      "Epoch 804/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7817 - accuracy: 0.6834\n",
      "Epoch 804: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7818 - accuracy: 0.6834 - val_loss: 0.7890 - val_accuracy: 0.6924\n",
      "Epoch 805/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7834 - accuracy: 0.6801\n",
      "Epoch 805: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7834 - accuracy: 0.6801 - val_loss: 0.7828 - val_accuracy: 0.6880\n",
      "Epoch 806/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7767 - accuracy: 0.6851\n",
      "Epoch 806: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7765 - accuracy: 0.6851 - val_loss: 0.7692 - val_accuracy: 0.6963\n",
      "Epoch 807/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7839 - accuracy: 0.6813\n",
      "Epoch 807: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7838 - accuracy: 0.6813 - val_loss: 0.7561 - val_accuracy: 0.7025\n",
      "Epoch 808/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7863 - accuracy: 0.6811\n",
      "Epoch 808: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7863 - accuracy: 0.6811 - val_loss: 0.7635 - val_accuracy: 0.6969\n",
      "Epoch 809/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7819 - accuracy: 0.6817\n",
      "Epoch 809: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7819 - accuracy: 0.6817 - val_loss: 0.7669 - val_accuracy: 0.6997\n",
      "Epoch 810/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7811 - accuracy: 0.6835\n",
      "Epoch 810: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7811 - accuracy: 0.6835 - val_loss: 0.7732 - val_accuracy: 0.6952\n",
      "Epoch 811/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7781 - accuracy: 0.6837\n",
      "Epoch 811: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7783 - accuracy: 0.6837 - val_loss: 0.7607 - val_accuracy: 0.6981\n",
      "Epoch 812/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.7815 - accuracy: 0.6829\n",
      "Epoch 812: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7814 - accuracy: 0.6830 - val_loss: 0.7650 - val_accuracy: 0.6933\n",
      "Epoch 813/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.7820 - accuracy: 0.6841\n",
      "Epoch 813: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7819 - accuracy: 0.6841 - val_loss: 0.7695 - val_accuracy: 0.6983\n",
      "Epoch 814/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7841 - accuracy: 0.6834\n",
      "Epoch 814: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7841 - accuracy: 0.6834 - val_loss: 0.7560 - val_accuracy: 0.7016\n",
      "Epoch 815/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7825 - accuracy: 0.6843\n",
      "Epoch 815: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7824 - accuracy: 0.6843 - val_loss: 0.7574 - val_accuracy: 0.7023\n",
      "Epoch 816/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7887 - accuracy: 0.6804\n",
      "Epoch 816: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7885 - accuracy: 0.6805 - val_loss: 0.8079 - val_accuracy: 0.6862\n",
      "Epoch 817/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7832 - accuracy: 0.6823\n",
      "Epoch 817: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7832 - accuracy: 0.6822 - val_loss: 0.7540 - val_accuracy: 0.6954\n",
      "Epoch 818/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7858 - accuracy: 0.6819\n",
      "Epoch 818: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7859 - accuracy: 0.6819 - val_loss: 0.7421 - val_accuracy: 0.7016\n",
      "Epoch 819/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.7782 - accuracy: 0.6849\n",
      "Epoch 819: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7782 - accuracy: 0.6849 - val_loss: 0.7528 - val_accuracy: 0.6976\n",
      "Epoch 820/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.7789 - accuracy: 0.6828\n",
      "Epoch 820: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7789 - accuracy: 0.6829 - val_loss: 0.7433 - val_accuracy: 0.7063\n",
      "Epoch 821/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7856 - accuracy: 0.6824\n",
      "Epoch 821: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7856 - accuracy: 0.6825 - val_loss: 0.7813 - val_accuracy: 0.6940\n",
      "Epoch 822/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7798 - accuracy: 0.6833\n",
      "Epoch 822: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7798 - accuracy: 0.6833 - val_loss: 0.7819 - val_accuracy: 0.6899\n",
      "Epoch 823/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.7826 - accuracy: 0.6836\n",
      "Epoch 823: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7826 - accuracy: 0.6836 - val_loss: 0.7717 - val_accuracy: 0.6926\n",
      "Epoch 824/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7771 - accuracy: 0.6854\n",
      "Epoch 824: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7771 - accuracy: 0.6854 - val_loss: 0.7673 - val_accuracy: 0.6954\n",
      "Epoch 825/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7834 - accuracy: 0.6837\n",
      "Epoch 825: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7834 - accuracy: 0.6837 - val_loss: 0.7608 - val_accuracy: 0.7001\n",
      "Epoch 826/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7760 - accuracy: 0.6856\n",
      "Epoch 826: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7760 - accuracy: 0.6856 - val_loss: 0.7372 - val_accuracy: 0.7089\n",
      "Epoch 827/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.7805 - accuracy: 0.6848\n",
      "Epoch 827: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7804 - accuracy: 0.6849 - val_loss: 0.7859 - val_accuracy: 0.6922\n",
      "Epoch 828/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7813 - accuracy: 0.6857\n",
      "Epoch 828: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7813 - accuracy: 0.6857 - val_loss: 0.7570 - val_accuracy: 0.6968\n",
      "Epoch 829/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7768 - accuracy: 0.6858\n",
      "Epoch 829: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7768 - accuracy: 0.6858 - val_loss: 0.7592 - val_accuracy: 0.7021\n",
      "Epoch 830/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7787 - accuracy: 0.6845\n",
      "Epoch 830: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7786 - accuracy: 0.6846 - val_loss: 0.7565 - val_accuracy: 0.7013\n",
      "Epoch 831/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.7783 - accuracy: 0.6852\n",
      "Epoch 831: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7782 - accuracy: 0.6852 - val_loss: 0.7637 - val_accuracy: 0.6976\n",
      "Epoch 832/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7783 - accuracy: 0.6847\n",
      "Epoch 832: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7783 - accuracy: 0.6846 - val_loss: 0.7481 - val_accuracy: 0.7024\n",
      "Epoch 833/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7795 - accuracy: 0.6838\n",
      "Epoch 833: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7795 - accuracy: 0.6838 - val_loss: 0.8088 - val_accuracy: 0.6868\n",
      "Epoch 834/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.7804 - accuracy: 0.6850\n",
      "Epoch 834: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7804 - accuracy: 0.6848 - val_loss: 0.7798 - val_accuracy: 0.6921\n",
      "Epoch 835/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.7756 - accuracy: 0.6865\n",
      "Epoch 835: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7756 - accuracy: 0.6866 - val_loss: 0.7628 - val_accuracy: 0.6955\n",
      "Epoch 836/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.7752 - accuracy: 0.6861\n",
      "Epoch 836: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7750 - accuracy: 0.6861 - val_loss: 0.7681 - val_accuracy: 0.6964\n",
      "Epoch 837/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7756 - accuracy: 0.6875\n",
      "Epoch 837: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7757 - accuracy: 0.6875 - val_loss: 0.7441 - val_accuracy: 0.7066\n",
      "Epoch 838/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7786 - accuracy: 0.6854\n",
      "Epoch 838: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7786 - accuracy: 0.6854 - val_loss: 0.7779 - val_accuracy: 0.6968\n",
      "Epoch 839/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7759 - accuracy: 0.6858\n",
      "Epoch 839: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7759 - accuracy: 0.6858 - val_loss: 0.7707 - val_accuracy: 0.6937\n",
      "Epoch 840/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7790 - accuracy: 0.6839\n",
      "Epoch 840: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7790 - accuracy: 0.6839 - val_loss: 0.8116 - val_accuracy: 0.6887\n",
      "Epoch 841/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.7758 - accuracy: 0.6868\n",
      "Epoch 841: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7763 - accuracy: 0.6867 - val_loss: 0.7605 - val_accuracy: 0.6945\n",
      "Epoch 842/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.7749 - accuracy: 0.6883\n",
      "Epoch 842: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7747 - accuracy: 0.6884 - val_loss: 0.7341 - val_accuracy: 0.7079\n",
      "Epoch 843/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7728 - accuracy: 0.6885\n",
      "Epoch 843: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7728 - accuracy: 0.6885 - val_loss: 0.7525 - val_accuracy: 0.7003\n",
      "Epoch 844/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.7744 - accuracy: 0.6880\n",
      "Epoch 844: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7748 - accuracy: 0.6879 - val_loss: 0.7620 - val_accuracy: 0.7006\n",
      "Epoch 845/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7782 - accuracy: 0.6870\n",
      "Epoch 845: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7781 - accuracy: 0.6870 - val_loss: 0.7644 - val_accuracy: 0.6941\n",
      "Epoch 846/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.7758 - accuracy: 0.6871\n",
      "Epoch 846: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7757 - accuracy: 0.6871 - val_loss: 0.7517 - val_accuracy: 0.6979\n",
      "Epoch 847/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7718 - accuracy: 0.6877\n",
      "Epoch 847: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7718 - accuracy: 0.6877 - val_loss: 0.7467 - val_accuracy: 0.7043\n",
      "Epoch 848/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.7665 - accuracy: 0.6908\n",
      "Epoch 848: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7663 - accuracy: 0.6908 - val_loss: 0.7300 - val_accuracy: 0.7107\n",
      "Epoch 849/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.7719 - accuracy: 0.6889\n",
      "Epoch 849: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7715 - accuracy: 0.6890 - val_loss: 0.7594 - val_accuracy: 0.7024\n",
      "Epoch 850/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7746 - accuracy: 0.6872\n",
      "Epoch 850: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7746 - accuracy: 0.6872 - val_loss: 0.7685 - val_accuracy: 0.6970\n",
      "Epoch 851/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.7699 - accuracy: 0.6890\n",
      "Epoch 851: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7704 - accuracy: 0.6888 - val_loss: 0.7808 - val_accuracy: 0.6992\n",
      "Epoch 852/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7695 - accuracy: 0.6895\n",
      "Epoch 852: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7695 - accuracy: 0.6895 - val_loss: 0.7224 - val_accuracy: 0.7166\n",
      "Epoch 853/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7702 - accuracy: 0.6884\n",
      "Epoch 853: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7701 - accuracy: 0.6885 - val_loss: 0.7704 - val_accuracy: 0.6998\n",
      "Epoch 854/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.7724 - accuracy: 0.6876\n",
      "Epoch 854: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7725 - accuracy: 0.6876 - val_loss: 0.7637 - val_accuracy: 0.6975\n",
      "Epoch 855/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7685 - accuracy: 0.6905\n",
      "Epoch 855: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7685 - accuracy: 0.6905 - val_loss: 0.7565 - val_accuracy: 0.7034\n",
      "Epoch 856/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7705 - accuracy: 0.6889\n",
      "Epoch 856: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7705 - accuracy: 0.6889 - val_loss: 0.7170 - val_accuracy: 0.7147\n",
      "Epoch 857/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7705 - accuracy: 0.6886\n",
      "Epoch 857: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7705 - accuracy: 0.6886 - val_loss: 0.7495 - val_accuracy: 0.7082\n",
      "Epoch 858/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7756 - accuracy: 0.6879\n",
      "Epoch 858: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7756 - accuracy: 0.6879 - val_loss: 0.7254 - val_accuracy: 0.7130\n",
      "Epoch 859/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7700 - accuracy: 0.6874\n",
      "Epoch 859: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7699 - accuracy: 0.6874 - val_loss: 0.7431 - val_accuracy: 0.7083\n",
      "Epoch 860/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7700 - accuracy: 0.6876\n",
      "Epoch 860: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7700 - accuracy: 0.6876 - val_loss: 0.7910 - val_accuracy: 0.6931\n",
      "Epoch 861/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7774 - accuracy: 0.6861\n",
      "Epoch 861: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7774 - accuracy: 0.6861 - val_loss: 0.7580 - val_accuracy: 0.7007\n",
      "Epoch 862/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7708 - accuracy: 0.6888\n",
      "Epoch 862: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7708 - accuracy: 0.6888 - val_loss: 0.7516 - val_accuracy: 0.7020\n",
      "Epoch 863/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.7694 - accuracy: 0.6904\n",
      "Epoch 863: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7694 - accuracy: 0.6904 - val_loss: 0.7772 - val_accuracy: 0.6954\n",
      "Epoch 864/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7701 - accuracy: 0.6889\n",
      "Epoch 864: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7701 - accuracy: 0.6889 - val_loss: 0.7813 - val_accuracy: 0.6961\n",
      "Epoch 865/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.7689 - accuracy: 0.6899\n",
      "Epoch 865: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7690 - accuracy: 0.6898 - val_loss: 0.7858 - val_accuracy: 0.6909\n",
      "Epoch 866/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7813 - accuracy: 0.6864\n",
      "Epoch 866: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7811 - accuracy: 0.6865 - val_loss: 0.7147 - val_accuracy: 0.7141\n",
      "Epoch 867/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7708 - accuracy: 0.6894\n",
      "Epoch 867: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7708 - accuracy: 0.6894 - val_loss: 0.7772 - val_accuracy: 0.6968\n",
      "Epoch 868/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.7740 - accuracy: 0.6893\n",
      "Epoch 868: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7739 - accuracy: 0.6893 - val_loss: 0.7786 - val_accuracy: 0.6875\n",
      "Epoch 869/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.7636 - accuracy: 0.6918\n",
      "Epoch 869: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7636 - accuracy: 0.6919 - val_loss: 0.7433 - val_accuracy: 0.7010\n",
      "Epoch 870/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7637 - accuracy: 0.6929\n",
      "Epoch 870: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7637 - accuracy: 0.6929 - val_loss: 0.7604 - val_accuracy: 0.7001\n",
      "Epoch 871/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.7707 - accuracy: 0.6913\n",
      "Epoch 871: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7711 - accuracy: 0.6912 - val_loss: 0.7803 - val_accuracy: 0.6954\n",
      "Epoch 872/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7659 - accuracy: 0.6904\n",
      "Epoch 872: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7659 - accuracy: 0.6904 - val_loss: 0.7704 - val_accuracy: 0.6951\n",
      "Epoch 873/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.7609 - accuracy: 0.6932\n",
      "Epoch 873: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7609 - accuracy: 0.6932 - val_loss: 0.7641 - val_accuracy: 0.6986\n",
      "Epoch 874/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7661 - accuracy: 0.6924\n",
      "Epoch 874: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7661 - accuracy: 0.6924 - val_loss: 0.7270 - val_accuracy: 0.7102\n",
      "Epoch 875/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.7667 - accuracy: 0.6914\n",
      "Epoch 875: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7667 - accuracy: 0.6914 - val_loss: 0.7610 - val_accuracy: 0.7020\n",
      "Epoch 876/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7684 - accuracy: 0.6911\n",
      "Epoch 876: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7683 - accuracy: 0.6912 - val_loss: 0.7425 - val_accuracy: 0.7027\n",
      "Epoch 877/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7706 - accuracy: 0.6906\n",
      "Epoch 877: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7704 - accuracy: 0.6906 - val_loss: 0.7593 - val_accuracy: 0.6980\n",
      "Epoch 878/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7697 - accuracy: 0.6893\n",
      "Epoch 878: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7697 - accuracy: 0.6893 - val_loss: 0.7363 - val_accuracy: 0.6981\n",
      "Epoch 879/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.7699 - accuracy: 0.6909\n",
      "Epoch 879: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7699 - accuracy: 0.6909 - val_loss: 0.7226 - val_accuracy: 0.7148\n",
      "Epoch 880/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7660 - accuracy: 0.6905\n",
      "Epoch 880: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7660 - accuracy: 0.6905 - val_loss: 0.7526 - val_accuracy: 0.7053\n",
      "Epoch 881/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7607 - accuracy: 0.6939\n",
      "Epoch 881: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7607 - accuracy: 0.6939 - val_loss: 0.7393 - val_accuracy: 0.7106\n",
      "Epoch 882/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7611 - accuracy: 0.6933\n",
      "Epoch 882: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7611 - accuracy: 0.6933 - val_loss: 0.7558 - val_accuracy: 0.7128\n",
      "Epoch 883/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.7690 - accuracy: 0.6906\n",
      "Epoch 883: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7688 - accuracy: 0.6905 - val_loss: 0.7576 - val_accuracy: 0.7015\n",
      "Epoch 884/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7648 - accuracy: 0.6930\n",
      "Epoch 884: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7648 - accuracy: 0.6930 - val_loss: 0.7571 - val_accuracy: 0.7007\n",
      "Epoch 885/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7647 - accuracy: 0.6941\n",
      "Epoch 885: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7647 - accuracy: 0.6941 - val_loss: 0.7528 - val_accuracy: 0.7078\n",
      "Epoch 886/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7610 - accuracy: 0.6945\n",
      "Epoch 886: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7610 - accuracy: 0.6945 - val_loss: 0.7342 - val_accuracy: 0.7129\n",
      "Epoch 887/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7667 - accuracy: 0.6919\n",
      "Epoch 887: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7668 - accuracy: 0.6919 - val_loss: 0.7497 - val_accuracy: 0.7079\n",
      "Epoch 888/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.7704 - accuracy: 0.6906\n",
      "Epoch 888: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7704 - accuracy: 0.6905 - val_loss: 0.7429 - val_accuracy: 0.7108\n",
      "Epoch 889/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.7655 - accuracy: 0.6937\n",
      "Epoch 889: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7654 - accuracy: 0.6937 - val_loss: 0.7311 - val_accuracy: 0.7132\n",
      "Epoch 890/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7602 - accuracy: 0.6946\n",
      "Epoch 890: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7602 - accuracy: 0.6946 - val_loss: 0.7551 - val_accuracy: 0.7037\n",
      "Epoch 891/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7651 - accuracy: 0.6925\n",
      "Epoch 891: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7652 - accuracy: 0.6925 - val_loss: 0.7465 - val_accuracy: 0.7010\n",
      "Epoch 892/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7571 - accuracy: 0.6962\n",
      "Epoch 892: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7571 - accuracy: 0.6962 - val_loss: 0.7575 - val_accuracy: 0.7072\n",
      "Epoch 893/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7604 - accuracy: 0.6943\n",
      "Epoch 893: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7604 - accuracy: 0.6943 - val_loss: 0.7569 - val_accuracy: 0.7033\n",
      "Epoch 894/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.7634 - accuracy: 0.6954\n",
      "Epoch 894: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7633 - accuracy: 0.6955 - val_loss: 0.6928 - val_accuracy: 0.7237\n",
      "Epoch 895/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7611 - accuracy: 0.6936\n",
      "Epoch 895: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7612 - accuracy: 0.6936 - val_loss: 0.7239 - val_accuracy: 0.7058\n",
      "Epoch 896/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7635 - accuracy: 0.6921\n",
      "Epoch 896: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7635 - accuracy: 0.6921 - val_loss: 0.7504 - val_accuracy: 0.7049\n",
      "Epoch 897/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.7627 - accuracy: 0.6915\n",
      "Epoch 897: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7622 - accuracy: 0.6918 - val_loss: 0.7718 - val_accuracy: 0.6889\n",
      "Epoch 898/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.7651 - accuracy: 0.6925\n",
      "Epoch 898: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7650 - accuracy: 0.6925 - val_loss: 0.7548 - val_accuracy: 0.7022\n",
      "Epoch 899/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.7603 - accuracy: 0.6938\n",
      "Epoch 899: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7602 - accuracy: 0.6939 - val_loss: 0.8048 - val_accuracy: 0.6945\n",
      "Epoch 900/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.7656 - accuracy: 0.6939\n",
      "Epoch 900: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7655 - accuracy: 0.6938 - val_loss: 0.7341 - val_accuracy: 0.7151\n",
      "Epoch 901/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7619 - accuracy: 0.6950\n",
      "Epoch 901: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7619 - accuracy: 0.6950 - val_loss: 0.7409 - val_accuracy: 0.7076\n",
      "Epoch 902/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7626 - accuracy: 0.6947\n",
      "Epoch 902: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7627 - accuracy: 0.6947 - val_loss: 0.7545 - val_accuracy: 0.7048\n",
      "Epoch 903/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7585 - accuracy: 0.6956\n",
      "Epoch 903: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7585 - accuracy: 0.6956 - val_loss: 0.7507 - val_accuracy: 0.7106\n",
      "Epoch 904/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7566 - accuracy: 0.6949\n",
      "Epoch 904: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7566 - accuracy: 0.6949 - val_loss: 0.7107 - val_accuracy: 0.7242\n",
      "Epoch 905/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.7654 - accuracy: 0.6928\n",
      "Epoch 905: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7653 - accuracy: 0.6928 - val_loss: 0.8554 - val_accuracy: 0.6845\n",
      "Epoch 906/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.7689 - accuracy: 0.6893\n",
      "Epoch 906: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7690 - accuracy: 0.6893 - val_loss: 0.7281 - val_accuracy: 0.7115\n",
      "Epoch 907/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7628 - accuracy: 0.6922\n",
      "Epoch 907: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7628 - accuracy: 0.6922 - val_loss: 0.7177 - val_accuracy: 0.7156\n",
      "Epoch 908/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7580 - accuracy: 0.6967\n",
      "Epoch 908: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7579 - accuracy: 0.6968 - val_loss: 0.7096 - val_accuracy: 0.7140\n",
      "Epoch 909/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7567 - accuracy: 0.6963\n",
      "Epoch 909: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7567 - accuracy: 0.6964 - val_loss: 0.7347 - val_accuracy: 0.7112\n",
      "Epoch 910/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7583 - accuracy: 0.6950\n",
      "Epoch 910: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7583 - accuracy: 0.6949 - val_loss: 0.7300 - val_accuracy: 0.7137\n",
      "Epoch 911/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7601 - accuracy: 0.6962\n",
      "Epoch 911: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7602 - accuracy: 0.6962 - val_loss: 0.7288 - val_accuracy: 0.7141\n",
      "Epoch 912/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7580 - accuracy: 0.6963\n",
      "Epoch 912: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7581 - accuracy: 0.6963 - val_loss: 0.7281 - val_accuracy: 0.7188\n",
      "Epoch 913/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7561 - accuracy: 0.6966\n",
      "Epoch 913: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7561 - accuracy: 0.6965 - val_loss: 0.7247 - val_accuracy: 0.7204\n",
      "Epoch 914/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.7587 - accuracy: 0.6950\n",
      "Epoch 914: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7585 - accuracy: 0.6951 - val_loss: 0.7301 - val_accuracy: 0.7151\n",
      "Epoch 915/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7543 - accuracy: 0.6970\n",
      "Epoch 915: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7543 - accuracy: 0.6970 - val_loss: 0.7951 - val_accuracy: 0.6880\n",
      "Epoch 916/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.7646 - accuracy: 0.6936\n",
      "Epoch 916: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7644 - accuracy: 0.6938 - val_loss: 0.7566 - val_accuracy: 0.7065\n",
      "Epoch 917/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7593 - accuracy: 0.6964\n",
      "Epoch 917: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7593 - accuracy: 0.6964 - val_loss: 0.7531 - val_accuracy: 0.7032\n",
      "Epoch 918/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.7692 - accuracy: 0.6936\n",
      "Epoch 918: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7690 - accuracy: 0.6937 - val_loss: 0.7257 - val_accuracy: 0.7083\n",
      "Epoch 919/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7545 - accuracy: 0.6966\n",
      "Epoch 919: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7545 - accuracy: 0.6966 - val_loss: 0.7659 - val_accuracy: 0.7024\n",
      "Epoch 920/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.7572 - accuracy: 0.6960\n",
      "Epoch 920: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7574 - accuracy: 0.6960 - val_loss: 0.8013 - val_accuracy: 0.6934\n",
      "Epoch 921/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.7543 - accuracy: 0.6987\n",
      "Epoch 921: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7546 - accuracy: 0.6986 - val_loss: 0.7439 - val_accuracy: 0.7115\n",
      "Epoch 922/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7572 - accuracy: 0.6959\n",
      "Epoch 922: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7571 - accuracy: 0.6959 - val_loss: 0.7358 - val_accuracy: 0.7161\n",
      "Epoch 923/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.7549 - accuracy: 0.6962\n",
      "Epoch 923: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7550 - accuracy: 0.6961 - val_loss: 0.7497 - val_accuracy: 0.7074\n",
      "Epoch 924/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7518 - accuracy: 0.6990\n",
      "Epoch 924: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7518 - accuracy: 0.6990 - val_loss: 0.7234 - val_accuracy: 0.7167\n",
      "Epoch 925/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.7725 - accuracy: 0.6920\n",
      "Epoch 925: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7719 - accuracy: 0.6923 - val_loss: 0.7721 - val_accuracy: 0.6962\n",
      "Epoch 926/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7580 - accuracy: 0.6966\n",
      "Epoch 926: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7579 - accuracy: 0.6967 - val_loss: 0.7232 - val_accuracy: 0.7141\n",
      "Epoch 927/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7554 - accuracy: 0.6983\n",
      "Epoch 927: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7552 - accuracy: 0.6985 - val_loss: 0.7408 - val_accuracy: 0.7134\n",
      "Epoch 928/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.7509 - accuracy: 0.6987\n",
      "Epoch 928: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7506 - accuracy: 0.6989 - val_loss: 0.7045 - val_accuracy: 0.7260\n",
      "Epoch 929/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7612 - accuracy: 0.6960\n",
      "Epoch 929: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7612 - accuracy: 0.6960 - val_loss: 0.7041 - val_accuracy: 0.7188\n",
      "Epoch 930/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7541 - accuracy: 0.6995\n",
      "Epoch 930: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7543 - accuracy: 0.6995 - val_loss: 0.7349 - val_accuracy: 0.7110\n",
      "Epoch 931/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.7526 - accuracy: 0.6994\n",
      "Epoch 931: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7527 - accuracy: 0.6993 - val_loss: 0.7221 - val_accuracy: 0.7202\n",
      "Epoch 932/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.7582 - accuracy: 0.6957\n",
      "Epoch 932: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7584 - accuracy: 0.6957 - val_loss: 0.7370 - val_accuracy: 0.7143\n",
      "Epoch 933/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7549 - accuracy: 0.6977\n",
      "Epoch 933: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7549 - accuracy: 0.6977 - val_loss: 0.7537 - val_accuracy: 0.7015\n",
      "Epoch 934/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7542 - accuracy: 0.6979\n",
      "Epoch 934: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7542 - accuracy: 0.6979 - val_loss: 0.7502 - val_accuracy: 0.7105\n",
      "Epoch 935/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.7541 - accuracy: 0.6983\n",
      "Epoch 935: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7539 - accuracy: 0.6984 - val_loss: 0.7980 - val_accuracy: 0.6933\n",
      "Epoch 936/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.7522 - accuracy: 0.6998\n",
      "Epoch 936: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7523 - accuracy: 0.6997 - val_loss: 0.7394 - val_accuracy: 0.7122\n",
      "Epoch 937/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7501 - accuracy: 0.6993\n",
      "Epoch 937: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7501 - accuracy: 0.6993 - val_loss: 0.7201 - val_accuracy: 0.7182\n",
      "Epoch 938/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7523 - accuracy: 0.6985\n",
      "Epoch 938: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7523 - accuracy: 0.6985 - val_loss: 0.7238 - val_accuracy: 0.7147\n",
      "Epoch 939/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7517 - accuracy: 0.6990\n",
      "Epoch 939: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7517 - accuracy: 0.6991 - val_loss: 0.7297 - val_accuracy: 0.7138\n",
      "Epoch 940/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7493 - accuracy: 0.7005\n",
      "Epoch 940: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7493 - accuracy: 0.7004 - val_loss: 0.7754 - val_accuracy: 0.6950\n",
      "Epoch 941/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.7551 - accuracy: 0.6979\n",
      "Epoch 941: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7549 - accuracy: 0.6980 - val_loss: 0.7207 - val_accuracy: 0.7160\n",
      "Epoch 942/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7497 - accuracy: 0.7012\n",
      "Epoch 942: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7497 - accuracy: 0.7012 - val_loss: 0.7304 - val_accuracy: 0.7162\n",
      "Epoch 943/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7501 - accuracy: 0.6983\n",
      "Epoch 943: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7501 - accuracy: 0.6983 - val_loss: 0.7590 - val_accuracy: 0.7088\n",
      "Epoch 944/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.7493 - accuracy: 0.7013\n",
      "Epoch 944: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7497 - accuracy: 0.7011 - val_loss: 0.7376 - val_accuracy: 0.7176\n",
      "Epoch 945/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.7509 - accuracy: 0.6997\n",
      "Epoch 945: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7507 - accuracy: 0.6999 - val_loss: 0.7401 - val_accuracy: 0.7143\n",
      "Epoch 946/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.7490 - accuracy: 0.7001\n",
      "Epoch 946: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7492 - accuracy: 0.7000 - val_loss: 0.7179 - val_accuracy: 0.7223\n",
      "Epoch 947/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.7502 - accuracy: 0.7012\n",
      "Epoch 947: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7500 - accuracy: 0.7012 - val_loss: 0.7263 - val_accuracy: 0.7211\n",
      "Epoch 948/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7507 - accuracy: 0.7008\n",
      "Epoch 948: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7507 - accuracy: 0.7008 - val_loss: 0.7093 - val_accuracy: 0.7219\n",
      "Epoch 949/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7541 - accuracy: 0.6997\n",
      "Epoch 949: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7540 - accuracy: 0.6997 - val_loss: 0.7075 - val_accuracy: 0.7234\n",
      "Epoch 950/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7470 - accuracy: 0.7015\n",
      "Epoch 950: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7470 - accuracy: 0.7015 - val_loss: 0.7166 - val_accuracy: 0.7202\n",
      "Epoch 951/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7448 - accuracy: 0.7033\n",
      "Epoch 951: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7448 - accuracy: 0.7033 - val_loss: 0.7476 - val_accuracy: 0.7105\n",
      "Epoch 952/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.7457 - accuracy: 0.7022\n",
      "Epoch 952: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7459 - accuracy: 0.7021 - val_loss: 0.7231 - val_accuracy: 0.7195\n",
      "Epoch 953/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7510 - accuracy: 0.7009\n",
      "Epoch 953: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7509 - accuracy: 0.7010 - val_loss: 0.7486 - val_accuracy: 0.7127\n",
      "Epoch 954/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.7439 - accuracy: 0.7022\n",
      "Epoch 954: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7442 - accuracy: 0.7022 - val_loss: 0.7111 - val_accuracy: 0.7220\n",
      "Epoch 955/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.7523 - accuracy: 0.6993\n",
      "Epoch 955: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7523 - accuracy: 0.6993 - val_loss: 0.7316 - val_accuracy: 0.7180\n",
      "Epoch 956/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.7480 - accuracy: 0.7006\n",
      "Epoch 956: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7476 - accuracy: 0.7007 - val_loss: 0.7351 - val_accuracy: 0.7066\n",
      "Epoch 957/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.7481 - accuracy: 0.7026\n",
      "Epoch 957: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7481 - accuracy: 0.7026 - val_loss: 0.7236 - val_accuracy: 0.7177\n",
      "Epoch 958/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7511 - accuracy: 0.7021\n",
      "Epoch 958: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7511 - accuracy: 0.7021 - val_loss: 0.7492 - val_accuracy: 0.7100\n",
      "Epoch 959/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7434 - accuracy: 0.7045\n",
      "Epoch 959: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7434 - accuracy: 0.7045 - val_loss: 0.7223 - val_accuracy: 0.7136\n",
      "Epoch 960/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.7504 - accuracy: 0.6999\n",
      "Epoch 960: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7502 - accuracy: 0.7000 - val_loss: 0.7405 - val_accuracy: 0.7087\n",
      "Epoch 961/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.7480 - accuracy: 0.7025\n",
      "Epoch 961: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7483 - accuracy: 0.7025 - val_loss: 0.7212 - val_accuracy: 0.7153\n",
      "Epoch 962/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.7446 - accuracy: 0.7034\n",
      "Epoch 962: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7445 - accuracy: 0.7034 - val_loss: 0.7160 - val_accuracy: 0.7168\n",
      "Epoch 963/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.7534 - accuracy: 0.6984\n",
      "Epoch 963: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7534 - accuracy: 0.6984 - val_loss: 0.7459 - val_accuracy: 0.7126\n",
      "Epoch 964/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.7441 - accuracy: 0.7033\n",
      "Epoch 964: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7440 - accuracy: 0.7032 - val_loss: 0.7308 - val_accuracy: 0.7160\n",
      "Epoch 965/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.7503 - accuracy: 0.7001\n",
      "Epoch 965: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7505 - accuracy: 0.7001 - val_loss: 0.7238 - val_accuracy: 0.7228\n",
      "Epoch 966/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.7495 - accuracy: 0.7009\n",
      "Epoch 966: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7496 - accuracy: 0.7008 - val_loss: 0.7208 - val_accuracy: 0.7220\n",
      "Epoch 967/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7473 - accuracy: 0.7020\n",
      "Epoch 967: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7473 - accuracy: 0.7020 - val_loss: 0.7437 - val_accuracy: 0.7070\n",
      "Epoch 968/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.7495 - accuracy: 0.7024\n",
      "Epoch 968: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7495 - accuracy: 0.7024 - val_loss: 0.7156 - val_accuracy: 0.7205\n",
      "Epoch 969/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.7501 - accuracy: 0.7015\n",
      "Epoch 969: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7501 - accuracy: 0.7015 - val_loss: 0.7070 - val_accuracy: 0.7283\n",
      "Epoch 970/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.7459 - accuracy: 0.7036\n",
      "Epoch 970: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7462 - accuracy: 0.7034 - val_loss: 0.7351 - val_accuracy: 0.7144\n",
      "Epoch 971/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7522 - accuracy: 0.7015\n",
      "Epoch 971: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7522 - accuracy: 0.7015 - val_loss: 0.7420 - val_accuracy: 0.7082\n",
      "Epoch 972/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.7426 - accuracy: 0.7030\n",
      "Epoch 972: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7426 - accuracy: 0.7031 - val_loss: 0.7382 - val_accuracy: 0.7070\n",
      "Epoch 973/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.7436 - accuracy: 0.7034\n",
      "Epoch 973: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7432 - accuracy: 0.7035 - val_loss: 0.7664 - val_accuracy: 0.7095\n",
      "Epoch 974/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.7468 - accuracy: 0.7023\n",
      "Epoch 974: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7473 - accuracy: 0.7021 - val_loss: 0.8270 - val_accuracy: 0.6821\n",
      "Epoch 975/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.7456 - accuracy: 0.7024\n",
      "Epoch 975: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7457 - accuracy: 0.7024 - val_loss: 0.7329 - val_accuracy: 0.7179\n",
      "Epoch 976/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7426 - accuracy: 0.7039\n",
      "Epoch 976: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7425 - accuracy: 0.7040 - val_loss: 0.7281 - val_accuracy: 0.7098\n",
      "Epoch 977/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.7458 - accuracy: 0.7021\n",
      "Epoch 977: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7457 - accuracy: 0.7021 - val_loss: 0.7139 - val_accuracy: 0.7242\n",
      "Epoch 978/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7919 - accuracy: 0.6883\n",
      "Epoch 978: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7918 - accuracy: 0.6883 - val_loss: 0.7309 - val_accuracy: 0.7075\n",
      "Epoch 979/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.7484 - accuracy: 0.7016\n",
      "Epoch 979: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7486 - accuracy: 0.7016 - val_loss: 0.7276 - val_accuracy: 0.7190\n",
      "Epoch 980/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7417 - accuracy: 0.7035\n",
      "Epoch 980: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7417 - accuracy: 0.7035 - val_loss: 0.7454 - val_accuracy: 0.7175\n",
      "Epoch 981/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.7454 - accuracy: 0.7042\n",
      "Epoch 981: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7453 - accuracy: 0.7042 - val_loss: 0.7274 - val_accuracy: 0.7138\n",
      "Epoch 982/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.7431 - accuracy: 0.7035\n",
      "Epoch 982: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7436 - accuracy: 0.7033 - val_loss: 0.7124 - val_accuracy: 0.7269\n",
      "Epoch 983/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.7429 - accuracy: 0.7053\n",
      "Epoch 983: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7430 - accuracy: 0.7053 - val_loss: 0.7169 - val_accuracy: 0.7192\n",
      "Epoch 984/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7438 - accuracy: 0.7026\n",
      "Epoch 984: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7439 - accuracy: 0.7025 - val_loss: 0.7273 - val_accuracy: 0.7191\n",
      "Epoch 985/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7424 - accuracy: 0.7036\n",
      "Epoch 985: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7424 - accuracy: 0.7036 - val_loss: 0.7199 - val_accuracy: 0.7206\n",
      "Epoch 986/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7442 - accuracy: 0.7040\n",
      "Epoch 986: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7442 - accuracy: 0.7040 - val_loss: 0.7240 - val_accuracy: 0.7141\n",
      "Epoch 987/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7481 - accuracy: 0.7024\n",
      "Epoch 987: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7480 - accuracy: 0.7025 - val_loss: 0.7270 - val_accuracy: 0.7142\n",
      "Epoch 988/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.7415 - accuracy: 0.7048\n",
      "Epoch 988: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7420 - accuracy: 0.7045 - val_loss: 0.7349 - val_accuracy: 0.7135\n",
      "Epoch 989/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7412 - accuracy: 0.7042\n",
      "Epoch 989: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7411 - accuracy: 0.7043 - val_loss: 0.7242 - val_accuracy: 0.7200\n",
      "Epoch 990/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.7412 - accuracy: 0.7047\n",
      "Epoch 990: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7416 - accuracy: 0.7047 - val_loss: 0.7232 - val_accuracy: 0.7227\n",
      "Epoch 991/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7420 - accuracy: 0.7051\n",
      "Epoch 991: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7420 - accuracy: 0.7050 - val_loss: 0.6983 - val_accuracy: 0.7333\n",
      "Epoch 992/1000\n",
      "760/762 [============================>.] - ETA: 0s - loss: 0.7388 - accuracy: 0.7063\n",
      "Epoch 992: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7388 - accuracy: 0.7063 - val_loss: 0.7308 - val_accuracy: 0.7141\n",
      "Epoch 993/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7432 - accuracy: 0.7039\n",
      "Epoch 993: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7433 - accuracy: 0.7038 - val_loss: 0.7443 - val_accuracy: 0.7206\n",
      "Epoch 994/1000\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.7402 - accuracy: 0.7050\n",
      "Epoch 994: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7402 - accuracy: 0.7050 - val_loss: 0.7176 - val_accuracy: 0.7269\n",
      "Epoch 995/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.7417 - accuracy: 0.7045\n",
      "Epoch 995: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7416 - accuracy: 0.7045 - val_loss: 0.7351 - val_accuracy: 0.7177\n",
      "Epoch 996/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7641 - accuracy: 0.7013\n",
      "Epoch 996: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7641 - accuracy: 0.7013 - val_loss: 0.7538 - val_accuracy: 0.7061\n",
      "Epoch 997/1000\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.7548 - accuracy: 0.7009\n",
      "Epoch 997: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7547 - accuracy: 0.7009 - val_loss: 0.7060 - val_accuracy: 0.7316\n",
      "Epoch 998/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.7431 - accuracy: 0.7065\n",
      "Epoch 998: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7429 - accuracy: 0.7066 - val_loss: 0.7232 - val_accuracy: 0.7198\n",
      "Epoch 999/1000\n",
      "759/762 [============================>.] - ETA: 0s - loss: 0.7371 - accuracy: 0.7089\n",
      "Epoch 999: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7370 - accuracy: 0.7089 - val_loss: 0.7248 - val_accuracy: 0.7217\n",
      "Epoch 1000/1000\n",
      "758/762 [============================>.] - ETA: 0s - loss: 0.7358 - accuracy: 0.7069\n",
      "Epoch 1000: val_loss did not improve from 0.46189\n",
      "762/762 [==============================] - 9s 12ms/step - loss: 0.7360 - accuracy: 0.7069 - val_loss: 0.7224 - val_accuracy: 0.7237\n",
      "model saved locally\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, CSVLogger\n",
    "\n",
    "current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "model_path = \"/home/jupyter/trained_models_srikar/\"\n",
    "model_name = f'model_{current_time}.h5'\n",
    "\n",
    "# Load the existing model\n",
    "existing_model_path = \"/home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5\"\n",
    "existing_model = tf.keras.models.load_model(existing_model_path)\n",
    "\n",
    "# Define a callback to save the best model based on validation loss\n",
    "checkpoint = ModelCheckpoint(model_path + \"best_epoch_\" + model_name, \n",
    "                             monitor='val_loss', \n",
    "                             verbose=1, \n",
    "                             save_best_only=True, \n",
    "                             mode='min')\n",
    "\n",
    "# TensorBoard callback to log training metrics\n",
    "log_dir = \"/home/jupyter/logs/\" + current_time\n",
    "tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# CSVLogger callback to save training logs in CSV format\n",
    "csv_logger = CSVLogger(model_path + \"training_logs.csv\", separator=',', append=False)\n",
    "\n",
    "callbacks = [checkpoint, tensorboard, csv_logger]\n",
    "\n",
    "# Assuming `model` is your new model\n",
    "# Set the weights of the new model to be the same as the existing model\n",
    "model.set_weights(existing_model.get_weights())\n",
    "\n",
    "model_history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=1000,\n",
    "    batch_size=32,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "model.save(model_path + model_name)\n",
    "print(\"model saved locally\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8b1665a-75be-4bca-b2c8-e8ff53fc14ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(model_history.history['loss'])\n",
    "plt.plot(model_history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88342fa4-7b74-4464-b8cc-4256d3789e87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHHCAYAAABDUnkqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/OQEPoAAAACXBIWXMAAA9hAAAPYQGoP6dpAACOF0lEQVR4nO3dd3hTZfsH8O9J2qZ7byiUvSnILCBbhoiAKAooiIALfMX94h4/X9x7ojIcyJKhICAb2bPsVegCOoDu3Sbn98eTk5O0KbS0SUr5fq6rV9ZJ+uRQmrv3fT/PI8myLIOIiIiojtA4egBERERENYnBDREREdUpDG6IiIioTmFwQ0RERHUKgxsiIiKqUxjcEBERUZ3C4IaIiIjqFAY3REREVKcwuCEiIqI6hcENEdV6kiThzTffrPLz4uPjIUkS5s2bd83jtmzZAkmSsGXLlhsaHxHVLgxuiKhS5s2bB0mSIEkStm/fXu5xWZYREREBSZJw1113OWCEREQCgxsiqhJXV1csWLCg3P1bt27FhQsXoNPpHDAqIiIVgxsiqpI777wTS5YsQWlpqcX9CxYsQKdOnRAaGuqgkRERCQxuiKhKxo4di6tXr2L9+vWm+4qLi7F06VKMGzfO6nPy8vLw3HPPISIiAjqdDi1atMBHH30EWZYtjisqKsIzzzyDoKAgeHl54e6778aFCxesvubFixfxyCOPICQkBDqdDm3atMGcOXNq7o0CWLJkCTp16gQ3NzcEBgbiwQcfxMWLFy2OSUlJwaRJk1C/fn3odDqEhYVhxIgRiI+PNx2zf/9+DB48GIGBgXBzc0OjRo3wyCOP1OhYiUjl5OgBENHNJTIyEtHR0fj9998xdOhQAMCaNWuQlZWFBx54AF988YXF8bIs4+6778bmzZsxefJkdOjQAevWrcMLL7yAixcv4tNPPzUdO2XKFPz6668YN24cevTogU2bNmHYsGHlxpCamoru3btDkiRMnz4dQUFBWLNmDSZPnozs7GzMmDGj2u9z3rx5mDRpErp06YJZs2YhNTUVn3/+OXbs2IFDhw7B19cXADB69GgcP34cTz31FCIjI5GWlob169cjMTHRdHvQoEEICgrCf//7X/j6+iI+Ph7Lli2r9hiJqAIyEVElzJ07VwYg79u3T/7qq69kLy8vOT8/X5ZlWb7vvvvkfv36ybIsyw0bNpSHDRtmet6KFStkAPL//d//WbzevffeK0uSJMfGxsqyLMsxMTEyAPnJJ5+0OG7cuHEyAPmNN94w3Td58mQ5LCxMvnLlisWxDzzwgOzj42MaV1xcnAxAnjt37jXf2+bNm2UA8ubNm2VZluXi4mI5ODhYbtu2rVxQUGA6btWqVTIA+fXXX5dlWZYzMjJkAPKHH35Y4WsvX77cdN6IyD5YliKiKhszZgwKCgqwatUq5OTkYNWqVRWWpP7++29otVr85z//sbj/ueeegyzLWLNmjek4AOWOK5uFkWUZf/zxB4YPHw5ZlnHlyhXT1+DBg5GVlYWDBw9W6/3t378faWlpePLJJ+Hq6mq6f9iwYWjZsiVWr14NAHBzc4OLiwu2bNmCjIwMq6+lZHhWrVqFkpKSao2LiCqHwQ0RVVlQUBAGDhyIBQsWYNmyZdDr9bj33nutHpuQkIDw8HB4eXlZ3N+qVSvT48qlRqNBkyZNLI5r0aKFxe3Lly8jMzMTs2fPRlBQkMXXpEmTAABpaWnVen/KmMp+bwBo2bKl6XGdTof3338fa9asQUhICHr37o0PPvgAKSkppuP79OmD0aNH46233kJgYCBGjBiBuXPnoqioqFpjJKKKseeGiG7IuHHjMHXqVKSkpGDo0KGmDIWtGQwGAMCDDz6IiRMnWj2mffv2dhkLIDJLw4cPx4oVK7Bu3Tq89tprmDVrFjZt2oSOHTtCkiQsXboUu3fvxl9//YV169bhkUcewccff4zdu3fD09PTbmMlulUwc0NEN2TUqFHQaDTYvXt3hSUpAGjYsCEuXbqEnJwci/tPnTplely5NBgMOHfunMVxp0+ftritzKTS6/UYOHCg1a/g4OBqvTdlTGW/t3Kf8riiSZMmeO655/DPP//g2LFjKC4uxscff2xxTPfu3fHuu+9i//79+O2333D8+HEsXLiwWuMkIusY3BDRDfH09MS3336LN998E8OHD6/wuDvvvBN6vR5fffWVxf2ffvopJEkyzbhSLsvOtvrss88sbmu1WowePRp//PEHjh07Vu77Xb58+UbejoXOnTsjODgY3333nUX5aM2aNTh58qRpBld+fj4KCwstntukSRN4eXmZnpeRkVFuynuHDh0AgKUpIhthWYqIblhFZSFzw4cPR79+/fDKK68gPj4eUVFR+Oeff7By5UrMmDHD1GPToUMHjB07Ft988w2ysrLQo0cPbNy4EbGxseVe87333sPmzZvRrVs3TJ06Fa1bt0Z6ejoOHjyIDRs2ID09vVrvy9nZGe+//z4mTZqEPn36YOzYsaap4JGRkXjmmWcAAGfOnMGAAQMwZswYtG7dGk5OTli+fDlSU1PxwAMPAADmz5+Pb775BqNGjUKTJk2Qk5ODH374Ad7e3rjzzjurNU4iso7BDRHZlEajwZ9//onXX38dixYtwty5cxEZGYkPP/wQzz33nMWxc+bMQVBQEH777TesWLEC/fv3x+rVqxEREWFxXEhICPbu3Yu3334by5YtwzfffIOAgAC0adMG77//fo2M++GHH4a7uzvee+89vPTSS/Dw8MCoUaPw/vvvm/qLIiIiMHbsWGzcuBG//PILnJyc0LJlSyxevBijR48GIBqK9+7di4ULFyI1NRU+Pj7o2rUrfvvtNzRq1KhGxkpEliS5bL6UiIiI6CbGnhsiIiKqUxjcEBERUZ3C4IaIiIjqFAY3REREVKcwuCEiIqI6hcENERER1Sm33Do3BoMBly5dgpeXFyRJcvRwiIiIqBJkWUZOTg7Cw8Oh0Vw7N3PLBTeXLl0qtyAYERER3RySkpJQv379ax5zywU3Xl5eAMTJ8fb2dvBoiIiIqDKys7MRERFh+hy/llsuuFFKUd7e3gxuiIiIbjKVaSlhQzERERHVKQxuiIiIqE5hcENERER1yi3Xc1NZer0eJSUljh4G1QBnZ2dotVpHD4OIiOyEwU0ZsiwjJSUFmZmZjh4K1SBfX1+EhoZybSMiolsAg5sylMAmODgY7u7u/DC8ycmyjPz8fKSlpQEAwsLCHDwiIiKyNQY3ZvR6vSmwCQgIcPRwqIa4ubkBANLS0hAcHMwSFRFRHceGYjNKj427u7uDR0I1Tfk3ZR8VEVHdx+DGCpai6h7+mxIR3ToY3BAREVGdwuCGKhQZGYnPPvvM0cMgIiKqEgY3dYAkSdf8evPNN2/odfft24dHH320ZgdLRERkY5wtZSMGgwxJsk+vR3Jysun6okWL8Prrr+P06dOm+zw9PU3XZVmGXq+Hk9P1/+mDgoJqdqBERER2wMyNDZToDTiRnI3E9Hy7fL/Q0FDTl4+PDyRJMt0+deoUvLy8sGbNGnTq1Ak6nQ7bt2/HuXPnMGLECISEhMDT0xNdunTBhg0bLF63bFlKkiT8+OOPGDVqFNzd3dGsWTP8+eefdnmPRERElcXg5jpkWUZ+cWmVvi5lFiC/uBSp2YVVfq75lyzLNfY+/vvf/+K9997DyZMn0b59e+Tm5uLOO+/Exo0bcejQIQwZMgTDhw9HYmLiNV/nrbfewpgxY3DkyBHceeedGD9+PNLT02tsnERERNXFstR1FJTo0fr1dQ753ifeHgx3l5r5J3r77bdxxx13mG77+/sjKirKdPudd97B8uXL8eeff2L69OkVvs7DDz+MsWPHAgD+97//4YsvvsDevXsxZMiQGhknERFRdTFzc4vo3Lmzxe3c3Fw8//zzaNWqFXx9feHp6YmTJ09eN3PTvn1703UPDw94e3ubtjYgIiKqDZi5uQ43Zy1OvD24Ss+Jv5KP3CKxEm7bej7V+t41xcPDw+L2888/j/Xr1+Ojjz5C06ZN4ebmhnvvvRfFxcXXfB1nZ2eL25IkwWAw1Ng4iYiIqovBzXVIklSp0lCpwYCCYj1kGXB2kuBqEIFJTZWVatqOHTvw8MMPY9SoUQBEJic+Pt6xgyIiIqoBLEvVkIJiPeKu5CE5qxB6fc01AttKs2bNsGzZMsTExODw4cMYN24cMzBERFQnMLipITonkakpLjWgxCy4qckZTzXpk08+gZ+fH3r06IHhw4dj8ODBuO222xw9LCIiomqT5Nr66Wsj2dnZ8PHxQVZWFry9vS0eKywsRFxcHBo1agRXV9cqva4syzhxKRv6MqezbT0faLhpo8NV59+WiIgc71qf32Uxc1NDJEmCzkoD8K0VOhIRETkeg5sa5OqkgQTLaEYGoxsiIiJ7YnBTU4pyEFZ0Ho2kFMv7GdsQERHZlUODm2+//Rbt27eHt7c3vL29ER0djTVr1lzzOUuWLEHLli3h6uqKdu3a4e+//7bTaK9D4wStXAI3FFnczdiGiIjIvhwa3NSvXx/vvfceDhw4gP3796N///4YMWIEjh8/bvX4nTt3YuzYsZg8eTIOHTqEkSNHYuTIkTh27JidR26FkytkaKCVZOhQYrr7FuvXJiIicrhaN1vK398fH374ISZPnlzusfvvvx95eXlYtWqV6b7u3bujQ4cO+O677yr1+raaLQUA8uUzkErykGgIQiY8AQAtQrysNhqTfXG2FBHRze2mnC2l1+uxcOFC5OXlITo62uoxu3btwsCBAy3uGzx4MHbt2lXh6xYVFSE7O9viy1YkZzcAgJukbmFQqyJHIiKiW4DDg5ujR4/C09MTOp0Ojz/+OJYvX47WrVtbPTYlJQUhISEW94WEhCAlJcXq8QAwa9Ys+Pj4mL4iIiJqdPwWXNwBAB4aBjdERESO4vDgpkWLFoiJicGePXvwxBNPYOLEiThx4kSNvf7MmTORlZVl+kpKSqqx1y7HmLlxRzGctcZTW7uqfkRERHWew4MbFxcXNG3aFJ06dcKsWbMQFRWFzz//3OqxoaGhSE1NtbgvNTUVoaGhFb6+TqczzcZSvmzGyRWABMh6uBibim+W2KZv376YMWOG6XZkZCQ+++yzaz5HkiSsWLGi2t+7pl6HiIgIqAXBTVkGgwFFRUVWH4uOjsbGjRst7lu/fn2FPTp2J2lM2RtlSrg9Ypvhw4djyJAhVh/7999/IUkSjhw5UqXX3LdvHx599NGaGJ7Jm2++iQ4dOpS7Pzk5GUOHDq3R70VERLcuJ0d+85kzZ2Lo0KFo0KABcnJysGDBAmzZsgXr1q0DAEyYMAH16tXDrFmzAABPP/00+vTpg48//hjDhg3DwoULsX//fsyePduRb8OSsxtQkg9XFCNIKoF7eiIQ0ARw8bDZt5w8eTJGjx6NCxcuoH79+haPzZ07F507d0b79u2r9JpBQUE1OcRrulbmjYiIqKocmrlJS0vDhAkT0KJFCwwYMAD79u3DunXrcMcddwAAEhMTkZycbDq+R48eWLBgAWbPno2oqCgsXboUK1asQNu2bR31FspzFk3FOrkIfsiBJOuBvMs2/ZZ33XUXgoKCMG/ePIv7c3NzsWTJEowcORJjx45FvXr14O7ujnbt2uH333+/5muWLUudPXsWvXv3hqurK1q3bo3169eXe85LL72E5s2bw93dHY0bN8Zrr72GkhJRnps3bx7eeustHD58GJIkQZIk03jLlqWOHj2K/v37w83NDQEBAXj00UeRm5trevzhhx/GyJEj8dFHHyEsLAwBAQGYNm2a6XsREdGtzaGZm59++umaj2/ZsqXcfffddx/uu+8+G43IClkGSvKrcDyAkgK4oRAapShVUgS4+QOaKq534+wOVGJHcScnJ0yYMAHz5s3DK6+8Asn4nCVLlkCv1+PBBx/EkiVL8NJLL8Hb2xurV6/GQw89hCZNmqBr167XfX2DwYB77rkHISEh2LNnD7Kysiz6cxReXl6YN28ewsPDcfToUUydOhVeXl548cUXcf/99+PYsWNYu3YtNmzYAADw8fEp9xp5eXkYPHgwoqOjsW/fPqSlpWHKlCmYPn26RfC2efNmhIWFYfPmzYiNjcX999+PDh06YOrUqdd9P0REVLc5NLi5KZTkA/8Lr/LTaiQl9vKlSpezHnnkEXz44YfYunUr+vbtC0CUpEaPHo2GDRvi+eefNx371FNPYd26dVi8eHGlgpsNGzbg1KlTWLduHcLDxbn43//+V65P5tVXXzVdj4yMxPPPP4+FCxfixRdfhJubGzw9PeHk5HTNMtSCBQtQWFiIn3/+GR4e4r1/9dVXGD58ON5//33TUgB+fn746quvoNVq0bJlSwwbNgwbN25kcENERLWvoZhuTMuWLdGjRw/MmTMHABAbG4t///0XkydPhl6vxzvvvIN27drB398fnp6eWLduHRITEyv12idPnkRERIQpsAFgtYl70aJF6NmzJ0JDQ+Hp6YlXX3210t/D/HtFRUWZAhsA6NmzJwwGA06fPm26r02bNtBq1UxYWFgY0tLSqvS9iIiobmLm5nqc3UUGpSqyEoH8DABAoS4ArkVXAUkLhFaxN8jYv1NZkydPxlNPPYWvv/4ac+fORZMmTdCnTx+8//77+Pzzz/HZZ5+hXbt28PDwwIwZM1BcXHz9F62kXbt2Yfz48XjrrbcwePBg+Pj4YOHChfj4449r7HuYc3Z2trgtSRIMBoNNvhcREd1cGNxcjyRVfaaTeyBQUggAKHYPgavB2LNTyR6aGzVmzBg8/fTTWLBgAX7++Wc88cQTkCQJO3bswIgRI/Dggw8CED00Z86cqXAl6LJatWqFpKQkJCcnIywsDACwe/dui2N27tyJhg0b4pVXXjHdl5CQYHGMi4sL9Hr9db/XvHnzkJeXZ8re7NixAxqNBi1atKjUeImI6NbGspQtOIsP5WLZCaUas00aDdf+YK8uT09P3H///Zg5cyaSk5Px8MMPAwCaNWuG9evXY+fOnTh58iQee+yxcoshXsvAgQPRvHlzTJw4EYcPH8a///5rEcQo3yMxMRELFy7EuXPn8MUXX2D58uUWx0RGRiIuLg4xMTG4cuWK1fWMxo8fD1dXV0ycOBHHjh3D5s2b8dRTT+Ghhx4qt/UGERGRNQxubMHFHWlO4YiXQyBDEov7AYBcavNvPXnyZGRkZGDw4MGmHplXX30Vt912GwYPHoy+ffsiNDQUI0eOrPRrajQaLF++HAUFBejatSumTJmCd9991+KYu+++G8888wymT5+ODh06YOfOnXjttdcsjhk9ejSGDBmCfv36ISgoyOp0dHd3d6xbtw7p6eno0qUL7r33XgwYMABfffVV1U8GERHdkiRZvlk2CKgZ19oyvbCwEHFxcWjUqBFcXV0reIXKSbiah6yCEoT7uiEwLxbQFwOBzW26mB9VrCb/bYmIyP6u9fldFjM3NqKsNSPLUNe3Mdg+c0NERHSrY3BjI0rbsAwZkIx92zbuuSEiIiIGNzajTIpi5oaIiMi+GNzYiJq5AaBh5oaIiMheGNxYURM91pJ56oaZG4e7xfrmiYhuaQxuzCir3ubnV2GjzAowc1O7KP+mZVc2JiKiuocrFJvRarXw9fU17VHk7u6uZmCqqKSkCHJpMUqKgEJnPVAqA0VFQGFhTQ6ZrkOWZeTn5yMtLQ2+vr4W+1EREVHdxOCmDGXH6upuwphdUILswlLk67TIc9YDeZcBbRaQyf2PHMHX1/eau5ETEVHdweCmDEmSEBYWhuDgYJSUlNzw6/yyKx7zdl7CsHbheLZtCfDPc4B3PWDCyhocLVWGs7MzMzZERLcQBjcV0Gq11fpALJGccDFHj4wiGa7e/kBuElCSDXB1XCIiIptiQ7GNOGlEr06pQQbc/MSdRVmAnjOmiIiIbInBjY04acSpLdEbAFdf9YHCTIeMh4iI6FbB4MZGnLXGzI1eBrROgM5HPFCQ4cBRERER1X0MbmzESStObanBODvK3Viayk930IiIiIhuDQxubETpuSnRG1fGVfpumLkhIiKyKQY3NuJcNnPD4IaIiMguGNzYiJN5zw0AuHiKy+JcB42IiIjo1sDgxkaU2VKlBmNwo2yeKXOFYiIiIlticGMj6mwpYzDDzTOJiIjsgsGNjWjLNhRLxsyNgYv4ERER2RKDGxsp11CsZG5kZm6IiIhsicGNjZi2X1AyN8YeHGZuiIiIbIvBjY0oi/iVlM3cGNhQTEREZEsMbmzEuexUcPbcEBER2QWDGxtRN85UylLsuSEiIrIHBjc2YsrcmMpSzNwQERHZA4MbG1F6bvSmspTSUMzMDRERkS0xuLER08aZ5RqKGdwQERHZEoMbGym3t5Rp+wUGN0RERLbE4MZGzPeW+vtoMlJzjb027LkhIiKyKQY3NqI0FAPAk78dxOpjaeIGy1JEREQ2xeDGRpSGYkVmobH3hsENERGRTTG4sRGloVhRqMQ07LkhIiKyKQY3NuJcJnOjB9e5ISIisgcGNzai1UiQzJI3enCdGyIiIntgcGNDzhr19JYyc0NERGQXDG5syMlsxpRBOdUydwUnIiKyJQY3NqQ1ayouNZWlmLkhIiKyJQY3NmTeVGxgzw0REZFdMLixIfPp4KUye26IiIjsgcGNDVnN3HCdGyIiIptyaHAza9YsdOnSBV5eXggODsbIkSNx+vTpaz5n3rx5kCTJ4svV1dVOI66aolK1eVjtuWFDMRERkS05NLjZunUrpk2bht27d2P9+vUoKSnBoEGDkJeXd83neXt7Izk52fSVkJBgpxFXTU5hiem6ng3FREREduHkyG++du1ai9vz5s1DcHAwDhw4gN69e1f4PEmSEBoaauvhVZt55sa0QjHLUkRERDZVq3pusrKyAAD+/v7XPC43NxcNGzZEREQERowYgePHj1d4bFFREbKzsy2+HIGZGyIiIvuoNcGNwWDAjBkz0LNnT7Rt27bC41q0aIE5c+Zg5cqV+PXXX2EwGNCjRw9cuHDB6vGzZs2Cj4+P6SsiIsJWb+GauP0CERGRfdSa4GbatGk4duwYFi5ceM3joqOjMWHCBHTo0AF9+vTBsmXLEBQUhO+//97q8TNnzkRWVpbpKykpyRbDvy4GN0RERPbh0J4bxfTp07Fq1Sps27YN9evXr9JznZ2d0bFjR8TGxlp9XKfTQafT1cQwq4U9N0RERPbh0MyNLMuYPn06li9fjk2bNqFRo0ZVfg29Xo+jR48iLCzMBiOsOXqZPTdERET24NDMzbRp07BgwQKsXLkSXl5eSElJAQD4+PjAzc0NADBhwgTUq1cPs2bNAgC8/fbb6N69O5o2bYrMzEx8+OGHSEhIwJQpUxz2Pq6nSZAH9FdEcCMb9JCuczwRERHdOIdmbr799ltkZWWhb9++CAsLM30tWrTIdExiYiKSk5NNtzMyMjB16lS0atUKd955J7Kzs7Fz5060bt3aEW/hmj5/oANah3njm/GdTD03MjM3RERENiXJsiw7ehD2lJ2dDR8fH2RlZcHb29su31OWZYx89WusdH4Fes9waJ8/aZfvS0REVFdU5fO71syWqsskSYLOxQUAMzdERES2xuDGTlx1zgBEzw0RERHZDoMbO3F1MU5HZ+aGiIjIphjc2ImbTpSluIgfERGRbTG4sRM340KCEhfxIyIisikGN3bi5CSWFGJwQ0REZFsMbuxFy+CGiIjIHhjc2IlGI/aW0sh64NZaWoiIiMiuGNzYi8ZspwvZ4LhxEBER1XEMbuxEo9WqNzhjioiIyGYY3NiJZJ654Vo3RERENsPgxk40WvOyFDM3REREtsLgxk4krbN6g5kbIiIim2FwYyeWPTdsKCYiIrIVBjd2otVqYZAlcYNlKSIiIpthcGMnThoJeuV0syxFRERkMwxu7ERrEdwwc0NERGQrDG7shJkbIiIi+2BwYydajQZ6GJuKuUIxERGRzTC4sRNmboiIiOyDwY2daDUSStlzQ0REZHMMbuzESSvBwMwNERGRzTG4sRORuVF6bpi5ISIishUGN3bipJFgkFmWIiIisjUGN3ai1WjYc0NERGQHDG7sxEnDnhsiIiJ7YHBjJxYrFLPnhoiIyGYY3NiJWOfG2FDMzA0REZHNMLixE8u9pbhCMRERka0wuLETJ635In7M3BAREdkKgxs70Wo0akMxe26IiIhshsGNnTiZL+LHzA0REZHNMLixE63FVHBmboiIiGyFwY2dOGkklHKFYiIiIptjcGMnFpkb9twQERHZDIMbO3HSaKz33BRkAKf+BvQljhkYERFRHcPgxk4s17kxy9xsfAdYOBY4utQxAyMiIqpjGNzYiZPWbIVi87JU6jFxmZlo/0ERERHVQQxu7EQjSdBDEjfMMzcZCeKyKNv+gyIiIqqDGNzYieXeUsbgpqQAyE0R1wuzHDMwIiKiOobBjZ1Y9twYG4ozk9QDmLkhIiKqEQxu7MRqz01mgnpAIYMbIiKimsDgxk60Ggl6uUzmJiNePYCZGyIiohrB4MZOnDQaU1nKoDdmbsyDG2ZuiIiIagSDGzsx77mR9UrPjVlZipkbIiKiGsHgxk7MdwU3mMpS7LkhIiKqaQxu7MR8bymDtcxNaQG3YCAiIqoBDG7sRGRujGUpgx4oyCy/tg2zN0RERNXm0OBm1qxZ6NKlC7y8vBAcHIyRI0fi9OnT133ekiVL0LJlS7i6uqJdu3b4+++/7TDa6jHP3Mj6EiA3TTzg6gM4e4jrRVzIj4iIqLocGtxs3boV06ZNw+7du7F+/XqUlJRg0KBByMvLq/A5O3fuxNixYzF58mQcOnQII0eOxMiRI3Hs2DE7jrzqJEmCQRI9N7JBD+iLxQNaHeDqLa4zc0NERFRtTo785mvXrrW4PW/ePAQHB+PAgQPo3bu31ed8/vnnGDJkCF544QUAwDvvvIP169fjq6++wnfffWfzMVeHbAxuDPpSwGDsr9E6Ay6eQE4yZ0wRERHVgFrVc5OVJcoy/v7+FR6za9cuDBw40OK+wYMHY9euXVaPLyoqQnZ2tsWXo8jmPTdK87DGiZkbIiKiGlRrghuDwYAZM2agZ8+eaNu2bYXHpaSkICQkxOK+kJAQpKSkWD1+1qxZ8PHxMX1FRETU6LirwqAxlqX0pWpwo3UGdEpww54bIiKi6qo1wc20adNw7NgxLFy4sEZfd+bMmcjKyjJ9JSUlXf9JtiKZ7QqulKU0zmrmhmUpIiKianNoz41i+vTpWLVqFbZt24b69etf89jQ0FCkpqZa3JeamorQ0FCrx+t0Ouh0uhoba3UYJCdABmRDKaCsdWORuWFwQ0REVF0OzdzIsozp06dj+fLl2LRpExo1anTd50RHR2Pjxo0W961fvx7R0dG2GmbNkcx6bswbipm5ISIiqjEOzdxMmzYNCxYswMqVK+Hl5WXqm/Hx8YGbmxsAYMKECahXrx5mzZoFAHj66afRp08ffPzxxxg2bBgWLlyI/fv3Y/bs2Q57H5UlS+J0i8yNWVlK5yOus+eGiIio2hyaufn222+RlZWFvn37IiwszPS1aNEi0zGJiYlITk423e7RowcWLFiA2bNnIyoqCkuXLsWKFSuu2YRca2iMp7tc5sYY3DBzQ0REVG0OzdzIsnzdY7Zs2VLuvvvuuw/33XefDUZkW0rmBnqznhtOBSciIqpRtWa21C3BmLmRZfMVis0aipm5ISIiqjYGN3ZkytwYSq1PBWfmhoiIqNoY3NiTxmydG9NUcCdmboiIiGoQgxs7UvaWgmwwayh2AXSe4npRrmMGRkREVIcwuLEjyZS5KTMVXOtivL/EMQMjIiKqQxjc2JGsUXpuzKeCO6nBjb4EqMQMMiIiIqoYgxs7UjI3kmw+FdxZTAcHAMgi8CEiIqIbxuDGnkyZG4PlVHAlcwOo9xMREdENYXBjR6aeG9l8KriTZXDDvhsiIqJqYXBjT8bMjWQxFdxZfCn0DG6IiIiqg8GNPUlKz43eciq4JKklK5aliIiIqoXBjT1plbKU3nIqOGA5Y4qIiIhuGIMbOzLNljLoxVo3gJgKDqhBDoMbIiKiamFwY09Kz43VzI0S3LAsRUREVB0MbuxIo7HWc1OmLMXZUkRERNXC4MaejFkajUXmxliWUspTLEsRERFVyw0FN0lJSbhw4YLp9t69ezFjxgzMnj27xgZWF0laJbgx21uqbOaGZSkiIqJquaHgZty4cdi8eTMAICUlBXfccQf27t2LV155BW+//XaNDrAuMWh1AACtochsET/OliIiIqpJNxTcHDt2DF27dgUALF68GG3btsXOnTvx22+/Yd68eTU5vrrFSQQ3Tobi8pkbDctSRERENeGGgpuSkhLodOKDesOGDbj77rsBAC1btkRycnLNja6OkY3BjQQZKMkXd7IsRUREVKNuKLhp06YNvvvuO/z7779Yv349hgwZAgC4dOkSAgICanSAdYoxuAEAFOWKy7JlKc6WIiIiqpYbCm7ef/99fP/99+jbty/Gjh2LqKgoAMCff/5pKleRFVpX9XqxMbgxZW5YliIiIqoJTjfypL59++LKlSvIzs6Gn5+f6f5HH30U7u7uNTa4usZJq0GR7AydVAIU5Yg7TVPBWZYiIiKqCTeUuSkoKEBRUZEpsElISMBnn32G06dPIzg4uEYHWJdoNRKKYMzUlMvccLYUERFRTbih4GbEiBH4+eefAQCZmZno1q0bPv74Y4wcORLffvttjQ6wLnEyD25kg7jUlJ0txcwNERFRddxQcHPw4EHcfvvtAIClS5ciJCQECQkJ+Pnnn/HFF1/U6ADrEovMjelOZm6IiIhq0g0FN/n5+fDy8gIA/PPPP7jnnnug0WjQvXt3JCQk1OgA6xInjYQiuaLgxnjJ2VJERETVckPBTdOmTbFixQokJSVh3bp1GDRoEAAgLS0N3t7eNTrAukSr1aAILpZ3cldwIiKiGnVDwc3rr7+O559/HpGRkejatSuio6MBiCxOx44da3SAdYkTy1JEREQ2d0NTwe+991706tULycnJpjVuAGDAgAEYNWpUjQ2urrHac6M0EisZHAY3RERE1XJDwQ0AhIaGIjQ01LQ7eP369bmA33VUqueGZSkiIqJquaGylMFgwNtvvw0fHx80bNgQDRs2hK+vL9555x0YDIaaHmOdYT1zw7IUERFRTbqhzM0rr7yCn376Ce+99x569uwJANi+fTvefPNNFBYW4t13363RQdYVThqNlZ4bZYVizpYiIiKqCTcU3MyfPx8//vijaTdwAGjfvj3q1auHJ598ksFNBbQaCQVlZ0spGRuWpYiIiGrEDZWl0tPT0bJly3L3t2zZEunp6dUeVF1lteeGZSkiIqIadUPBTVRUFL766qty93/11Vdo3759tQdVV2m1EorLJsuUjA1nSxEREdWIGypLffDBBxg2bBg2bNhgWuNm165dSEpKwt9//12jA6xLxDo3ZmUpSQtIkrjOshQREVGNuKHMTZ8+fXDmzBmMGjUKmZmZyMzMxD333IPjx4/jl19+qekx1hnlZktpza+zLEVERFQTbnidm/Dw8HKNw4cPH8ZPP/2E2bNnV3tgdZGTRmPZc6OxEuhwthQREVG13FDmhm5M+cyNWWypvUbPTUYCsO4VIO+KbQdIRERUB9xw5oaqrtzeUhprZSkrPTefG5u0SwuBYR/bboBERER1ADM3dlQ+c2PWXFzRbKl8s6n1WRdtNzgiIqI6okqZm3vuueeaj2dmZlZnLHWek1ZCkWwW0FSmLHVihXrdO9xmYyMiIqorqhTc+Pj4XPfxCRMmVGtAdZm/u0vVy1JHl6rXi3NtNzgiIqI6okrBzdy5c201jltCoKcOJZJ55uY6s6Xy04GEnert4jzbDpCIiKgOYM+NHWk0EjzcPczuuE5ZKv8qAFm9zcwNERHRdTG4sTMvT0/1htVF/MzKUmWDmSIGN0RERNfD4MbOfLy91BsaK/035pmbssEMy1JERETX5dDgZtu2bRg+fDjCw8MhSRJWrFhxzeO3bNkCSZLKfaWkpNhnwDXA17uizI2V4KZs5obBDRER0XU5NLjJy8tDVFQUvv766yo97/Tp00hOTjZ9BQcH22iENc/P21u9Ya0sZd5QrAQzHsb3V5xj28ERERHVAQ5doXjo0KEYOnRolZ8XHBwMX1/fmh+QHQT5VlCWsrYreJExmPEKAfLSmLkhIiKqhJuy56ZDhw4ICwvDHXfcgR07dlzz2KKiImRnZ1t8OVKgn696w1rmRjYABr24rgQznqHiUl8MlFrZnoGIiIhMbqrgJiwsDN999x3++OMP/PHHH4iIiEDfvn1x8ODBCp8za9Ys+Pj4mL4iIiLsOOLygv3UzI3B2lRwQO27UXpuvELUx0qYvSEiIrqWm2rjzBYtWqBFixam2z169MC5c+fw6aef4pdffrH6nJkzZ+LZZ5813c7OznZogBPoo/bcFBk0cFNumJeo9MWAs6talnLzE5kdfbGYQeXmZ7fxEhER3WxuquDGmq5du2L79u0VPq7T6aDT6ew4omvTOjmhBE5wRinySyU1uLGauTFmaVw8ARcPoKCYfTdERETXcVOVpayJiYlBWFiYo4dRJSWSCGTyS81Ov0YLSFpx3VCmLOXiCbgYy1kMboiIiK7JoZmb3NxcxMbGmm7HxcUhJiYG/v7+aNCgAWbOnImLFy/i559/BgB89tlnaNSoEdq0aYPCwkL8+OOP2LRpE/755x9HvYUbotfoAH0B8vRlYkutM1CqV2dMKYv46YyZG4DTwYmIiK7DocHN/v370a9fP9NtpTdm4sSJmDdvHpKTk5GYmGh6vLi4GM899xwuXrwId3d3tG/fHhs2bLB4jZtBqXHzzBK5bHDjApQWlm8odjEPbpi5ISIiuhaHBjd9+/aFLMsVPj5v3jyL2y+++CJefPFFG4/K9vQaJbgpc/rLrlLM4IaIiKjKbvqem5uRXiManIuhtXxAU2YhP1NDsQegM/bcFLEsRUREdC0MbhxArxXBjdWyFKBmbqz23DBzQ0REdC0MbhzAYAxuiuUymRulLFVutpQXgxsiIqJKYnDjAAZjhqbIYGW2FCDKUrJsFtx4iL4boPxO4URERGSBwY0DyMbMTVGFDcXFQEmB2GcKMJalGNwQERFVBoMbByh1EVsw5Mhulg+Yem5KLYMYZw+WpYiIiCrppt9+4WZ0pvljWJngirOuvTDB/AHz2VJKcOPsAWg0InsDqE3GREREZBUzNw5Q4tcUX+jvQZahbObGLLgxnykFsCxFRERUSQxuHEDnJE57UanB8gHTbKlSyzVuzC9ZliIiIromBjcOoAY3essHTD03xZarEwNmwQ0zN0RERNfC4MYBdE5ifZuikgoyN/pidSViZWViU1mKmRsiIqJrYXDjADrnCspSpoZia2Up9twQERFVBmdLOUCFZSknsf4NSgsBGDcULVuWKsoFDAYxg4qIiIjK4SekA5jKUmUzNzqx/g2KstXZUkpQ4xkMSFpA1gO5KXYaKRER0c2HmRsHUDI3hSVlMjduvuKyIBPQGR9Tem60zoBvBJARD6THAQk7gdRjgG9DoP0YNQgiIiK6xTG4cQDznhtZliFJknjA1UdcFmYByn1KWQoA/BuL4CZpN7DxHZhKV0U5QM//2GXsREREtR2DGwdQylKyDJToZbg4KcGNr7gszAQkY8VQZxbc+DUSlydWwhTYAEBOsi2HS0REdFNhcOMASlkKEE3FLspt88yNwj1Ave7fWFwmH7Z8Qc6gIiIiMmFDsQNYBjdmTcXmPTf5V8V1i+CmkeULeYYaX4TBDRERkYLBjQNIkmTK1lgEN+aZm/x0cd1a5kbRoLu45MJ+REREJgxuHMRVCW7MZ0yZ99wowY2bv/q4X6TlizSIFpcMboiIiEwY3DiIztnKWjdK5kZfDBQbt19wNwtunN0Ar3Dj/YFqJkc5loiIiBjcOIrVncF1XuosKUBcV7I5CqXvJrStOpOKmRsiIiITBjcOorNWlpIkNXsDiJJU2W0WApqIy9B23EyTiIjICk4Fd5AKt2Bw9QEKMsR185KUIno6AAno+iigLxH3MbghIiIyYXDjIMoqxeW2YDAvQ5nPlFIEtQDu/kJcz0kVl8W5YkVAZVVjIiKiWxjLUg5itecGsCxLWQtuzCn7SckGoKSgBkdHRER082Jw4yAVlqWUhfwAwM3v2i/i7A7AmK1haYqIiAgAgxuHUTM3ZctSVcjcaDRq9obTwYmIiAAwuHEY0zo3JWXLUr7q9esFN4BZcMPMDREREcDgxmFqpOcG4HRwIiKiMhjcOEiFZSnznhtrU8HLMmVuuHkmERERwODGYVytbb8A3EBZypi54c7gREREABjcOIy6QjF7boiIiGoSgxsHUaeCX2O21PWmggPcX4qIiKgMBjcOoqxQXOE6N9Y2zbSGU8GJiIgscPsFB1HKUuW2X/BvDDTuJy7LbpppDWdLERERWWBw4yAVrlCs0QITVlT+hRjcEBERWWBZykEqXOemqjgVnIiIyAKDGwcx9dyULUtVFaeCExERWWBw4yAVlqWqilPBiYiILDC4cZAaK0txKjgREZEFBjcOUuH2C1XFqeBEREQWGNw4SIW7glcVZ0sRERFZYHDjIDU3W4rBDRERkTkGNw7iZszcFBSXVu+F2FBMRERkgYv4OYi7TgQ3+SV6yLIMSZJu7IVMmZtcwGAAMhPEbf9GNTBKIiKim49DMzfbtm3D8OHDER4eDkmSsGLFius+Z8uWLbjtttug0+nQtGlTzJs3z+bjtAVPnYgrZRnIL65GU7GbH+DkKq6nHgVm9wG+6gIc+q0GRklERHTzcWhwk5eXh6ioKHz99deVOj4uLg7Dhg1Dv379EBMTgxkzZmDKlClYt26djUda89yctVCSNXnVKU1pnYDwjuL6rq+BwizAUAKsfBJYOB5IPV79wRIREd1EHFqWGjp0KIYOHVrp47/77js0atQIH3/8MQCgVatW2L59Oz799FMMHjzYVsO0CUmS4OHihNyiUuQV6QGvarxY/S5A4i7g6FLz7wCcWgWc3wK8cA5wdq3miImIiG4ON1VD8a5duzBw4ECL+wYPHoxdu3Y5aETV42Hsu8krqmZTcURXcSkby1vDvwCe3A04uYlenOyL1Xt9IiKim8hNFdykpKQgJCTE4r6QkBBkZ2ejoKDA6nOKioqQnZ1t8VVbeBj7bqod3NTvYnk7shcQ3BLwqSduZ1+q3usTERHdRG6q4OZGzJo1Cz4+PqaviIgIRw/JxMPFGNxUdzq4Vyjg08B4PQzwb6xeB4Cc5Oq9PhER0U3kpgpuQkNDkZqaanFfamoqvL294ebmZvU5M2fORFZWlukrKSnJHkOtFKUslVtUzS0YACDCmL1p2BOmTmVvZm6IiOjWc1OtcxMdHY2///7b4r7169cjOjq6wufodDrodDpbD+2GKNPB86tblgKA7tOAzESgx3T1Pm9mboiI6Nbj0MxNbm4uYmJiEBMTA0BM9Y6JiUFiYiIAkXWZMGGC6fjHH38c58+fx4svvohTp07hm2++weLFi/HMM884YvjV5m4sS+XWRHBTvxMwZYM6LRwAvMLFJRuKiYjoFuLQ4Gb//v3o2LEjOnYUH8jPPvssOnbsiNdffx0AkJycbAp0AKBRo0ZYvXo11q9fj6ioKHz88cf48ccfb7pp4Aq1obgGylLWKJmbbGZuiIjo1uHQslTfvn0hy3KFj1tbfbhv3744dOiQDUdlP57KVPDqNhRXRMncsCxFRES3kJuqobiuUcpS1Z4KXhFTz00KYLBRdoiIiKiWYXDjQJ41tc5NRTyCAUkjFvfLu2yb70FERFTLMLhxIKXnpkamglujdQI8jYsecjo4ERHdIhjcOJCyzk2+rXpuAHUhPwY3RER0i2Bw40Aetu65AQBvK03FJ/8Cjiyp+mvlpwPXaAAnIiKqDRjcOJBalrJDcKOsdRPzO7DoQWDZFCB+u+WxFw4AJ/603nwctw34oBGw7mXbjZWIiKgGMLhxINMKxcU2nMkU0FRcphwDEncDf5qtYPzPa2ompjgf+HkEsPghYO5QIKvMwn8n/hSXu78B0k7abrxERETVxODGgdxNe0vZMHOj7Bh+YS+wfw5gKAWa3gG4eAKXDgInVojHz20CinPE9aQ9wN8vWL6Osl8VAKx/w3bjJSIiqiYGNw5kPhX8WosZVktoO8DJDSjMAk6sFPf1eAro/Ii4fmaduDy1Wlw26S8uz64Dcs2mj+emqdfPrgPSz9tmvERERNXE4MaBlJ4bgwwUlhhs8020zkC928T10kIR6DToDgS3EvflpAD6UuDMWnH79ueA8NtEhueoWdNx2XVyypatiIiIagkGNw7k7qw1XbfZFgyAWpoCgMhegJNOXf8mNxVI2g0UpANufkBEd6DDOPHY4QXq88wzNwCQf8V24yUiIqoGBjcOpNFIcHcRAc5vuxOxI9ZGAUNEN/W6UnbyChWXOSlA/A5xvekdYuG/tqMBjROQchTING5cmmcMbgKbG28zuCEiotqJwY2DKaWpTzecwZO/HbRN701EV/V6k37i0tMY3BSkA1djxXUlcHH3B3wixPWsi0BpkejZAYDg1uJSCW5kWfTtFGTU/LiJiIhuAIMbB/NwUUtTWQUlSM8rtsE3CQSGvA8MeB0Iainuc/cHNM7i+sUD4tK3gfocJbOTm6IGMhondWq50oNzYgWwYAyw0myKORERkQM5OXoAtzolc6O4kFGAAE9dzX+j7o9b3pYk0XeTfQFIPyfu841QH1d6cnJS1JKURxDgGSyuKz03iXvE5Zm1YgVjd/+aHzsREVEVMHPjYIUllgv4XcgosN839wqxvO1jFtwoe1LlpKhTwj0CxRegZnPSjotLQ6nY1oGIiMjBGNw42LnLeRa3L2Tk2++bK303gCg5KQENoAY+ualmmZtgwN0suJFlIPW4+pzjy207XiIiokpgcONgTYM9LW47LHPjHS5mSik8zWZTKdPAPYNFaQoQPTe5aUD+VfU5cds4i4qIiByOwY2DfTWuIx7uEYmXhohGX4dlbnwbWj5mPlVcaR72CFLLUgUZQMoRcT2gqZhpJeuBSzFVG4Ms226ncYMBWP0ccGC+bV6fiIhqJQY3DtYy1Btv3t0Gbet5A3Bg5sa83wawnC1lnrlx8wcgAZCB+H/F/cGt1WnkV89W/vtnJgLvNQTWvXIjo7++1GPAvh+B9a/Z5vWJiKhWYnBTS9T3cwcgghtlrRuDwUYZDYVF5qZMcKPMlirIALKNWy14BIvSlZufuH1+q7gMaaMGN1eqENycWAkUZQG7vwYKMqs8/OtSSmaFWVyHh4joFsLgppYI93WFJAEFJXqk5xXjl13xaPn6Wuw6d/X6T75R5pkb8zVuABHAaI1T0lOOiUtPY7+N0neTHCMuQ9oAgc3E9apkbiR1jR+bzLQyD2gyEmr+9YmIqFZicFNL6Jy0CPFyBQCcTcvFayuPo7jUgJ+223D3bfPMTdmylCSpwU9xjrj0MK5xo/TdKMI6AAHG4KYqmZuCdPX60cWVf16lX988uImv+dcnIqJaicFNLVLfzw0A8NmGM6b73FxsuM6iR5CYAg4Afg3LP24e/LgHAEEtjM8zC27qdRIlrUDjysU5yUCRMRhKPqJetybfLLiJ+xfIvlT193AthZnq9UxmboiIbhUMbmqRhgEeAIDd59UP/cs5hbb7hlonYMh7QO8XAb/I8o+bl63a3Qdojds1uJsFN21Hi0s3P7VcdTVW9ON8f7uYrZQRD3weBez8yvL1zaeRQwaSD1fzDZXBshQR0S2JwU0t8lT/pohuHGBxX2p2kW2/adepQP8KZiu5mK3BE/WAel1vNqbWI9XrptJULHB+i7h+Zh1wZLEIcGIWWL5+fpl+osJscWnQAzu/BC4csD6uo0uB3+6zzPxYYx7cMHNDRHTLYHBTi0QGemDB1G5Y9VQv/Dq5GwAgJavQNjuFV4Z5w29YB/W6EsQAgE899bpSmrp6Vl0DpzATOPSLuJ5+3nJNGyU40Ylp8CgyBjdx24B/XgVWPV1+TCnHgD8mA2f/AU7+KWZZxe+wvlaO+QwsZm6IiG4Z3DizlpEkCW3r+aCgWOw5VVCiR3ZhKXzcnO0/mD4viE01b39ONBgruj0OGErUkpRCCXounxL9NorMRHFZWiB6crzDxW0lc+PXEEg5KqZsAyIIAoDLZ0QWR2MMskqLgeWPqa9bkAGseRE4sggYvxRodofleMyDm8xEsaifpgrxvL4UkDRVew4RETkcf2vXUm4uWlNAk5ptw76ba/GLBB5ZWz5ocHYFer8A+De2vL9eJ3F5dr26H1VZSXuBeXcBh35TZ0v5NRKXSuZGWVdHX6QGRgBweIFYmE+Rd0UEUoD1lZHNy1L6IrFPVkXitwO/3ANcNe6QXloMfN0FmDu04ucQEVGtxOCmFgv1FlPDU7IcFNxUVYPuotm4RNlCQip/zJb3xMrGW94D9MXiPn9jcKP03GRdUI+/eACYPxzY+I7owwHUWVx5V9TVkzPiyn8v89lSwLX7bvb9CJzbCBxfZhxDksggJe0GivMqfh4REdU6DG5qsRAfY3DjqMxNVWm0QAuzTEeT/up1ZUHAyyfFZZYxI+PkpgYrSuYm66L6vO2fiR6cfz8Ss7BcfYHez4vHclPV4CbdynpASuZGWaDwWmvdKNPQ843PMQ+MrpXxISKiWofBTS0W6i0CgpsmcwMArYar1xv1Bup3EX0r7cdYP97dH9B5ietK5ibbLHOTetTy+K5T1WnrV86IzToBIL1M5qakUM0gKc3Q12oqVoIbJSAyL2nlMLghIrqZsKG4FjOVpW6WzA0ANOoDuHiJVY3D2gNRY8Xmmzmp6qwpc+7+gKvZbCmDwTJzo+j2uAhWejylZmmyzY7LTQGK8wEXsUeXKfMiaYDQdmJmVUVlKYNeNDoDah+QeTMyMzdERDcVBje1mFKWSr2ZMjfOrsCIr4BLh4BGfcVMI68QwOWc9ePdA8ymgucAeZfFTCxzkhbo/6qa4VEWCywrIx4IaS1mWeWmiPtcfdVMT0WZm7zLgKFUXLeWuWFwQ0R0U2FwU4vdlJkbAGgzUnyZ820gghRZL7IpskHc7x6gZm4Ks9VmYp2P2DEcAMI7qIENYLlCsrmMOPFa3/VUm5XdfAFf49YSFWVuzDNA+VYyNzkp1p9HRES1EntuarEQY3CTmJ6P2LRr7NF0M9A6AwFNxPWmZlPL3QNEIAOIspTSbxPUAvA0bv/QsKflazm7qtkec+lxQOpxNbABxLYQSuYm+yKgN2aFclLU7Ex2snp8gbWG4gqmtRMRUa3E4KYWiwz0gIeLFjmFpRj82b/YG3ed7QZqu7s+BQa+BXR/XL3PzbznJkdd18anHtAgWlxvcWf51yq7MzkgenGykizvc/MDPIPFrCzZIB7PuwJ81RWYe6dY2dh8w86CDHGfRVmKmRsiopsJg5tazFPnhL+e6oUukX7QG2T8edhKo+3NJLIX0GuG5fYN5j03kNVF+XzqA3d/ATy+A2gYXf61zPtuglqJy4y48s3Irr5idWXTdPAE4MJ+UfJKOyG2iTAvS8l6kUFiQzER0U2LwU0t1zjIEw92Fz0jp5Jv8tKUwrse4CRKbnD3F2UmrYu4nWZcB8e7PuDqA4S2tf4a5sFNA7EPl8jcXLA8zs1PXPoZ+24y4sVWD4oz6ywzN4Dou+FUcCKimxaDm5tAy1CR2TiVkuO4TTRrkkYDBLcW130ixKXSMKwENz71r/0a5mWpyNvFZUaCWOjP4nsZ96Uybyo2XzvnzNrywU1BhmVwk39FTBcnIqKbAmdL3QQaB3nAWSsht6gUFzIKEOHv7ughVd/Ib8V08Yiu4rbOW2ykqSy8p2zJUBHzzE1IG5GhKcgAkmMsj1NKTqbMTYLYWVxx8QDg4mn5nIJ0y4Zi2SCmi3uFVuadVV9xHlBSYL2viIiIrouZm5uAs1aDpsEis3EqpY6UpoJbAh3GqruNu5aZ/eRXheDGMwQIaimuK1PM24wSU867PyluK5mbtJPqIoDKLKriXMtjCjLNMjfG8VmbDl6cD8wZCsy/u2YzO3OGAJ9HWWaPiIio0hjc3CRahRqDm+RsB4/ERsyndnuFqysNV0TJamicRdYmqIX6mKQB7vkBeDEOaNhD3Kdkgi6fBCCLgKj/a5avqZTKsi8Bpca1hZSMT+wG4MpZy+M3vwsk7gTitgLnNls+tv4N4O8XxIrLVVGUK5qci3OBy6er9lwiIgLA4Oam0TJMBDcfrz+DQZ9uxe7zVx08ohrm6qNe9298/eM9gsWlV6jI/iiZG0BsxKl1Fgv4KYLbAPU6qbdD2gLt7gXGLRYzqiK6i5WUATWzI2nVsWx6B/hpEKA3rmR88SCw+xv19Q79rF7PSAB2fAbsnQ3Ebbn+ezFnvrt52eZoIiKqFAY3N4kWoWpm40xqLl5efhSl+ipmBWoz88xNQCWCm4huQLv7gD4vitvmmRtrzcgaDXD3l+ptZWp488HAc6eBh1eLNXcANbhx9VEbngHRi5Nu3Ebi0C+iBKYETKf+FsGILAPx/6rP2ffT9d+LOfPdzcuu2UO3luTD6orZRFQlDG5uEm3CveGkEf0fbs5anL+chyUH1L/s98alY8z3u3DkQqaDRlhN5j03lcncOLkAo38EbpsgbptnbnzqWX9OSBtgyHti+4ZOE9X7nV0BrZM6bVzZYdzNT2zY2WG8emyqsRk5zbgeT7cnxK7jhhLg0zbA7D6ihKU4/Tfw51PAv5+o9+WkivuOLC5ftrIIbipY1+jqOXWl5RtxYT+w4/Oql8zIfi6fAb7vDSye4OiREN2UGNzcJAI9dZjzcBcserQ7nh8sshQf/3MGFzMLkJlfjOkLDmJvXDq+2VzBBpXXkJSejynz9+FQogMbWM0zN/5Nqv58rzD1NbwrCG4AoPsTwIvngPCO5R9zN2ZusoyrJLv5io04R34DdHpY3Jd6XGRnLhunrAc1B3o/L1ZABsRf28eXG18vQGR3Dv4MbHxLLTNt/0Tct2wq8MsIy0DFIrixUpaK3QB8eRuw7hXr72/j28D3fcQ+XRX5+3lg/etA/LaKj6krSouBkptsbzYAuGLstyq7tAERVQqDm5tI7+ZB6NY4AA92b4DmIZ64kluEh37cg8d+OYC0nCIAwNYzl1FYokd6XjHOX87FznNXsDLmIvKLSyt83Xk747HhZBo+WCt+oRaVWp/5Y9MyWFUzN2VJklqaut4aORVRMjfWbge3EZepx8X2DQUZACSx2nKr4cArycCgd9XjtS7AmJ+BsCj1PiVwMW8+jtsGJO4yO+Y6PTepJ8Tlhb3lH5NlYO8PYjq8+WuWlRFvvKxgI9G6QpaBH/qJYLCkwNGjqZq8K+LSfKXsm1VdWJuLbjq1Irj5+uuvERkZCVdXV3Tr1g1791r5xW00b948SJJk8eXq6mrH0TqezkmLeZO6ItzHFeev5GFPXDo0EuDt6oSCEj2mLziILu9uQP+Pt2LcD3vw9MIYvPnn8Qpfb3+CyNjsjU/Hkv1JaPXaWny+wXJm0PazV9D6jXX4Ydt5ay9RA2/KbNfv661xU5HOk4HQdkDzITf2/LLBjauvej1ECW5OqH9V+zVUZ3VJEtD5EXXH8vpdxXYTj20DmgwQ92XEi1LTldMAJKDZYHF/7Eb1+1yv56bA2INhHgQpsi+JrSOAiv/iLylQp5jnJFs/BgAOLwR+HnFz74hemCnKiNkXRUbtZpJvDG5KC4DSIseOpTri/gU+aCRKsOYunwF+GgycXe+YcVGd5/DgZtGiRXj22Wfxxhtv4ODBg4iKisLgwYORllbxTsze3t5ITk42fSUk1PG/QK0I93XDosei8UTfJniibxMseiwaozqKcsyGk2nQG2R46ZzQwLjg3x8HLyL+Sl651yko1uP4xSwAgN4g4+XlR2GQgS82ncWJS+KDUpZlPPjTHhSXGvDu3ydt84aUkpJXGODicWOv0WEs8Pj2Gw+OlIZi022zYCfEOE08KxFIMgbf5n0+gAh0+r0srre7V71fGU9GPHB+i7he7zb1mHPG4KakwHKfq8JMMTXcnPmu5WWbTS+b/dtUFNyYBzRlV2Y2t/wxMdalj1jef2A+8F2vm2Mml/lu7xcPOm4cNyLPbDbkzZy9ObdR/MyeWm15/8mVQNJuMaOQyAYcHtx88sknmDp1KiZNmoTWrVvju+++g7u7O+bMmVPhcyRJQmhoqOkrJCTEjiOuPSL83fHSkJZ4aUhLdIn0x+A26gq6w9qH4cibg7DtxX7o1yIIeoOMLzaeLfcahy9kotSgpo1L9OK6KdAxyNgXr/biGHuaa15IG7E+TQMrm2Tai2+EZYBjPpXczU/sdwUAx5eJS/MZWoouk8X6OkqPDqAuFpgeB5w3lqQa9xNfgNjrKjdNLRfpvAGdcWp8dpmmYvOAJqNM9iatTHCz/TORfSnMFgHZrq8tA5prZW4UCTssSzq7vxHjPbr02s8rKQB+GAAsf9xxZYkcs/d66dB1jk0F/vyPGnzWpKS9VW/gVjI3wM29mGOu8Y/UshnA3Mvisrau5XQpRh0j3ZQcGtwUFxfjwIEDGDhwoOk+jUaDgQMHYteuinsGcnNz0bBhQ0RERGDEiBE4frzikktRURGys7MtvuqqLo380SrMG82CPfG/ke0gGVf/nTGwOQBg2aGLeGZRDHIK1QbWA8aSVONANVtyX6f6cHPWIiYpE8cvZWO2WSnKIAO5RRX379ywoBbAs6eAe9S/5H7bk4Ax3+1CSpadGkJdPIAndwEdHxRr5SjlJIVSmlI23iybuVG4+6srLwPqasvp59UPzyb9AM8gtSfn3Ca1JOXfWO0bKluaMv+gK1uaUmZwASLtv+1D8f1O/iUyMOteBg7/rh6TXUFwI8ticUTF8RXisshsYcGLB6w/V3HxIHBxv/h+1wuEbMX8/V0vuDm2FDg4XwSDe2o4m/DXDNHAfX5T5Z+TZxbcmG8FcrMxBTdlsoR5xvszE8R2I7XJlbNi1uPCsY4eCVWDQ4ObK1euQK/Xl8u8hISEICXFeq2/RYsWmDNnDlauXIlff/0VBoMBPXr0wIUL1tPks2bNgo+Pj+krIiLC6nF1gbNWg7//0wtrZ/SGj7v64RQV4YsXBreARgKWH7qIp34/ZNqAUwluxnVrgKj6Pgjx1uGloS3RrbHIYGw5nYbNpy1LhOblrRK9AYv3JeFiZtUbNvUGGT/visdJZdVlrxCx+B5EKezLjbHYG59uEVzZnFcoMOJr4PnTQMMyWaQm/SxvW8vcWKNkbpIPiz2qnD1ETw6gBlBn1qpBU0ATs+CmzM+1eYmibHBjXpbKTVG3ldg7Ww2SzPt7yn7gKIpzxdR2xaFfxWXKEQDGLMz1goXMRPX6upeBwqxrH38tWRdvrPfHPDN19ey1Z5CZH7v2pYqn4VeVvgS4ckZcv17fj8EglgxI2CX2WVPczGWp3FRxmZNimcEzD96U81NbKBnQC/uv/TNDtZrDy1JVFR0djQkTJqBDhw7o06cPli1bhqCgIHz//fdWj585cyaysrJMX0lJdXthNEmSoLVSO5rWrykWPRYNFycNtpy+jLk74mEwyKbgpkukP/54oge2vtAPgZ46dG4o+k3m7IiD3iAjMsAdtzXwBQDEpuViwZ5EXMoswBcbz+LFP45cs2FZlmVsOZ2GAwnpFjOxFuxJwOsrj+PZxeV/6V/IKEBKtsjYLNmfhDxbZIuqqutjIqsDAJCAwOaVe56yhYMSGET2FOv0AECru8TlmXXA0SXiepMB6lo9ZT9kC8zKUubNxwaDWeamzL+/+Wai5h/i+VetT5POLdPvpjRQmwc0WUmilFMR8+AmLw2I+d36cfrSa5et8q4C3/YAZvcT07qromxPUdlNVct+H4VsKF/yA0Rj7+5vgRXTRCamMmWmjAQ1UEyt+P8IALGa9ca3gNXPWgY3N3PmJs9Y2tEXW5ZUzX/GaltpSgnIIAOXbrJeLTJx6K7ggYGB0Gq1SE21/CWZmpqK0NDK7cDs7OyMjh07IjbWegOlTqeDTqer9ljrgi6R/nh1WCu8vvI43lt7Cu3r+yCroASuzhq0DveGk1YDJ604tnOkyNxk5ItfzL2aBSK/WI+DiZn4YO0pXMoqRD1fN1zNEzM59sWnQ5ZlUynsTGoOpszfj3HdGiDEW4dnFokAJtzHFYsei0a4rxt+3C4+QE4mZyMlqxChPuqsN/PtJXKKSrHs0EU81L0hZFnG9tgriLuSh/HdGloN5GxGowGGfwmEtjf2xXhd/zmAOM4jSP1F39gsAxR+m8jsZMSLPhmtC9BymMi8ACJtb868LGWxVUMSUJInnh/cqvKzg3KSyzdgK39V67zF7Ku8y+KDvWy25tJBoMVQ66+rjNvJTcz4ubAXwOOWxxTnA990F8HfxL+sv87RxeLDvTBTvKeILiIYyr8q1hFSyn/pcSIoCTBbI0kJ5CSNeOzsP0Dk7ZYlQ4V5jwtgPVN0ZDGw9r/q7aZ3AI1utz5uxVWzPrfrBTdKcHo1FhYBakGGWLjRzU9di8kRTq4S57LlnZb3xywQGSrzhTEBEfyZBzE5lwCPAHE9zzy4OYVaJdfs8+jCPqBxX4cNhW6cQzM3Li4u6NSpEzZuVFPlBoMBGzduRHR05RpL9Xo9jh49irCwMFsNs055qHtDNAnyQHGpAV9uEgFhVH1fOGstfxTEfeov2F5Ng9AoQPTlXDL2wFzMLEBhifjrNTO/BPFX803Hz/r7JBLT8/HJP2fw6XrxC95Fq8GlrEJ8uv4M1p9IRYLZ8dvOWDbv7Y0Tf+UFe4nA9J1VJ/Ds4hiM+mYnHvppL15feRwrY2qodFAVGg3Q7TExM6sqlNIUYFnekiSxg7npsQGikVlZV8d8lk9JIVCinjOLzI3yARHQzLIXSNmDqyLWmoqVICywOaDVqccpwY2ySOK1+m6UzE3b0eLywr7yxyQfFkFQ3DZ1VljMAuCT1sD5reL2od/U4xN3issD84APmwBHFonbJQXA7L7A113FdhdHFovXVDI3zQaJy51fAn9MsZ4pUgI6Z+PU/rLZKwBI2Flm/DGWt89vEWUl8x3izUsuV85ee0FBJRDSFwN6s+nfl2KAr7sBC8dbfVqNkmVxnvaXmdCRny5WS1403jJjlxEPrHgC+Os/5TNlBRmAbHYulB4ofYllkF4bMjelxaI36sRKy8D2wn6HDalCyYev3/NGji9LPfvss/jhhx8wf/58nDx5Ek888QTy8vIwadIkAMCECRMwc+ZM0/Fvv/02/vnnH5w/fx4HDx7Egw8+iISEBEyZMsVRb+GmIkkSbm8WBEAs+AcAnSP9yh3n5qJFm3AxY0cjAdFNAhBp1nQMiHV1JEmsngzAtMLxgYQMbD4tXrtYb0Biej48dU6YO6kLAGBFzEW89Zf4K9bP2Bu0tWxwEy+CmzfvboPezYNQXGrAsoMXEZOUaTqmspuHZhWUYOPJVBgM1yh/2JrSVOwVVr4RWQkAAKDtPeIywtiTc/Ws2YJuZWbN5KaqzZjKL+HQdkBAU3Hd2R3oOlVc9wiGRTZAWVHZ2nRwJbjxDAa8w8X1tFPq9HJlJpi1gEWhLBDYZpT4vpmJ5QMGZSsLQHxIpp0UHzDZF4E1L4rALvWoeowSXBycLy5Pr1HHUZgJGEpFSWfZVOCXUep4+84E+r0CaJxE47C1TIGSuVGaxnOtZG6UhROVGX2XYsS443eI2yufEmUlZYVqwDK4kfXXzlKU3XVeEbdVlLYSd6qlnYIMEcDV9Ey081uAf14FVj1jGVinHhPjlw3A2XXiw/XCfuDYMvWYSzGWr5VX5t9b6fHKK5MlM5/l5yix64EDc4H1b5TP3NSmRQiL84G5dwLz7qp9jdi1jMODm/vvvx8fffQRXn/9dXTo0AExMTFYu3atqck4MTERycnqX5cZGRmYOnUqWrVqhTvvvBPZ2dnYuXMnWrdu7ai3cNPp1TTQ4nanhuWDGwDoYgx6OkT4wsfNGY3Mgpu29byxZkZvrHiyJ0Z2EB+AhxIzsTP2Cl5cKkoiSo8OAIztGoGeTQMxqHUIDDKQnFWICH83fHCvmC3079nLKNUb8PE/p9Hh7X+QcDUfGkmUw+ZP6oIfJ3TGY70bY9Y97fC/Ue0AAPvjM3AxswCL9ydBX0HgUqo34OG5ezF5/n68v/YUFu9Pwsivd+Dc5Vyrx1eWLMvIKrDc3ykjrxgZeRX0hQQbA5qmA1BYasDKmIvqsSFtxcKDwW2AFsaUv7u/GgQl7RGXSr+Ne4C6Bo/ygZi0W1w26CZKXYBIp982QTQv93tZ3SwUUGdpWc3cGD98PALVLM2ZteLSJ0KsyAyID8JTf6vPK84HfrkH+Oc1dQp7aFv1fRz+XWwPcWSJ+JA2L9OknxNBiZKxuHwK+P0BcV3pbUrcJXqQlAyS8vz47eLSK0yUTTTOItBRslw+EWKDVWWT0xSzoMr0no2BshLc5KSK97bzS2MZLF0NlroY/5C6dEgEUfOGiUyTsm3HQbMd4q8o5XLJcszWVBTcmP8bKStPr3kJmD9cZLqsST58YytQ//uxen3z/9Tr5uPePweYMwT4aZDlOjVlM1m5ZXqylIyIEvQ4GcvQGfH2XUH6/FaR4Tu6VA1clDJuZqJln1v+Vev9V46Sekw0/Jfk22atqdoUyFWTQ3tuFNOnT8f06dOtPrZlyxaL259++ik+/fRTO4yq7ureJABajWQKCG5rYD24Gd+tIQ4mZmJaP9HHYJ65Gdw6FPV83VDP1w0XMgoAxOGX3Qn4Zbf4hern7owvx92GbzbHYk9cOqbcLrZUeGloS8Sm5aJ9fR+8NaItPHVO8HFzRlZBCdYdT8X3286juNRgGpe3q8jsDGwdgoGtRcCblV+CV1YcxfkreXjopz04fzkPRaUG9GgSgLXHUjCuawOsPpqM3/cmIsTbFYcSMwEA35vNuvp5Zzxua+iHzzecxYf3tcdtDfzwz4lUfLPlHHILSxAV4YsZA5qjQYC71XOzaF8S/rvsKD67vwNGdqyH5KwCDPtiO5w0Era92A+uzlrLJ3R9TGRLoh7Agj2JeHvVCTQO9MDvj3ZHiLcrMG4RTiZn4/nvD2Fav6a4s12Y2Pn88ikgcbfow1EyN27+ok8ldgOKzm2Dc3AbaC4Y09QR3UXPzUPLRW+QRyAwxbgK7Om/1V6Y8I4iIDKfLl1aLAIo5cPHI0gtoygLDYa0Ea/f7Qlgz7fAisfFwom+DcRfv+c2qsdqdSJjVL+zmMm1/nX1e/k3FkGa4tRqMVvMxVMEZLu/ER+O7gHA6J/Eh2lhFrDd7P9++jkRUCnBTd//ik1ON7+rHqfVqX0qIW1FoJh6FMB94r7cNJHhKslTjwFE5mbF4+J71u+qzvYKaKb2YCg7xANivIq4raIHyL+RmrmJ7CV2i1eyVfnpohdL4yR6gXzqW88WlZWwU/wsJBiDnD3fAR3GWfYRZcSLNYa8w4D/HBalVHP56aKBvUE3y61OEneLMWqMHwux68V9DbpbBjfmvVfmgVfZzE3ZdWKULKFyf0BTEfDkXxEZkka9xf15V8svp2CuOB/Y+h7QagRQv5P1Y/QlIlBrOlD8/Cku7Ad+vlu97eYrjkk+Im7LenXWoYunCCQSdt7YljC2YN5Ll32x8jM2K+NSjAjW+/5XlN5vcg7P3JD9eeqc0DHCFwDQLNgTvu4uVo+LDPTAH0/0QP+WIabnNQ32hLNWwp3t1R6nDmYZGgAY27UBNjzbB/V83fDuqHbY8Gwf8QEOoEmQJzY93xefPdARPm7O0GokDI8Sr/X8ksMoLjWgZagXvhl/G74cZ2VzSwA+7s5oHiyaec9fFh9KC/YkYur8/fhw3Wnc9eV2vLriGI5fysamU+KDun19H4vX2HAyDZ+sP4PzV/LwwpIjeOmPI3jslwM4nJSJc5fzsOzgRdzz7Q4cNpbBktLzkZyl/nW51Lgj+x8HL0CWZby49AjS84qRllNkCqYs6DyB6CcBd3+cME59P38lD2Nn70aqcVbY+2tP4filbLyy/Ciy8kuQ4GnMriQaszJKScLNz/QBu3P9H5i/YpX4cNb5iCyJJAFN+ovAxpwxAyK7ByFTZ/z3M58OvniCZb+LR5BallJ6aJRfpne8DdTrLD701xlXZVaep/BtID5YzT9cvOuJYCL9vGVZS8kAhXcE+r8qgrSGvYCpm4Gw9mqZ7sBc9TmyQWQLlNeJvF0sJWDesO0Vqn5ImtYpMgYYp1YDHzUDNrwhbmt16odY8hE1oIn/Vy1J1e9izGiV2b/szDrL2zG/iQ9pJdvWeoS4PLdJvNePmovMVMwCYMEYYE4FjdllJewQ41KyRClHyq++HL9dlLEyE61P2d/4lgjcvugILHlYBAKJe4CF48TjUWOBKGPW7OAv4tJaxslDlLdN2b0KMzfG868EQubBs9IPpfz7n90AfNhYlBcrsu8HsSjiwrEVLzFwdAmwZZYoc5or22ivrDaeYlb+lI2z4JRysVL+VGQmir4v894qezEPIK+1wviNiNsmfl7N18IyJ8vij6HqLOtgRwxublH9W4lG055lSlTXM29SFyx/sieaBHma7gv3cUU9X9HDMTG6If43qi0CPCs/Q+3xPk3gpJFQUCJ+WYzv1gB3tgtDmI9bhc8p2yd0Mjkb543r7yhr7gxrF4aeTQPwRN8mWPxYNMZ1a4Bn72gOV2cNLmYWmBqaz1/Jw+L9F6CRgGn9mmDuw13QJtwbV3KLMXHuXsQkZWLQp9vQ54MtmL8zHlkFJThkDHr2xqXj971J+Pes2kegNENXJM44TmethPNX8vDA7N1YfyIVW4x9Shn5JRj46VY8tF58KMiXDqEwZilkpWfF3R9o1AcA0FU6icwTxsXhIrqU/yvdnLE8FF/ig1c2GseYckz8ko7bBpxZI/5yVaZ+ewSV32E9qJW4dHIB7v4SkLRikcDzW0XGwpwyBV7pUdG6AA8sAFreVX5sxTniMixKLKY4eR0wabX6GsoUfINxSQBlXAfmiQZcrzA1MGnQXe0pUoIzQPQjAeoHtVLSOWKcgu8RKIIhwHLKfcJO9UMwQvSNIbxDmTdgTOeHGL/HkUVq1sYnAmg9UgSll0+JIMJQInZ4XzVDHFNk/MAwXzzR2u72yUeApDK9TmWbf5UyJiD+TctSMl2A6A+aP1yU1vKvAmEdgIFvAu2Mma2z60Two/QKKdmVlncB45eKMuq9c0Q5MDfVMhOoBDHKDDZTcGPW06UshXBqlfjw3Pej+p7MN5g1pywqmZsKbJ5l/Zg44473qccsP4yVEpNkzKwmHxZ/NGSXLfFIam9Z7EaRLVL88yrw19Plzzsg+mC+7i6Cxqoy6EWQ+9uYistD5sFZTa3FpFCWH0g9bn3ZhcUPAZ+0BN5rIALjLe+rAV7eVWD+3cDhRTU7pmpgcHOLmnp7Y3wxtiOeG1TJtVqM6vu5o209yyyIJEn4/qFO+PyBDnjz7jam6eBVec17O4m/hF2dNbi7g5Vf6mV0MU5VlyTRE6R4sHsDRDcOwKSekfhybEf8NqU7XhrSEq7OWvxvVDv8Z0AzU0M1ADQ0KzvNuqcdXhjcEv1aBmPRY9FoEeKFzPwSjJ29GwUlehTrDXjjz+N4/JcDppJeUakB76wSO3U3DxEB35448Uti+9kreGnpEew+fxWyLCM9rxiyLJsWQfxybEfU83VD3JU8TP1ZNAQrfU2Xc4qQKAcjyRAEyVAC1xWTIf39nBiomx+O6SNwVfaCh1SE+/Tir97N+Y3R6/1N6P3BZqTlFOJyThEOJKSbFmxEy2HICu2BL/MGYoe+FXLgLhqWD/+OrL/fLH+SzTM3RvvzzWZfhbSG3FnsPVX0xxPl97PybYD0vGKsTvZGyb3zxXTv8A5qFgNQSyAK813UzbW7F3jkH7HZaKdJ6msoM6Yie6kZGicd0LCHuO5lNosy2NiXl5siyiHKB6ASWLgHiJWpy0rcpfa6RHQ3jrODuHTzEx/sit7PiUUaMxNFeQwQ5UHPILE4JABAVjdl1Zf5EFHGDVhOawdEcCjrgf0/Wb6348stZ2GZBz9Kr5SiIEP9dxr5nRh74i4RbLUaDkz6WwR5DXqIpQDyLgPH/hA9Hk6uwOg5Ivi5+0vxbzn2dxFMBhozelvfF8EuoDaQK/+mSuCTa5a5adxPBKJZSSI4jt2gjvWvp8vPLstIsFx7Zu/35RezlGWzAE62nPGkHNtmpLhMPmx92QSPQJFF9G0gljI4Z7a6tCnzt6r885KPiLLW8eXl9367nox40TR+dp31XriSQsuFOstuzVJdSlO9vtjy+yjMM7Pp54Et/xMb7AKitBq3Fdj5Rc2OqRoY3NyinLUa3B0VDi9X5+sfXAlt6/lgRId6VQ5sFE8PbIao+j54ekBz+Lhdf0z9WgYjKsIXU29vjOcHiV+svu7OeGlIS/z+aHe8MbwNNBWsgTOwlfoB/cHo9ph1TzvMfqgT7u+iNtx66pzw2l3iw7CgRA+NJKbRA8CuMrO0Ckr08NQ54b3R7QEABxMzMPXn/Xjwpz1YtD8JD8zejehZm3DbO+vx1aZYXDU2EvdqFoRFj3VHdGO19+TzBzpgfLcG6NbIH98/1BkzXN/Gr6WW20Csji3ElJ8PYpdBlFnqS+KX0rdxIbiQUYDE9Hz8uisB93+/C6O/3YWZy46iRG8A3P3xjOvbWGbojUx44csSESDIK6fD53L5qaXnC93KBTfT/8kTr6WM13AfUmVf6PKMv2jrdVIzKL4N8drKY5i24CAe3h2KvBBjearpABEAACIoMWf8IIxNy0Gi2VIBAESPyPjFwPDPUBTYyvKxDmWmSXcwllgie6r36TzVWWv756o7qCs8gkRWTFPm568kX/zCbxCtbqDa9h7ApwHQ579q0ASIPqkWxl3p4/8Vl7dNEJcthwF3vCOybo9uFoGSpAEGvqU+v0k/NeBTZr0plIDutLGE0/5+wCtclCSVQK0g0+yDSRLlFvO/8JUpxP6NxXIGw78QfUTDPgbG/KJuWuvkIv6dAGDbR+IyqKUI0no9U369HSWTdWAusOhBMQOqbHCTf0Wsl2SeuXFxV7/PX0+LICuwhQgyMxPEB725EyvFZeTt4lzLBhGcndsEbP1ALAqZEW+5bYl5JkvZv63VcHF+cpLNAiqz3xeexnKmkmVcO1NkVHJS1L61+O3lSzTK6wNV36zVfMzWyoCpx9XMJXDjZamsC8Dv48rvo2a+kGXZ/qnCLPX/y3OngR5PievKnmlKoHX1XNX2ULMhBjdUK4T5uGHl9F54om+T6x8MwMfNGSun9cTLd7ZCr2aB+HJsR/w6uVulgrU7WociwMMFURG+6NrIH2O7NsCgNuX/Yu/VLBADWopA6P4uDfD2iDamlZsB4C6zvqMxnSPQMcIX/h4uKCwxYP2JVDhrJfRsKgIXZbVlZSuJIC8dPHVOqO/njgVTu+H3qd2xYEo3tK/vi3dHtcOix6IxuE0oFr70AHpN+x6lZr3/JzOdkJJdiE0G0ZNUImvxXelw7JVbmnaG/2bLOVOZbuG+JLy6/BhOpYgeJI0EvH5Xa8zXD0aSHAwJMvJkHV4tmYRCWT1/C44VWpRGkgxBSCnUYvf5q9h8Og3PLo7BZzuu4OmS6dDLxg+GRn2Avi8DIW2R3/QubDgh+i52xF7FpLn7RLO4sxtyGg0GAFypf4ephCQ7uyPHoyEuZhbgri+3Y/R3O1GqL/+L8vut53DPH2aBSeuR5bbGOOp3B54IW4Sh25ui/8dbMODjLVh3PEXM3gJEM3RZHoHiA81T3Q5GVsoXAHD78+r1gCbAM0eB7o+LPhxAZH68wizXLfKLBJrdod7u+R9g4p8iuHhoOfDUQaDXDPFhDQANeqDIU2wRU+pn9n/BzR/oPNlyvKHt1EUUD8wDvu8DfGIM+vwbq31KK6eJD6vUE4DSeK7MHLvtIeCp/WIGWNk/TJSZe8r6O0qztTVRY0VwqGw8u+8nNbgJagW4GBe8PDDfLHNj/COj40PiUgkMoh4A2hvLYkoJSqEEN21Gquf94kFg+RMiU3Z0iWXZDVCDG1lWMzch7YDAZuqYAMtNez2NY1MCyqxEEWht+0gNMAyllpkmwHLRzWstlWAucY9xzSez4Ma8B8hgEFO/5xj7k0zLONxg5mbvD8Dp1cDiiZZlRPOFLMv2aikBspufKN32fkFk9q6cFqVPJdAqLah4Wxc7Y3BDdcLwqPBy5bKK+Hu44N+X+mHRo92vm2n69IEO+PDe9nhjeGtIkoS3RrSBRgLcXbR4flALaDUSNBLwcI9ISJKErsZymUYCZk/ojN+mdMdf03vhw3tFVifHuI2EsiAiIMp60U0C0MNK/5OzVoPIsCAYwtVZIeFh4Zj9UCdMeOwlZN49DwOLP8R7pWPRt0UwZt3TDj5uzqad3pXgbOnBC6by2dC2YZjUMxKtIoIxtvhl/Kd4GroXfY1zDR/ASUlkXQyyhN+P5iJL4wvZmEk4I4vS4Zt/Hsekufuw7KD4hRfafiDeKn0YJw0R+CG3B9Zpb8cvHRdg9QUdikoNCPbSwcvVCXvj0/HemlPIKSzBmKTR+E/xdNy+MRJXXUR2KKYkAnd+tRM/bDuPwhIDLucU4Uyq5ZT9/fHpeH/tKcTK9XBZ9kEO3HG55xsWxyRnFWDSvH1YE6fHydQ8nL+ch3OX8/Dsohhc9TX2xBj/4pbNSkpZkvHnx1PN7K0pFef9nFNTNcNQlnGV4ji3Npg4dx9W5LaC7GLsSesyBTvjMvDp+jMoKNbj6IUs/L43UZQ1XdzV1aHv/xWYuhmnXVpjRVZTlMha/JraQO0N8Y0Q5R8l8wSIYKPlMHH99GrR0KtMf6/fVazv4+QmdqKf3Qf4NlqUEgDRDH4dctOBKNaKn1PZyVUt5VjTuA/wQqzovwFEuULJRHiFiBk4ALBuphgPoDYktxgCjPlZlOp0PqLfp7UxQDyzTp0mXpgFWSlJtbhTLQ2eWKnONDswV82YNTdm0C7sF70heZdFlkvSiHKTklFS+r2UbB+g9l416C7+bZQMjhJcKY4ts8xUZFwnuJFlUdJRSsV5V0TP0/y7LRflNM/cZMSJ96QEVUrwfKPBjXJ+CjOBP6erYzHf8qNsc7jyvZRGelcfoIsx2D4wzzKLVLY87SC1Yio4kb25u1TuR9/b1Rn3dVY3W20T7oOlT/SAs0aDyEAP/DSxMyRJMk0Zv69zfew8dwUvDGmJfi3Eh2S7+j5oV98Hn204a2p2jgy0PsW8Ii7N+gKXxF+gD/SJgkbJNDUYhYYxe5GekIFX7mwFV2ctRnWsh3k74+HhosUnYzrgyQUHsCP2KnbEil9ej/dpAkmS8GTfJnj0l0xckINxe7NA/DK5G/TrBgG7TiNb4428IhlLDl7Eg24hcM27iFhZZHHOGWeoDWsXhjFdItC7WSDe83HF0K13ALuLgd0iO+BkLAuO7lQftzXww9Sf92POjjhsOJmKxAwJ55x6objUgIM5vrhDCxwpbYCk9ALM2xlvet9HLmSidbg3ACAlqxD/+f0QDDJwe6v6eCLtMyRczUeDP1NwV3sJBlksQfD15lhcyS1Cy1AvvDqsNZy1Ej7dcAa7z6dj0onbsLztGEjHliJP1uGAJgp9ZTEb7fv9WWhQLxH3uAdDmT/4vmECjsiNsbqoGxZlFSLc10qTe5t7sO54Ct445IUUXMbWM5eRGjIVU5olQR81AU99shdX84qx5cxlnEzORnGpAfnFesiyjI0n0/DluI7YeDIbc7bn4lLWTuQUPYx3MQaFB5zxkJc3tIUZoilZkiBHPQBpyywYNC7QBDQVGRplmwwnVzETrSBdNP426QdM2SA+wDISLJuk618/uDmarsF/8t9GILJwVdcaX7p3RUW5m+JSA06lZKNtZG8xLrMPuGWxMgKCx6Bnq/1wOqkucPjJzgz0d80UPXOtR4jVuYvzRDDkU1+U/bISgbPrgdZ3Q07YBUk24LwhFEkpzugTbpxNab5YYNIetcem+xOiGbwoWzQWK/073vVF2S20nbqfW/sHRJnvz6cAyBYBLloNF/1Op1ap38snQgRvp1YB3/cGHvxDjNuiLLVfTHt3chHBAAD8/YKY7XXvHDEbK/mwWNtJX2RZJjIPbpTrQS2BYZ+IpRgOLzCWinJFqW/fDyIDpuxJl3xEPM8vUmTwNMYguTBbLTlpnETm6WqsyGKZl6WUpmJlDzwlUPVRM7loNlgsuZB2SkypV1w5Wyu2rGDmhqiKbmvgh3bGqeV9WwSjT3O1QXlAqxAcfmOQqT/HXLdGap9C2dWer0uZpQJA4245U+yniZ2xc2Z/NAsRqf8ptzdChwhfvDKsNXzcnTH1dnWNjp5NA0xjH9gqBC1DxXMmRkcCALTGndANnqLk9vOuBFyURUbJq2GUqR+qcZAHPrk/Cn2aB0GSJPx3SEt892AntArzRvMQT7g6a0zZo2HtwnBH6xDMGCjKAInpYoHG36Z0w6vDWmG1vhuuyl7YIJk10xodviAyLClZhXjwpz24lFWIxoEe+OyBjvhw0mAU6oJwICEDb/11wrhFx2Gcu5yHQE8X/DChM3o1C0S3xgH44oGOCPBwwZHUYkzMnIKehZ9jSNF72FKkNtRfgTc+23AWRzLFsgXZ8MBXjw/HwfoPI0kOwVebYzFxzl78ujvB1KSdcDUPj/16AI8dikQKAjCiQzg8XLSYldoFr2qfxrrYPFOP1eGkTNMaTh//cxr/t/okdp2/iu+3nsP/rT6J06k5yCksRZMgT7Rt0gDFegOulBqDYOMCjHt8h+Gy7IO1hi7I1wN7k3KR09S4bsug/8PJ0RuxuMl7SGtsLKeEtgUe3QK8FIfCDo+Y3mtRYGu8v/YUvtkSW670dzGzAGk5hfjjwAXEy2HYL7dEXJYBD/20B2dTc8r9G62MuYj+H2/B3V/twFurTiKt/RMAgGKvCKxr+hqeXX0RE+fuw5D4scjt9gwK4Ips2Q1zTgJT5u9DmrFkC52nCBAA47Ykxvew8W3g6jlknRQZnz2GVpizPU4N7BRKr5KsF9mfRn3U0lz8dnWmlDIDTykFetcD7vxQfJD7GDMTZRvLyza6t70HGPKeyDSlHlXLnOZlqcIs4KOmYs2h0iIx427fD+IxZekAi/WDzHp0rpxRgzFlBefw20QPmbu/+r6zL4mtL7bMUhddLMgUqxiveByYO0Q8pkjcLc6PXyN1cczMBBHIKI31WhdjU7HZatpKWcp8Bp/S8J6VZJmxumq2/pMDMXNDVMMqKnV1bxyAZYfELwnzslSlKP0FgNqMq9zUaiz2Bqvv544V09RG2j7Ng9Am3BvHL2VjWj+1SVWjkfDzI10Rm5arlsSaDQZ6vwj3+j3gvaAYien5eFEaiTu14WjTeywmJhZg3o44fHhve+ic1H4USZIwpG0ohrQVHwqbT6fh0Z/3o2mwF9oYMy8zBjbH3VHhWH0kGc1CPNEl0h9dIv3xnWEKhu8YgP+7px0891/AmmMpaF/fB0cuZOHIhUx8vuEsvtkSi6JSA8J8XPHz5K7w1DnBU+eEL8d1xP/+PokG/uKcXMjIR/+WwZjcq5HFcgTB3q6YcUdzvLbiGLbHXgEQgId7RGKomwewQ6woXOzih5TsQmzP16CzEyAHNEO7CF8MbReKvfHpWLBHrC2z9cxlxCRlok24Nz5adxp5xaLh/JmBzfHUgGbYdCoVk+fvx+97k7D6iOhp6NciCEcvZqFX00CcSM62KLf9uD0OsgxE+Lvhw3uj0LGBL5LSCzDwk61IKdYhRAMczvGC5kIWFp/RY3nR15ChweBFMVh3PBX1PIfhn0enwyO8FV7/bif2xTdAu8wYLHqsuylD+fXmWHy8uz+e0OYi0zUcZ+bFYF+8WBRy48k09GwSgI4N/eDh4oQHf9wDJ61k2pT263G3Yfa/53E4KRMP/rQHSx/vgQh/dxSW6PHGyuNYtF/tFfl5dwLWezeCXPglIsMaIyWlFEAeXJw0iM0oxaiT/XCpsC2C3TUICvRH3JU8/GfhISyY0h0ajYQzqTl4dnEMIgM88FCre9HNa6no+fmhP7SSKPXtNrTC1jOXEXslH03DotQyS/9XRSDUsCcMw7/C+ct5aNjwdjjHbhBZEWXlbqUUWO82YMpGESS5GoOF0HbiwzqgKXIKSxCTlAlPnRNahQXB1TNEXbvHv4nYKNQzBFg6SZSr+s5UyzP+TdRFHq+eBfZ8L/Y8UyjZpYpWrFaWZAiLAtKMx4SYNa57hwOXs8XGssqsrfNbRInp0K+i1KZxFg3ap/4W5wZQz1VkL9EcnXZCBC5KSUrSiPOUtFsEN6HtRNO2UpZSgj9AlBWVrKESGAEsSxHdaro1VjM3jYKqGNw46cTMltRjloFOJUiSCGKSswrL9SUFe7si2FvdjR1aJ6D/K3AFMKbzCfy4PQ4H5Ba4LXoIJreIQPcWwDMDm123V6lfi2D8+2J/eOi0Fsc2DvLEUwOaWRz7eJ8meKx3Y0iShB5NAnFf5ytoGuSF3h9uxvFL2Th+STQPd2rohw/vbY/6fmpJr2+LYPRtcZ3NQY0e6BKBOdvjEHclD77uznh+cAt4FngDxq2h2jVvghWHgaMG8eHn3Vz8ZT+0bRje+kv0KwV56XAltwhLD1zAUmNvbtdG/vi/kW3R3Jg5698yBG/d3QavrzyO7MJSaCTg/0a1Q5i3KzQaCVtOp2HSvH1oHeaN1OxCXMkVmZ1xXRuiu3HmXNNgT9zWwBfHL0UiSnMebx50R+zx3ZBlQDYm3NcdFx+0F3NlfH9ciwe9CrE/QQQsRy9m4bFfDuCTMR3g5eqEH/89DwM0+F4ehdI8GcjLgKuzBk4aDQ4kZOCA8XnOWgklehnFxuVLgr10GNI2FD2aBOD+2btwJjUX43/cg6WPR+PH7XFYtD8JGgl4qn8znE3Lwd9HU4wb6wYgOU584OmcNJh1Tzs8u/gwzqblAnDDQ/1bo0+LIAz/cjt2n0/HkgNJuL9LA7z113Ecu5iNYxezseoI8MVdC3D38WeA5Bh4IRMAcADiQ/6FpYcxO6QVgvCvKMd1nwZDu/sx+2Au5ny4HWk5RRge7IcvAVGeUvqgjH1L6XnF+L+dTujeKA9juhj/bw77BOj4IPZoovD0J9tMEwG6RPphcVgUpLP/iOOUrEWzQaIcmH7euNifLMYy4DVg64di25VjfwDrXxPHu/qIjE76ufJbkCiUwCjlmAhuUk8Y/zHMg5t6IvjY9qF6X/YFkTVRtsToN1MEe2nHRW+PR6BZcHO7uhFt9iU1uHHzF4tdJu0Wgc/ih0S2R1lN3Dy4kSQRGJbtz1Ea0B2MZSkiO2ng7447WoegS6SfxSKIldZpokifX2uhvgoEeOoq3XCtmNSrEYK9dBjWPgz/HapOva7sdP9QH9dKLzWgvKarsxb9W4Ygwt8NAR7qytlTb2+EpY9Ho/GNnDcjZ60Gb93dBr7uznh5aCt46pzEh4RxzZk+XUQPx0ZDJ/zR809IA980vY8HukSgRYgX/ni8B35+pCtGdAhHm3BvzBjYDAumdDMFNooJ0ZF4e0QbSBJwZ7sw1PN1My1N0LdFMNY/0weLH4s29XM5ayWM6Wy56vHIjvXwaukj6F74JQ7JzZBTWIrcolKE+biKsUNsXgsAP2w7j192JUCWgXq+bnBx0uDfs1cw6NOteG/NKWTklyDcxxVH3xyMF4e0QNdIf8yf1BWr/9MLT/Vvins71TcFNlH1fdDO+LMyulN9aDUS/Dxc8Mvkbmjg747E9Hw88dtB/LwrHgDwxdiOeOaO5nj9rjbw1DnBSSOZyp2ACHRHdayH1mEiOxLg4YKxXRugSZAnnr1DlEY+WHsaq48kY0fsVThrJQyPEk3mz/6dgnkNZ0HvKkqx8YYQPDy0J1ycNDiUmInnD4j7z/l0w74LuZiy/CLeW3cWaTlij7JVaf7IgDdQnIvSU2JBw78uuOL3vYl44tcDWHbwIl784wiWHzIu4ucdhv2u0Rj/0z6kZBci0FMHJ42EffEZOCOZbcHg3wSyLEN28RC9QoC6xotvQ9H0++ROYOS3ahOuiycwdpG6VELS3vIbqWqc1Rlw2z4QWRUlA6SssG0cp4mrrxr4rHtZlJlcfcUWKcr98dtF6UmZhdUwWi0xZV9QZ0p5BIqeHkCsl3RqtWjEVsZZdmFJ87WYlFJZZqIowzkYMzdEdiJJEn6YcP1Gztqinq8b9rwsfnHf6PpFN0qSJLSv74PNpy8j2EuHpwc2r5Ex9G4ehJjXB6l3aDTAA78BOSlo2rQFxnQuxLnLeRjcu6vIYhkpaxgBQIMAd4uFICsyIToSQ9qGws/K9iZNg0WQ9nCPSGw7cxl3tA4pt6r3sHYiY5RiCECwlw6Z+SUo1hswokM9BHq6YM72OHw9/jb83+qTOJCQgS83iXLAg90bom+LIDy3+DBOJGebGrTv79IAbi5aPNm3KZ7sq5YnnzOuE/VY78bYeCoNYzpHwEkrYdPJNFOZEQBCvF0xb1IXDPtiuynT07GBL4a1Ex+0oT6u+OupXijRG5CUno/J80XpZVj7MEiShNeHt8Z/fj+EF4e0hJuL1nSOFuxNxPnLeZi2QPScjO/WEG8Mbw1njYRlhy7izS0Z2Kh5DF87f4E/5H6YHt0Qg1qH4pP1p/H3UQ3GFL2G0xcikPWdWGjRxUmDN4a3RocIX0ycsw87ilrhLu0eOOkLkOUchFcOByL7sPiQ10iAQQaeW3wYc7bHY0jbUCzen4RSg4yBrULw+QMd8NrKY1h28CI+Pe6G75yBPFmHHYnAG39tQnGpAc+GdMR4rFbXEPKLVP8RnXTAXZ+I9WAGvCHWaqrXSWR6Dv8uykZK+QgQDbu9nhELIWbEA192EmUhNz+LJQoQ1kGUn0LaAffNE+WptBPqukCdHxGz8Rr1FvfHbRMNyYZSMS3fJ0INVLIuqpvlugeqG90mlJlSD+Bgpgc6yrL6f9F8LabQ9qJBujhHrP7dYbzakOwAkizXoW1AKyE7Oxs+Pj7IysqCt7f39Z9ARA6x6sglvLbiGD64Nwp3tA65/hPqoGkLDmL1kWR8Pe42FJboseRAEj67vyNCfdRSYsLVPNzzzU5T4/Km5/qgcZAnCkv0mPrzfvx79gq0Ggk7Xupv8bwb9cuueLy2UpRT5j/S1aKhXqE3yLjnmx3IyC/Bmqdvh4eu4r+jd527iqk/70duUSlCvHVY9dTtCPLSoURvwOL9SVh7LEX0SckG9GoWjF8mdzM9N7eoFOuOpeCHf8/jQkYBBrQKxqO9G6NNuMg8Xc0twoUN3yIq5g2Uyho8UPwq9sst4e/hgtzCUnwz/jasOZaCPw5abr9Qz9cNa2bcDm9XZxxKzMCob3bCG3lY4fIadhna4E15Ckr04qPTAwXY4fUyfEtEmdDQeQpmSZOx7OBFPNanMab0aoy84lKM+X43/NydMa/1Abisf9n0vQrDusA14yxQmIn88B5IHrUETXAR+PEOtZelXidgqtkqyQaDKFEHtxL7qcX9C8w3Tlf3bww8vh2yszuSdi1Bg3+miubh/q+K/ePCbxOLSJ7fAvw8Qiya2GUysOZFMWtt2CfAh+XXGzNAQovC+RjYtj7q+4lZgy+FH4bTysfFAe3vR+qF8whJN06B94sU6zhptOVe60ZV5fObwQ0RUS2VX1yKS5kFaBrsdc3jYpIy8dBPe9Am3BsLH1UXoyss0ePDdafROMgD47uVn8F3I2RZxofrTkOSgOcHtagwo6Y3yJCAClcKL/uaRaUGOGkkOGnLl10TruZhw8k0DGodggj/qi2jgKIcpC98Am+djsBKQy/0aBKAnx/pivwSPbyNZdMLGfnYGXsVP24/j8T0fMx5uAt6NAk0je3ur3bg6MUs3N40AP8al1TQaiQ8P6gF3l97Cq20yVjjLLZH+S3gKbxyUf03GNAyGB0ifPHxerHX2JRGV/Fq8lOmx9e63YXBQVchJe7CEn0fvFT6GB7u0Qgvts2G63yx2OVG71GY5/MEPru/A1ycNCjRy/A3K9uipBCF7zeDS2kupEfWQmrQDXO2x+GzVXsR4/oYNJCBLlPFbK32DwD3fA9ciQW+6iTKZd2fFGWwzpNFpunDpupK0kapsi+6FX1jcd/8OyT0+XcsACC+1aO4N6YjHtL+g0fc/oVXuzvFNh01iMHNNTC4IaK6KL+4FDonrWmWE1l6feUxrD2Wgt+mdDMtm1CWLMso1hssZgICYrPbrafTMLZbA4z5bhcOX8jCpJ6ReGN4G4z5fhf2xqVjbMMctMvegv9lDECxkyce7NYQC/YmoLDEAElS18rTQo8/vD9Dk6ITMEDCo8XPYULAcQzLXYb/lYzFbP1wAECTIA/8eIcTfI7Ow6hj0UiQQxHirUNGfgmKSw1oGuyJ/w5piYGtQxCTlIkXvlkEVxRj1vQJaBzkgV7vbxZ7u7m8jDaaeFGOKs5BRveZ+KzoLoxq548OPxv7a9rdJ9b86f0i0P8VsSKysfn4il8HBGbE4LDcBPGj/sKaoym4mFmAoxezMKSxC767dK84vyUP42e9KPnqNHr8/VgHNGnYADWJwc01MLghIqIbFX8lD5tOpWFctwZwddZiR+wVjP9R3b8qwMMFsyd0RqeGflhx6CJmLIoBAEQGuOP5wS3w1O+HTIFOuI8rLmUVwhu56K+JQWSvMejQpB5e+uMIUrOL4OvujNG31cdP2+OsjERMWHpxcEscTMzAeuNWJ08PaAYPnRb/+1s0Ab/i9CumOv1tes4MvIAVhR3h7qLFEbfH4FSUKcpWV84AQz9AVvtHkPXHDDSI/RV6F2887fQKPs57BUciH0aXSR8DABKv5qP3h2LdoYO6R+Ev5WJq8bPIajgIXjonbDyVhtubBeLnR7rWaL9eVT6/2VBMRERUSZGBHnikl7oNRs+mYm+7AwkZ0BtkTL29sWnF8pEd6yEmKRPzdsbjxSEtcWe7MKRlF+HtVSfgotVg4aPReP3PYyjVB+LBgXegs3H7ljVP98a93+7E+St5mLtDBDbP3tEcGkl8v0aBHvjon9P4dXci3l9rOeNq1ZFLyMwXDcoPdInArgOtMRVqcHOkKATuLlrkF+sRq/FBS2SKwAbApzuv4ssV/2Ck5IlPXIDNhc2wqjgCu1znY8sDaiN+gwB3dIn0w774DKzWd8c9bgcxc+JDqBfREKlZRdgeewWh3q5Ws2D2wswNERGRjciyjOyCUvi4q8sirDpyCf7uLlb3k1P8ticBryw/Zrq995UBCPZSG8JlWcbCfUl4d/VJ5BaVontjf+yJSzdlhZoEeWDtjN749K/9eC5mMLQwoARO+LTbVjzapzke/GkPnkl7DQO06iaZ44pfxk5DWzQNcMUww2YsymiOFARger+meH5wC4vxLdqXiJf+OIrIAHesfboXXF3U95eWU2gx1prCzA0REVEtIEmSRWADAHe1D7/u80Z1rIf315xCdmEp2tXzKRcsSJKEsV0boG+LIGw4kYq7o+ph8vx9poUc3x3VDs5aDV4c2RVI6wBcOgjnoKZ4cajYIWzm0FaIm+dv8Zr16kdix7j+qOfrhoLivjizOAaxabmY1DOy3Pju6xQhNv1tHGAR2ACwSWBTVVzEj4iIqJZxd3HCwz1F+evuqIqDoTAfNzwUHQkfd2eM6CCOu69TfdNq1wDUvemUNWwA9GgSAK2vunDkQe8BeHfqvahn3BzWzUWLbx/shPXP9im3BhMgZsGN6RxR9dlrdsKyFBERUS1kMMg4fikbbcK9KzWl3mCQceRiFtrV87GcNZd3Fdj0DtD1UYs9qmKP7obH8om4UO9OdJ70EaQaXJPGFjhb6hoY3BAREd18qvL5zbIUERER1SkMboiIiKhOYXBDREREdQqDGyIiIqpTGNwQERFRncLghoiIiOoUBjdERERUpzC4ISIiojqFwQ0RERHVKQxuiIiIqE5hcENERER1CoMbIiIiqlMY3BAREVGdwuCGiIiI6hQnRw/A3mRZBiC2TiciIqKbg/K5rXyOX8stF9zk5OQAACIiIhw8EiIiIqqqnJwc+Pj4XPMYSa5MCFSHGAwGXLp0CV5eXpAkqUZfOzs7GxEREUhKSoK3t3eNvnZdw3NVNTxflcdzVXk8V1XD81V5tjhXsiwjJycH4eHh0Giu3VVzy2VuNBoN6tevb9Pv4e3tzR/8SuK5qhqer8rjuao8nquq4fmqvJo+V9fL2CjYUExERER1CoMbIiIiqlMY3NQgnU6HN954AzqdztFDqfV4rqqG56vyeK4qj+eqani+Ks/R5+qWaygmIiKiuo2ZGyIiIqpTGNwQERFRncLghoiIiOoUBjdERERUpzC4qSFff/01IiMj4erqim7dumHv3r2OHlKt8Oabb0KSJIuvli1bmh4vLCzEtGnTEBAQAE9PT4wePRqpqakOHLH9bNu2DcOHD0d4eDgkScKKFSssHpdlGa+//jrCwsLg5uaGgQMH4uzZsxbHpKenY/z48fD29oavry8mT56M3NxcO74L+7jeuXr44YfL/ZwNGTLE4phb5VzNmjULXbp0gZeXF4KDgzFy5EicPn3a4pjK/L9LTEzEsGHD4O7ujuDgYLzwwgsoLS2151uxi8qcr759+5b7+Xr88cctjrkVzte3336L9u3bmxbmi46Oxpo1a0yP16afKwY3NWDRokV49tln8cYbb+DgwYOIiorC4MGDkZaW5uih1Qpt2rRBcnKy6Wv79u2mx5555hn89ddfWLJkCbZu3YpLly7hnnvuceBo7ScvLw9RUVH4+uuvrT7+wQcf4IsvvsB3332HPXv2wMPDA4MHD0ZhYaHpmPHjx+P48eNYv349Vq1ahW3btuHRRx+111uwm+udKwAYMmSIxc/Z77//bvH4rXKutm7dimnTpmH37t1Yv349SkpKMGjQIOTl5ZmOud7/O71ej2HDhqG4uBg7d+7E/PnzMW/ePLz++uuOeEs2VZnzBQBTp061+Pn64IMPTI/dKuerfv36eO+993DgwAHs378f/fv3x4gRI3D8+HEAteznSqZq69q1qzxt2jTTbb1eL4eHh8uzZs1y4KhqhzfeeEOOioqy+lhmZqbs7OwsL1myxHTfyZMnZQDyrl277DTC2gGAvHz5ctNtg8Egh4aGyh9++KHpvszMTFmn08m///67LMuyfOLECRmAvG/fPtMxa9askSVJki9evGi3sdtb2XMly7I8ceJEecSIERU+51Y9V7Isy2lpaTIAeevWrbIsV+7/3d9//y1rNBo5JSXFdMy3334re3t7y0VFRfZ9A3ZW9nzJsiz36dNHfvrppyt8zq18vvz8/OQff/yx1v1cMXNTTcXFxThw4AAGDhxouk+j0WDgwIHYtWuXA0dWe5w9exbh4eFo3Lgxxo8fj8TERADAgQMHUFJSYnHuWrZsiQYNGtzy5y4uLg4pKSkW58bHxwfdunUznZtdu3bB19cXnTt3Nh0zcOBAaDQa7Nmzx+5jdrQtW7YgODgYLVq0wBNPPIGrV6+aHruVz1VWVhYAwN/fH0Dl/t/t2rUL7dq1Q0hIiOmYwYMHIzs72/RXel1V9nwpfvvtNwQGBqJt27aYOXMm8vPzTY/diudLr9dj4cKFyMvLQ3R0dK37ubrlNs6saVeuXIFer7f4xwKAkJAQnDp1ykGjqj26deuGefPmoUWLFkhOTsZbb72F22+/HceOHUNKSgpcXFzg6+tr8ZyQkBCkpKQ4ZsC1hPL+rf1cKY+lpKQgODjY4nEnJyf4+/vfcudvyJAhuOeee9CoUSOcO3cOL7/8MoYOHYpdu3ZBq9XesufKYDBgxowZ6NmzJ9q2bQsAlfp/l5KSYvVnT3msrrJ2vgBg3LhxaNiwIcLDw3HkyBG89NJLOH36NJYtWwbg1jpfR48eRXR0NAoLC+Hp6Ynly5ejdevWiImJqVU/VwxuyKaGDh1qut6+fXt069YNDRs2xOLFi+Hm5ubAkVFd8sADD5iut2vXDu3bt0eTJk2wZcsWDBgwwIEjc6xp06bh2LFjFn1uVLGKzpd5b1a7du0QFhaGAQMG4Ny5c2jSpIm9h+lQLVq0QExMDLKysrB06VJMnDgRW7dudfSwymFZqpoCAwOh1WrLdYSnpqYiNDTUQaOqvXx9fdG8eXPExsYiNDQUxcXFyMzMtDiG5w6m93+tn6vQ0NByTeulpaVIT0+/5c9f48aNERgYiNjYWAC35rmaPn06Vq1ahc2bN6N+/fqm+yvz/y40NNTqz57yWF1U0fmyplu3bgBg8fN1q5wvFxcXNG3aFJ06dcKsWbMQFRWFzz//vNb9XDG4qSYXFxd06tQJGzduNN1nMBiwceNGREdHO3BktVNubi7OnTuHsLAwdOrUCc7Ozhbn7vTp00hMTLzlz12jRo0QGhpqcW6ys7OxZ88e07mJjo5GZmYmDhw4YDpm06ZNMBgMpl++t6oLFy7g6tWrCAsLA3BrnStZljF9+nQsX74cmzZtQqNGjSwer8z/u+joaBw9etQiIFy/fj28vb3RunVr+7wRO7ne+bImJiYGACx+vm6V81WWwWBAUVFR7fu5qtH25FvUwoULZZ1OJ8+bN08+ceKE/Oijj8q+vr4WHeG3queee07esmWLHBcXJ+/YsUMeOHCgHBgYKKelpcmyLMuPP/643KBBA3nTpk3y/v375ejoaDk6OtrBo7aPnJwc+dChQ/KhQ4dkAPInn3wiHzp0SE5ISJBlWZbfe+892dfXV165cqV85MgRecSIEXKjRo3kgoIC02sMGTJE7tixo7xnzx55+/btcrNmzeSxY8c66i3ZzLXOVU5Ojvz888/Lu3btkuPi4uQNGzbIt912m9ysWTO5sLDQ9Bq3yrl64oknZB8fH3nLli1ycnKy6Ss/P990zPX+35WWlspt27aVBw0aJMfExMhr166Vg4KC5JkzZzriLdnU9c5XbGys/Pbbb8v79++X4+Li5JUrV8qNGzeWe/fubXqNW+V8/fe//5W3bt0qx8XFyUeOHJH/+9//ypIkyf/8848sy7Xr54rBTQ358ssv5QYNGsguLi5y165d5d27dzt6SLXC/fffL4eFhckuLi5yvXr15Pvvv1+OjY01PV5QUCA/+eSTsp+fn+zu7i6PGjVKTk5OduCI7Wfz5s0ygHJfEydOlGVZTAd/7bXX5JCQEFmn08kDBgyQT58+bfEaV69elceOHSt7enrK3t7e8qRJk+ScnBwHvBvbuta5ys/PlwcNGiQHBQXJzs7OcsOGDeWpU6eW++PiVjlX1s4TAHnu3LmmYyrz/y4+Pl4eOnSo7ObmJgcGBsrPPfecXFJSYud3Y3vXO1+JiYly7969ZX9/f1mn08lNmzaVX3jhBTkrK8vidW6F8/XII4/IDRs2lF1cXOSgoCB5wIABpsBGlmvXz5Uky7Jcs7kgIiIiIsdhzw0RERHVKQxuiIiIqE5hcENERER1CoMbIiIiqlMY3BAREVGdwuCGiIiI6hQGN0RERFSnMLgholueJElYsWKFo4dBRDWEwQ0ROdTDDz8MSZLKfQ0ZMsTRQyOim5STowdARDRkyBDMnTvX4j6dTueg0RDRzY6ZGyJyOJ1Oh9DQUIsvPz8/AKJk9O2332Lo0KFwc3ND48aNsXTpUovnHz16FP3794ebmxsCAgLw6KOPIjc31+KYOXPmoE2bNtDpdAgLC8P06dMtHr9y5QpGjRoFd3d3NGvWDH/++adt3zQR2QyDGyKq9V577TWMHj0ahw8fxvjx4/HAAw/g5MmTAIC8vDwMHjwYfn5+2LdvH5YsWYINGzZYBC/ffvstpk2bhkcffRRHjx7Fn3/+iaZNm1p8j7feegtjxozBkSNHcOedd2L8+PFIT0+36/skohpS41txEhFVwcSJE2WtVit7eHhYfL377ruyLItdmx9//HGL53Tr1k1+4oknZFmW5dmzZ8t+fn5ybm6u6fHVq1fLGo3GtDN4eHi4/Morr1Q4BgDyq6++arqdm5srA5DXrFlTY++TiOyHPTdE5HD9+vXDt99+a3Gfv7+/6Xp0dLTFY9HR0YiJiQEAnDx5ElFRUfDw8DA93rNnTxgMBpw+fRqSJOHSpUsYMGDANcfQvn1703UPDw94e3sjLS3tRt8SETkQgxsicjgPD49yZaKa4ubmVqnjnJ2dLW5LkgSDwWCLIRGRjbHnhohqvd27d5e73apVKwBAq1atcPjwYeTl5Zke37FjBzQaDVq0aAEvLy9ERkZi48aNdh0zETkOMzdE5HBFRUVISUmxuM/JyQmBgYEAgCVLlqBz587o1asXfvvtN+zduxc//fQTAGD8+PF44403MHHiRLz55pu4fPkynnrqKTz00EMICQkBALz55pt4/PHHERwcjKFDhyInJwc7duzAU089Zd83SkR2weCGiBxu7dq1CAsLs7ivRYsWOHXqFAAxk2nhwoV48sknERYWht9//x2tW7cGALi7u2PdunV4+umn0aVLF7i7u2P06NH45JNPTK81ceJEFBYW4tNPP8Xzzz+PwMBA3HvvvfZ7g0RkV5Isy7KjB0FEVBFJkrB8+XKMHDnS0UMhopsEe26IiIioTmFwQ0RERHUKe26IqFZj5ZyIqoqZGyIiIqpTGNwQERFRncLghoiIiOoUBjdERERUpzC4ISIiojqFwQ0RERHVKQxuiIiIqE5hcENERER1CoMbIiIiqlP+HxMC8EIEOLhyAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(model_history.history['loss'])\n",
    "plt.plot(model_history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "02017b5a-a57d-4422-9312-df87bc61d016",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(model_history.history['loss'])\n",
    "plt.plot(model_history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.savefig('model_training_visualization.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9fbf2f1b-1309-4d97-96f5-4e34a73b9ce3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "77/77 [==============================] - ETA: 0s - loss: 1.8085 - accuracy: 0.4839\n",
      "Epoch 1: val_loss improved from inf to 1.44134, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 4s 32ms/step - loss: 1.8085 - accuracy: 0.4839 - val_loss: 1.4413 - val_accuracy: 0.5720\n",
      "Epoch 2/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 1.3999 - accuracy: 0.5746\n",
      "Epoch 2: val_loss improved from 1.44134 to 1.40990, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 1.4011 - accuracy: 0.5743 - val_loss: 1.4099 - val_accuracy: 0.5212\n",
      "Epoch 3/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 1.3635 - accuracy: 0.5763\n",
      "Epoch 3: val_loss improved from 1.40990 to 1.28454, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 1.3628 - accuracy: 0.5766 - val_loss: 1.2845 - val_accuracy: 0.5959\n",
      "Epoch 4/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 1.3553 - accuracy: 0.5737\n",
      "Epoch 4: val_loss did not improve from 1.28454\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 1.3561 - accuracy: 0.5735 - val_loss: 1.3206 - val_accuracy: 0.5861\n",
      "Epoch 5/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 1.3163 - accuracy: 0.5826\n",
      "Epoch 5: val_loss improved from 1.28454 to 1.24145, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 1.3154 - accuracy: 0.5829 - val_loss: 1.2415 - val_accuracy: 0.5940\n",
      "Epoch 6/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 1.3232 - accuracy: 0.5756\n",
      "Epoch 6: val_loss did not improve from 1.24145\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 1.3244 - accuracy: 0.5752 - val_loss: 1.2679 - val_accuracy: 0.5943\n",
      "Epoch 7/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 1.3046 - accuracy: 0.5791\n",
      "Epoch 7: val_loss did not improve from 1.24145\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 1.3045 - accuracy: 0.5790 - val_loss: 1.3296 - val_accuracy: 0.5515\n",
      "Epoch 8/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 1.3046 - accuracy: 0.5758\n",
      "Epoch 8: val_loss did not improve from 1.24145\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 1.3053 - accuracy: 0.5757 - val_loss: 1.2997 - val_accuracy: 0.5785\n",
      "Epoch 9/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 1.3038 - accuracy: 0.5779\n",
      "Epoch 9: val_loss improved from 1.24145 to 1.22149, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 1.3055 - accuracy: 0.5775 - val_loss: 1.2215 - val_accuracy: 0.6004\n",
      "Epoch 10/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 1.2778 - accuracy: 0.5791\n",
      "Epoch 10: val_loss improved from 1.22149 to 1.17318, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 19ms/step - loss: 1.2778 - accuracy: 0.5790 - val_loss: 1.1732 - val_accuracy: 0.6017\n",
      "Epoch 11/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 1.2541 - accuracy: 0.5835\n",
      "Epoch 11: val_loss did not improve from 1.17318\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 1.2533 - accuracy: 0.5836 - val_loss: 1.1998 - val_accuracy: 0.5956\n",
      "Epoch 12/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 1.2383 - accuracy: 0.5782\n",
      "Epoch 12: val_loss did not improve from 1.17318\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 1.2386 - accuracy: 0.5781 - val_loss: 1.1892 - val_accuracy: 0.5623\n",
      "Epoch 13/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 1.2157 - accuracy: 0.5859\n",
      "Epoch 13: val_loss improved from 1.17318 to 1.16135, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 1.2151 - accuracy: 0.5866 - val_loss: 1.1614 - val_accuracy: 0.5950\n",
      "Epoch 14/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 1.1932 - accuracy: 0.5908\n",
      "Epoch 14: val_loss improved from 1.16135 to 1.07448, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 19ms/step - loss: 1.1926 - accuracy: 0.5908 - val_loss: 1.0745 - val_accuracy: 0.6040\n",
      "Epoch 15/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 1.1470 - accuracy: 0.5873\n",
      "Epoch 15: val_loss did not improve from 1.07448\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 1.1463 - accuracy: 0.5875 - val_loss: 1.1243 - val_accuracy: 0.6032\n",
      "Epoch 16/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 1.1419 - accuracy: 0.5887\n",
      "Epoch 16: val_loss did not improve from 1.07448\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 1.1416 - accuracy: 0.5885 - val_loss: 1.1847 - val_accuracy: 0.5883\n",
      "Epoch 17/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 1.1093 - accuracy: 0.6013\n",
      "Epoch 17: val_loss improved from 1.07448 to 1.05058, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 19ms/step - loss: 1.1087 - accuracy: 0.6015 - val_loss: 1.0506 - val_accuracy: 0.6183\n",
      "Epoch 18/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 1.0793 - accuracy: 0.6017\n",
      "Epoch 18: val_loss improved from 1.05058 to 0.99060, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 1.0786 - accuracy: 0.6020 - val_loss: 0.9906 - val_accuracy: 0.6101\n",
      "Epoch 19/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 1.0820 - accuracy: 0.6016\n",
      "Epoch 19: val_loss did not improve from 0.99060\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 1.0810 - accuracy: 0.6021 - val_loss: 1.0102 - val_accuracy: 0.6255\n",
      "Epoch 20/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 1.0505 - accuracy: 0.6014\n",
      "Epoch 20: val_loss did not improve from 0.99060\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 1.0500 - accuracy: 0.6016 - val_loss: 1.0037 - val_accuracy: 0.6237\n",
      "Epoch 21/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 1.0590 - accuracy: 0.6011\n",
      "Epoch 21: val_loss did not improve from 0.99060\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 1.0600 - accuracy: 0.6003 - val_loss: 1.0709 - val_accuracy: 0.6006\n",
      "Epoch 22/1000\n",
      "75/77 [============================>.] - ETA: 0s - loss: 1.0378 - accuracy: 0.6105\n",
      "Epoch 22: val_loss did not improve from 0.99060\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 1.0345 - accuracy: 0.6118 - val_loss: 1.0686 - val_accuracy: 0.6032\n",
      "Epoch 23/1000\n",
      "75/77 [============================>.] - ETA: 0s - loss: 1.0509 - accuracy: 0.6043\n",
      "Epoch 23: val_loss did not improve from 0.99060\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 1.0482 - accuracy: 0.6052 - val_loss: 1.0962 - val_accuracy: 0.5679\n",
      "Epoch 24/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 1.0582 - accuracy: 0.6018\n",
      "Epoch 24: val_loss did not improve from 0.99060\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 1.0579 - accuracy: 0.6019 - val_loss: 1.0242 - val_accuracy: 0.6356\n",
      "Epoch 25/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 1.0301 - accuracy: 0.6117\n",
      "Epoch 25: val_loss did not improve from 0.99060\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 1.0313 - accuracy: 0.6115 - val_loss: 1.0502 - val_accuracy: 0.5738\n",
      "Epoch 26/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 1.0306 - accuracy: 0.6074\n",
      "Epoch 26: val_loss did not improve from 0.99060\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 1.0309 - accuracy: 0.6074 - val_loss: 1.0002 - val_accuracy: 0.6282\n",
      "Epoch 27/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 1.0232 - accuracy: 0.6098\n",
      "Epoch 27: val_loss improved from 0.99060 to 0.95638, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 19ms/step - loss: 1.0227 - accuracy: 0.6103 - val_loss: 0.9564 - val_accuracy: 0.6505\n",
      "Epoch 28/1000\n",
      "75/77 [============================>.] - ETA: 0s - loss: 1.0129 - accuracy: 0.6159\n",
      "Epoch 28: val_loss did not improve from 0.95638\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 1.0146 - accuracy: 0.6153 - val_loss: 1.0635 - val_accuracy: 0.6100\n",
      "Epoch 29/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 1.0005 - accuracy: 0.6098\n",
      "Epoch 29: val_loss improved from 0.95638 to 0.94704, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 1.0005 - accuracy: 0.6097 - val_loss: 0.9470 - val_accuracy: 0.6237\n",
      "Epoch 30/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.9980 - accuracy: 0.6094\n",
      "Epoch 30: val_loss did not improve from 0.94704\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.9969 - accuracy: 0.6099 - val_loss: 1.0116 - val_accuracy: 0.6103\n",
      "Epoch 31/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 1.0165 - accuracy: 0.6061\n",
      "Epoch 31: val_loss did not improve from 0.94704\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 1.0164 - accuracy: 0.6064 - val_loss: 0.9660 - val_accuracy: 0.6219\n",
      "Epoch 32/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.9769 - accuracy: 0.6138\n",
      "Epoch 32: val_loss did not improve from 0.94704\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.9769 - accuracy: 0.6140 - val_loss: 1.0271 - val_accuracy: 0.6089\n",
      "Epoch 33/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 1.0055 - accuracy: 0.6121\n",
      "Epoch 33: val_loss did not improve from 0.94704\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 1.0054 - accuracy: 0.6124 - val_loss: 1.0143 - val_accuracy: 0.6005\n",
      "Epoch 34/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.9908 - accuracy: 0.6130\n",
      "Epoch 34: val_loss did not improve from 0.94704\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.9904 - accuracy: 0.6131 - val_loss: 0.9525 - val_accuracy: 0.6292\n",
      "Epoch 35/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.9665 - accuracy: 0.6148\n",
      "Epoch 35: val_loss improved from 0.94704 to 0.93135, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 19ms/step - loss: 0.9664 - accuracy: 0.6151 - val_loss: 0.9313 - val_accuracy: 0.6189\n",
      "Epoch 36/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 1.0119 - accuracy: 0.6042\n",
      "Epoch 36: val_loss did not improve from 0.93135\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 1.0123 - accuracy: 0.6043 - val_loss: 1.0168 - val_accuracy: 0.6194\n",
      "Epoch 37/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.9819 - accuracy: 0.6090\n",
      "Epoch 37: val_loss did not improve from 0.93135\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.9816 - accuracy: 0.6094 - val_loss: 0.9600 - val_accuracy: 0.6223\n",
      "Epoch 38/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.9800 - accuracy: 0.6145\n",
      "Epoch 38: val_loss improved from 0.93135 to 0.93083, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.9813 - accuracy: 0.6142 - val_loss: 0.9308 - val_accuracy: 0.6375\n",
      "Epoch 39/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.9772 - accuracy: 0.6095\n",
      "Epoch 39: val_loss did not improve from 0.93083\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.9782 - accuracy: 0.6092 - val_loss: 0.9703 - val_accuracy: 0.6208\n",
      "Epoch 40/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.9684 - accuracy: 0.6137\n",
      "Epoch 40: val_loss did not improve from 0.93083\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.9687 - accuracy: 0.6134 - val_loss: 0.9500 - val_accuracy: 0.6146\n",
      "Epoch 41/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.9768 - accuracy: 0.6101\n",
      "Epoch 41: val_loss improved from 0.93083 to 0.92412, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.9771 - accuracy: 0.6098 - val_loss: 0.9241 - val_accuracy: 0.6341\n",
      "Epoch 42/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.9647 - accuracy: 0.6181\n",
      "Epoch 42: val_loss did not improve from 0.92412\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.9653 - accuracy: 0.6179 - val_loss: 0.9548 - val_accuracy: 0.6192\n",
      "Epoch 43/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.9573 - accuracy: 0.6186\n",
      "Epoch 43: val_loss did not improve from 0.92412\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.9590 - accuracy: 0.6180 - val_loss: 0.9911 - val_accuracy: 0.6180\n",
      "Epoch 44/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 1.0186 - accuracy: 0.6121\n",
      "Epoch 44: val_loss did not improve from 0.92412\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 1.0179 - accuracy: 0.6123 - val_loss: 0.9539 - val_accuracy: 0.6212\n",
      "Epoch 45/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.9760 - accuracy: 0.6152\n",
      "Epoch 45: val_loss did not improve from 0.92412\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.9771 - accuracy: 0.6148 - val_loss: 0.9385 - val_accuracy: 0.6262\n",
      "Epoch 46/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.9565 - accuracy: 0.6128\n",
      "Epoch 46: val_loss improved from 0.92412 to 0.91525, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 19ms/step - loss: 0.9564 - accuracy: 0.6126 - val_loss: 0.9152 - val_accuracy: 0.6201\n",
      "Epoch 47/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.9286 - accuracy: 0.6199\n",
      "Epoch 47: val_loss improved from 0.91525 to 0.87707, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.9293 - accuracy: 0.6198 - val_loss: 0.8771 - val_accuracy: 0.6346\n",
      "Epoch 48/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.9440 - accuracy: 0.6200\n",
      "Epoch 48: val_loss did not improve from 0.87707\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.9437 - accuracy: 0.6201 - val_loss: 0.9124 - val_accuracy: 0.6140\n",
      "Epoch 49/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.9393 - accuracy: 0.6101\n",
      "Epoch 49: val_loss did not improve from 0.87707\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.9395 - accuracy: 0.6101 - val_loss: 0.9417 - val_accuracy: 0.6053\n",
      "Epoch 50/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.9430 - accuracy: 0.6145\n",
      "Epoch 50: val_loss did not improve from 0.87707\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.9432 - accuracy: 0.6145 - val_loss: 1.0063 - val_accuracy: 0.6051\n",
      "Epoch 51/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.9639 - accuracy: 0.6117\n",
      "Epoch 51: val_loss did not improve from 0.87707\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.9648 - accuracy: 0.6116 - val_loss: 1.3844 - val_accuracy: 0.5894\n",
      "Epoch 52/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 1.0733 - accuracy: 0.5935\n",
      "Epoch 52: val_loss did not improve from 0.87707\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 1.0722 - accuracy: 0.5939 - val_loss: 0.9435 - val_accuracy: 0.6063\n",
      "Epoch 53/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.9415 - accuracy: 0.6131\n",
      "Epoch 53: val_loss improved from 0.87707 to 0.87090, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.9417 - accuracy: 0.6130 - val_loss: 0.8709 - val_accuracy: 0.6355\n",
      "Epoch 54/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.9539 - accuracy: 0.6185\n",
      "Epoch 54: val_loss did not improve from 0.87090\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.9534 - accuracy: 0.6187 - val_loss: 0.9307 - val_accuracy: 0.6230\n",
      "Epoch 55/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.9746 - accuracy: 0.6109\n",
      "Epoch 55: val_loss did not improve from 0.87090\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.9744 - accuracy: 0.6106 - val_loss: 0.9455 - val_accuracy: 0.6028\n",
      "Epoch 56/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.9177 - accuracy: 0.6211\n",
      "Epoch 56: val_loss improved from 0.87090 to 0.85267, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.9169 - accuracy: 0.6216 - val_loss: 0.8527 - val_accuracy: 0.6214\n",
      "Epoch 57/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.9169 - accuracy: 0.6244\n",
      "Epoch 57: val_loss did not improve from 0.85267\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.9167 - accuracy: 0.6243 - val_loss: 0.9184 - val_accuracy: 0.6216\n",
      "Epoch 58/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.9217 - accuracy: 0.6213\n",
      "Epoch 58: val_loss did not improve from 0.85267\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.9205 - accuracy: 0.6220 - val_loss: 0.9581 - val_accuracy: 0.5900\n",
      "Epoch 59/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.9448 - accuracy: 0.6158\n",
      "Epoch 59: val_loss did not improve from 0.85267\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.9442 - accuracy: 0.6157 - val_loss: 0.9061 - val_accuracy: 0.6127\n",
      "Epoch 60/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.9104 - accuracy: 0.6229\n",
      "Epoch 60: val_loss did not improve from 0.85267\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.9110 - accuracy: 0.6229 - val_loss: 0.9619 - val_accuracy: 0.6162\n",
      "Epoch 61/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.9195 - accuracy: 0.6225\n",
      "Epoch 61: val_loss did not improve from 0.85267\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.9182 - accuracy: 0.6232 - val_loss: 0.8988 - val_accuracy: 0.6357\n",
      "Epoch 62/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.9327 - accuracy: 0.6180\n",
      "Epoch 62: val_loss did not improve from 0.85267\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.9327 - accuracy: 0.6175 - val_loss: 0.9617 - val_accuracy: 0.5991\n",
      "Epoch 63/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.9025 - accuracy: 0.6239\n",
      "Epoch 63: val_loss did not improve from 0.85267\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.9023 - accuracy: 0.6236 - val_loss: 0.9101 - val_accuracy: 0.6222\n",
      "Epoch 64/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.9478 - accuracy: 0.6107\n",
      "Epoch 64: val_loss did not improve from 0.85267\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.9478 - accuracy: 0.6109 - val_loss: 0.9070 - val_accuracy: 0.6110\n",
      "Epoch 65/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.9138 - accuracy: 0.6172\n",
      "Epoch 65: val_loss did not improve from 0.85267\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.9127 - accuracy: 0.6177 - val_loss: 0.9561 - val_accuracy: 0.6093\n",
      "Epoch 66/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.8985 - accuracy: 0.6277\n",
      "Epoch 66: val_loss did not improve from 0.85267\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.8991 - accuracy: 0.6275 - val_loss: 0.9322 - val_accuracy: 0.6192\n",
      "Epoch 67/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.9029 - accuracy: 0.6218\n",
      "Epoch 67: val_loss did not improve from 0.85267\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.9022 - accuracy: 0.6221 - val_loss: 1.0047 - val_accuracy: 0.6307\n",
      "Epoch 68/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.9607 - accuracy: 0.6059\n",
      "Epoch 68: val_loss improved from 0.85267 to 0.81947, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.9622 - accuracy: 0.6058 - val_loss: 0.8195 - val_accuracy: 0.6444\n",
      "Epoch 69/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.8789 - accuracy: 0.6299\n",
      "Epoch 69: val_loss did not improve from 0.81947\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.8786 - accuracy: 0.6299 - val_loss: 0.8205 - val_accuracy: 0.6437\n",
      "Epoch 70/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.8799 - accuracy: 0.6207\n",
      "Epoch 70: val_loss did not improve from 0.81947\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.8810 - accuracy: 0.6200 - val_loss: 0.8378 - val_accuracy: 0.6371\n",
      "Epoch 71/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.9633 - accuracy: 0.6161\n",
      "Epoch 71: val_loss did not improve from 0.81947\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.9631 - accuracy: 0.6163 - val_loss: 0.8826 - val_accuracy: 0.6379\n",
      "Epoch 72/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.9164 - accuracy: 0.6224\n",
      "Epoch 72: val_loss did not improve from 0.81947\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.9175 - accuracy: 0.6221 - val_loss: 0.8958 - val_accuracy: 0.6200\n",
      "Epoch 73/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.9106 - accuracy: 0.6244\n",
      "Epoch 73: val_loss did not improve from 0.81947\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.9104 - accuracy: 0.6241 - val_loss: 0.8234 - val_accuracy: 0.6669\n",
      "Epoch 74/1000\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.8672 - accuracy: 0.6311\n",
      "Epoch 74: val_loss did not improve from 0.81947\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.8656 - accuracy: 0.6331 - val_loss: 0.8386 - val_accuracy: 0.6421\n",
      "Epoch 75/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.8996 - accuracy: 0.6226\n",
      "Epoch 75: val_loss did not improve from 0.81947\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.8996 - accuracy: 0.6226 - val_loss: 0.8237 - val_accuracy: 0.6574\n",
      "Epoch 76/1000\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.8619 - accuracy: 0.6344\n",
      "Epoch 76: val_loss did not improve from 0.81947\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.8626 - accuracy: 0.6344 - val_loss: 0.8481 - val_accuracy: 0.6251\n",
      "Epoch 77/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.8930 - accuracy: 0.6206\n",
      "Epoch 77: val_loss did not improve from 0.81947\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.8929 - accuracy: 0.6207 - val_loss: 0.8471 - val_accuracy: 0.6225\n",
      "Epoch 78/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.8824 - accuracy: 0.6333\n",
      "Epoch 78: val_loss did not improve from 0.81947\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.8826 - accuracy: 0.6333 - val_loss: 0.8251 - val_accuracy: 0.6313\n",
      "Epoch 79/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.8641 - accuracy: 0.6303\n",
      "Epoch 79: val_loss improved from 0.81947 to 0.81431, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.8637 - accuracy: 0.6301 - val_loss: 0.8143 - val_accuracy: 0.6486\n",
      "Epoch 80/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.8633 - accuracy: 0.6279\n",
      "Epoch 80: val_loss did not improve from 0.81431\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.8629 - accuracy: 0.6283 - val_loss: 0.8945 - val_accuracy: 0.5968\n",
      "Epoch 81/1000\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.8759 - accuracy: 0.6226\n",
      "Epoch 81: val_loss did not improve from 0.81431\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.8740 - accuracy: 0.6235 - val_loss: 0.8615 - val_accuracy: 0.6245\n",
      "Epoch 82/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.8737 - accuracy: 0.6247\n",
      "Epoch 82: val_loss did not improve from 0.81431\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.8737 - accuracy: 0.6250 - val_loss: 0.8251 - val_accuracy: 0.6678\n",
      "Epoch 83/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.8669 - accuracy: 0.6235\n",
      "Epoch 83: val_loss improved from 0.81431 to 0.79078, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 19ms/step - loss: 0.8671 - accuracy: 0.6232 - val_loss: 0.7908 - val_accuracy: 0.6559\n",
      "Epoch 84/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.8809 - accuracy: 0.6232\n",
      "Epoch 84: val_loss did not improve from 0.79078\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.8806 - accuracy: 0.6236 - val_loss: 0.8248 - val_accuracy: 0.6276\n",
      "Epoch 85/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.8491 - accuracy: 0.6328\n",
      "Epoch 85: val_loss did not improve from 0.79078\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.8491 - accuracy: 0.6329 - val_loss: 1.0126 - val_accuracy: 0.6254\n",
      "Epoch 86/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.8881 - accuracy: 0.6201\n",
      "Epoch 86: val_loss did not improve from 0.79078\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.8886 - accuracy: 0.6196 - val_loss: 0.8366 - val_accuracy: 0.6497\n",
      "Epoch 87/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.8471 - accuracy: 0.6316\n",
      "Epoch 87: val_loss did not improve from 0.79078\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.8481 - accuracy: 0.6307 - val_loss: 0.8302 - val_accuracy: 0.6552\n",
      "Epoch 88/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.8645 - accuracy: 0.6279\n",
      "Epoch 88: val_loss did not improve from 0.79078\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.8642 - accuracy: 0.6281 - val_loss: 0.8181 - val_accuracy: 0.6264\n",
      "Epoch 89/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.8347 - accuracy: 0.6399\n",
      "Epoch 89: val_loss did not improve from 0.79078\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.8340 - accuracy: 0.6403 - val_loss: 0.9008 - val_accuracy: 0.6070\n",
      "Epoch 90/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.8653 - accuracy: 0.6356\n",
      "Epoch 90: val_loss did not improve from 0.79078\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.8654 - accuracy: 0.6350 - val_loss: 0.8387 - val_accuracy: 0.6202\n",
      "Epoch 91/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.8310 - accuracy: 0.6375\n",
      "Epoch 91: val_loss did not improve from 0.79078\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.8315 - accuracy: 0.6378 - val_loss: 0.8165 - val_accuracy: 0.6219\n",
      "Epoch 92/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.8649 - accuracy: 0.6286\n",
      "Epoch 92: val_loss did not improve from 0.79078\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.8650 - accuracy: 0.6284 - val_loss: 0.8140 - val_accuracy: 0.6324\n",
      "Epoch 93/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.8652 - accuracy: 0.6288\n",
      "Epoch 93: val_loss did not improve from 0.79078\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.8650 - accuracy: 0.6283 - val_loss: 0.8451 - val_accuracy: 0.6383\n",
      "Epoch 94/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.8666 - accuracy: 0.6217\n",
      "Epoch 94: val_loss did not improve from 0.79078\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.8668 - accuracy: 0.6220 - val_loss: 0.8520 - val_accuracy: 0.6694\n",
      "Epoch 95/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.8470 - accuracy: 0.6283\n",
      "Epoch 95: val_loss improved from 0.79078 to 0.77971, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.8464 - accuracy: 0.6286 - val_loss: 0.7797 - val_accuracy: 0.6536\n",
      "Epoch 96/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.8598 - accuracy: 0.6271\n",
      "Epoch 96: val_loss did not improve from 0.77971\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.8607 - accuracy: 0.6270 - val_loss: 0.7962 - val_accuracy: 0.6382\n",
      "Epoch 97/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.8443 - accuracy: 0.6270\n",
      "Epoch 97: val_loss improved from 0.77971 to 0.77187, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.8449 - accuracy: 0.6269 - val_loss: 0.7719 - val_accuracy: 0.6562\n",
      "Epoch 98/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.8200 - accuracy: 0.6390\n",
      "Epoch 98: val_loss did not improve from 0.77187\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.8194 - accuracy: 0.6393 - val_loss: 0.8585 - val_accuracy: 0.6246\n",
      "Epoch 99/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.8327 - accuracy: 0.6279\n",
      "Epoch 99: val_loss improved from 0.77187 to 0.76819, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.8324 - accuracy: 0.6284 - val_loss: 0.7682 - val_accuracy: 0.6387\n",
      "Epoch 100/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.8403 - accuracy: 0.6371\n",
      "Epoch 100: val_loss did not improve from 0.76819\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.8399 - accuracy: 0.6372 - val_loss: 0.7905 - val_accuracy: 0.6474\n",
      "Epoch 101/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.8233 - accuracy: 0.6416\n",
      "Epoch 101: val_loss improved from 0.76819 to 0.75466, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.8231 - accuracy: 0.6414 - val_loss: 0.7547 - val_accuracy: 0.6671\n",
      "Epoch 102/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.8108 - accuracy: 0.6407\n",
      "Epoch 102: val_loss improved from 0.75466 to 0.74652, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.8105 - accuracy: 0.6410 - val_loss: 0.7465 - val_accuracy: 0.6630\n",
      "Epoch 103/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.8753 - accuracy: 0.6166\n",
      "Epoch 103: val_loss did not improve from 0.74652\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.8749 - accuracy: 0.6167 - val_loss: 0.7613 - val_accuracy: 0.6603\n",
      "Epoch 104/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.8604 - accuracy: 0.6278\n",
      "Epoch 104: val_loss did not improve from 0.74652\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.8616 - accuracy: 0.6275 - val_loss: 0.8444 - val_accuracy: 0.6289\n",
      "Epoch 105/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.8438 - accuracy: 0.6290\n",
      "Epoch 105: val_loss improved from 0.74652 to 0.74512, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 19ms/step - loss: 0.8440 - accuracy: 0.6290 - val_loss: 0.7451 - val_accuracy: 0.6578\n",
      "Epoch 106/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.8162 - accuracy: 0.6384\n",
      "Epoch 106: val_loss did not improve from 0.74512\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.8163 - accuracy: 0.6387 - val_loss: 0.7801 - val_accuracy: 0.6620\n",
      "Epoch 107/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.8166 - accuracy: 0.6433\n",
      "Epoch 107: val_loss did not improve from 0.74512\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.8167 - accuracy: 0.6431 - val_loss: 0.7719 - val_accuracy: 0.6367\n",
      "Epoch 108/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.8406 - accuracy: 0.6373\n",
      "Epoch 108: val_loss did not improve from 0.74512\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.8404 - accuracy: 0.6373 - val_loss: 0.8483 - val_accuracy: 0.6364\n",
      "Epoch 109/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.8408 - accuracy: 0.6355\n",
      "Epoch 109: val_loss improved from 0.74512 to 0.72853, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.8407 - accuracy: 0.6358 - val_loss: 0.7285 - val_accuracy: 0.6706\n",
      "Epoch 110/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.8210 - accuracy: 0.6406\n",
      "Epoch 110: val_loss did not improve from 0.72853\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.8213 - accuracy: 0.6405 - val_loss: 0.7905 - val_accuracy: 0.6338\n",
      "Epoch 111/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.8394 - accuracy: 0.6312\n",
      "Epoch 111: val_loss did not improve from 0.72853\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.8404 - accuracy: 0.6309 - val_loss: 0.7695 - val_accuracy: 0.6551\n",
      "Epoch 112/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.8120 - accuracy: 0.6423\n",
      "Epoch 112: val_loss did not improve from 0.72853\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.8117 - accuracy: 0.6426 - val_loss: 0.7758 - val_accuracy: 0.6414\n",
      "Epoch 113/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.8025 - accuracy: 0.6461\n",
      "Epoch 113: val_loss did not improve from 0.72853\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.8031 - accuracy: 0.6456 - val_loss: 0.8417 - val_accuracy: 0.6565\n",
      "Epoch 114/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.8157 - accuracy: 0.6386\n",
      "Epoch 114: val_loss did not improve from 0.72853\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.8163 - accuracy: 0.6384 - val_loss: 0.7332 - val_accuracy: 0.6689\n",
      "Epoch 115/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.8292 - accuracy: 0.6387\n",
      "Epoch 115: val_loss did not improve from 0.72853\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.8290 - accuracy: 0.6386 - val_loss: 0.7322 - val_accuracy: 0.6694\n",
      "Epoch 116/1000\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.8021 - accuracy: 0.6516\n",
      "Epoch 116: val_loss did not improve from 0.72853\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.8022 - accuracy: 0.6512 - val_loss: 0.7398 - val_accuracy: 0.6610\n",
      "Epoch 117/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.7928 - accuracy: 0.6454\n",
      "Epoch 117: val_loss did not improve from 0.72853\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.7928 - accuracy: 0.6456 - val_loss: 0.7615 - val_accuracy: 0.6597\n",
      "Epoch 118/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.8151 - accuracy: 0.6495\n",
      "Epoch 118: val_loss did not improve from 0.72853\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.8148 - accuracy: 0.6494 - val_loss: 0.7310 - val_accuracy: 0.6725\n",
      "Epoch 119/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.7950 - accuracy: 0.6483\n",
      "Epoch 119: val_loss did not improve from 0.72853\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.7947 - accuracy: 0.6484 - val_loss: 0.7392 - val_accuracy: 0.6642\n",
      "Epoch 120/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.7797 - accuracy: 0.6560\n",
      "Epoch 120: val_loss did not improve from 0.72853\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.7792 - accuracy: 0.6563 - val_loss: 0.7448 - val_accuracy: 0.6494\n",
      "Epoch 121/1000\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.7877 - accuracy: 0.6501\n",
      "Epoch 121: val_loss improved from 0.72853 to 0.72148, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 19ms/step - loss: 0.7901 - accuracy: 0.6499 - val_loss: 0.7215 - val_accuracy: 0.6711\n",
      "Epoch 122/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.7961 - accuracy: 0.6479\n",
      "Epoch 122: val_loss did not improve from 0.72148\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.7958 - accuracy: 0.6481 - val_loss: 0.7430 - val_accuracy: 0.6665\n",
      "Epoch 123/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.7885 - accuracy: 0.6483\n",
      "Epoch 123: val_loss did not improve from 0.72148\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.7887 - accuracy: 0.6481 - val_loss: 0.7384 - val_accuracy: 0.6599\n",
      "Epoch 124/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.7895 - accuracy: 0.6535\n",
      "Epoch 124: val_loss did not improve from 0.72148\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.7901 - accuracy: 0.6534 - val_loss: 0.7440 - val_accuracy: 0.6731\n",
      "Epoch 125/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.8197 - accuracy: 0.6484\n",
      "Epoch 125: val_loss did not improve from 0.72148\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.8201 - accuracy: 0.6482 - val_loss: 0.7893 - val_accuracy: 0.6548\n",
      "Epoch 126/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.7873 - accuracy: 0.6470\n",
      "Epoch 126: val_loss did not improve from 0.72148\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.7875 - accuracy: 0.6471 - val_loss: 0.7547 - val_accuracy: 0.6706\n",
      "Epoch 127/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.7625 - accuracy: 0.6552\n",
      "Epoch 127: val_loss did not improve from 0.72148\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.7619 - accuracy: 0.6556 - val_loss: 0.7548 - val_accuracy: 0.6628\n",
      "Epoch 128/1000\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.7685 - accuracy: 0.6534\n",
      "Epoch 128: val_loss did not improve from 0.72148\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.7713 - accuracy: 0.6530 - val_loss: 0.8059 - val_accuracy: 0.6550\n",
      "Epoch 129/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.7719 - accuracy: 0.6601\n",
      "Epoch 129: val_loss did not improve from 0.72148\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.7718 - accuracy: 0.6599 - val_loss: 0.7294 - val_accuracy: 0.6644\n",
      "Epoch 130/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.7749 - accuracy: 0.6554\n",
      "Epoch 130: val_loss improved from 0.72148 to 0.68909, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.7749 - accuracy: 0.6554 - val_loss: 0.6891 - val_accuracy: 0.6884\n",
      "Epoch 131/1000\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.7778 - accuracy: 0.6573\n",
      "Epoch 131: val_loss did not improve from 0.68909\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.7782 - accuracy: 0.6559 - val_loss: 0.6962 - val_accuracy: 0.6972\n",
      "Epoch 132/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.7925 - accuracy: 0.6504\n",
      "Epoch 132: val_loss did not improve from 0.68909\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.7925 - accuracy: 0.6505 - val_loss: 0.7031 - val_accuracy: 0.6909\n",
      "Epoch 133/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.8315 - accuracy: 0.6372\n",
      "Epoch 133: val_loss improved from 0.68909 to 0.67097, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.8314 - accuracy: 0.6373 - val_loss: 0.6710 - val_accuracy: 0.6875\n",
      "Epoch 134/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.7510 - accuracy: 0.6582\n",
      "Epoch 134: val_loss did not improve from 0.67097\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.7516 - accuracy: 0.6580 - val_loss: 0.6832 - val_accuracy: 0.6866\n",
      "Epoch 135/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.8170 - accuracy: 0.6445\n",
      "Epoch 135: val_loss did not improve from 0.67097\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.8169 - accuracy: 0.6443 - val_loss: 0.7187 - val_accuracy: 0.6752\n",
      "Epoch 136/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.7732 - accuracy: 0.6618\n",
      "Epoch 136: val_loss did not improve from 0.67097\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.7727 - accuracy: 0.6621 - val_loss: 0.6837 - val_accuracy: 0.6843\n",
      "Epoch 137/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.7487 - accuracy: 0.6671\n",
      "Epoch 137: val_loss improved from 0.67097 to 0.66968, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 19ms/step - loss: 0.7493 - accuracy: 0.6667 - val_loss: 0.6697 - val_accuracy: 0.6934\n",
      "Epoch 138/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.7659 - accuracy: 0.6584\n",
      "Epoch 138: val_loss improved from 0.66968 to 0.66065, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.7655 - accuracy: 0.6586 - val_loss: 0.6606 - val_accuracy: 0.6952\n",
      "Epoch 139/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.7561 - accuracy: 0.6632\n",
      "Epoch 139: val_loss did not improve from 0.66065\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.7560 - accuracy: 0.6632 - val_loss: 0.6976 - val_accuracy: 0.6891\n",
      "Epoch 140/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.7537 - accuracy: 0.6670\n",
      "Epoch 140: val_loss did not improve from 0.66065\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.7536 - accuracy: 0.6670 - val_loss: 0.6927 - val_accuracy: 0.6873\n",
      "Epoch 141/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.7386 - accuracy: 0.6679\n",
      "Epoch 141: val_loss improved from 0.66065 to 0.63998, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 19ms/step - loss: 0.7384 - accuracy: 0.6679 - val_loss: 0.6400 - val_accuracy: 0.7184\n",
      "Epoch 142/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.7203 - accuracy: 0.6760\n",
      "Epoch 142: val_loss did not improve from 0.63998\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.7205 - accuracy: 0.6757 - val_loss: 0.6438 - val_accuracy: 0.7110\n",
      "Epoch 143/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.7268 - accuracy: 0.6697\n",
      "Epoch 143: val_loss did not improve from 0.63998\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.7268 - accuracy: 0.6697 - val_loss: 0.6754 - val_accuracy: 0.7017\n",
      "Epoch 144/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.7737 - accuracy: 0.6640\n",
      "Epoch 144: val_loss improved from 0.63998 to 0.62649, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.7743 - accuracy: 0.6641 - val_loss: 0.6265 - val_accuracy: 0.7048\n",
      "Epoch 145/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.7163 - accuracy: 0.6795\n",
      "Epoch 145: val_loss did not improve from 0.62649\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.7163 - accuracy: 0.6793 - val_loss: 0.6336 - val_accuracy: 0.7204\n",
      "Epoch 146/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.7030 - accuracy: 0.6836\n",
      "Epoch 146: val_loss did not improve from 0.62649\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.7028 - accuracy: 0.6838 - val_loss: 0.6795 - val_accuracy: 0.6920\n",
      "Epoch 147/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.7337 - accuracy: 0.6763\n",
      "Epoch 147: val_loss did not improve from 0.62649\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.7336 - accuracy: 0.6764 - val_loss: 0.6594 - val_accuracy: 0.6942\n",
      "Epoch 148/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.7174 - accuracy: 0.6805\n",
      "Epoch 148: val_loss did not improve from 0.62649\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.7175 - accuracy: 0.6804 - val_loss: 0.6356 - val_accuracy: 0.7160\n",
      "Epoch 149/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.7339 - accuracy: 0.6768\n",
      "Epoch 149: val_loss did not improve from 0.62649\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.7337 - accuracy: 0.6767 - val_loss: 0.7069 - val_accuracy: 0.6917\n",
      "Epoch 150/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.7113 - accuracy: 0.6807\n",
      "Epoch 150: val_loss did not improve from 0.62649\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.7119 - accuracy: 0.6808 - val_loss: 0.6420 - val_accuracy: 0.6987\n",
      "Epoch 151/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.7106 - accuracy: 0.6819\n",
      "Epoch 151: val_loss did not improve from 0.62649\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.7110 - accuracy: 0.6819 - val_loss: 0.6585 - val_accuracy: 0.7098\n",
      "Epoch 152/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.7217 - accuracy: 0.6800\n",
      "Epoch 152: val_loss improved from 0.62649 to 0.62184, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.7229 - accuracy: 0.6794 - val_loss: 0.6218 - val_accuracy: 0.7175\n",
      "Epoch 153/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.7190 - accuracy: 0.6786\n",
      "Epoch 153: val_loss did not improve from 0.62184\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.7190 - accuracy: 0.6782 - val_loss: 0.6516 - val_accuracy: 0.7032\n",
      "Epoch 154/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6889 - accuracy: 0.6932\n",
      "Epoch 154: val_loss did not improve from 0.62184\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.6899 - accuracy: 0.6932 - val_loss: 0.6226 - val_accuracy: 0.7172\n",
      "Epoch 155/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.7476 - accuracy: 0.6743\n",
      "Epoch 155: val_loss did not improve from 0.62184\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.7472 - accuracy: 0.6744 - val_loss: 1.0494 - val_accuracy: 0.6333\n",
      "Epoch 156/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.7549 - accuracy: 0.6705\n",
      "Epoch 156: val_loss did not improve from 0.62184\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.7556 - accuracy: 0.6700 - val_loss: 0.6923 - val_accuracy: 0.6904\n",
      "Epoch 157/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.7065 - accuracy: 0.6842\n",
      "Epoch 157: val_loss improved from 0.62184 to 0.56725, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.7060 - accuracy: 0.6844 - val_loss: 0.5673 - val_accuracy: 0.7469\n",
      "Epoch 158/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6881 - accuracy: 0.6882\n",
      "Epoch 158: val_loss did not improve from 0.56725\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.6890 - accuracy: 0.6877 - val_loss: 0.6360 - val_accuracy: 0.7194\n",
      "Epoch 159/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.7042 - accuracy: 0.6845\n",
      "Epoch 159: val_loss did not improve from 0.56725\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.7046 - accuracy: 0.6841 - val_loss: 0.6111 - val_accuracy: 0.7186\n",
      "Epoch 160/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6888 - accuracy: 0.6859\n",
      "Epoch 160: val_loss did not improve from 0.56725\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.6890 - accuracy: 0.6860 - val_loss: 0.6309 - val_accuracy: 0.7016\n",
      "Epoch 161/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.7378 - accuracy: 0.6786\n",
      "Epoch 161: val_loss did not improve from 0.56725\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.7380 - accuracy: 0.6786 - val_loss: 0.6006 - val_accuracy: 0.7256\n",
      "Epoch 162/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6826 - accuracy: 0.6957\n",
      "Epoch 162: val_loss did not improve from 0.56725\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.6824 - accuracy: 0.6958 - val_loss: 0.5884 - val_accuracy: 0.7406\n",
      "Epoch 163/1000\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.6843 - accuracy: 0.6899\n",
      "Epoch 163: val_loss did not improve from 0.56725\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.6855 - accuracy: 0.6898 - val_loss: 0.5889 - val_accuracy: 0.7204\n",
      "Epoch 164/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.7037 - accuracy: 0.6859\n",
      "Epoch 164: val_loss did not improve from 0.56725\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.7042 - accuracy: 0.6855 - val_loss: 0.6020 - val_accuracy: 0.7217\n",
      "Epoch 165/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6707 - accuracy: 0.6976\n",
      "Epoch 165: val_loss did not improve from 0.56725\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.6705 - accuracy: 0.6976 - val_loss: 0.6208 - val_accuracy: 0.7339\n",
      "Epoch 166/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6846 - accuracy: 0.6939\n",
      "Epoch 166: val_loss did not improve from 0.56725\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.6843 - accuracy: 0.6940 - val_loss: 0.6000 - val_accuracy: 0.7274\n",
      "Epoch 167/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6611 - accuracy: 0.7036\n",
      "Epoch 167: val_loss did not improve from 0.56725\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.6619 - accuracy: 0.7033 - val_loss: 0.7755 - val_accuracy: 0.6846\n",
      "Epoch 168/1000\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.9560 - accuracy: 0.6256\n",
      "Epoch 168: val_loss did not improve from 0.56725\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.9551 - accuracy: 0.6260 - val_loss: 0.8624 - val_accuracy: 0.6481\n",
      "Epoch 169/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.8465 - accuracy: 0.6391\n",
      "Epoch 169: val_loss did not improve from 0.56725\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.8464 - accuracy: 0.6392 - val_loss: 0.6996 - val_accuracy: 0.6988\n",
      "Epoch 170/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.7727 - accuracy: 0.6599\n",
      "Epoch 170: val_loss did not improve from 0.56725\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.7722 - accuracy: 0.6600 - val_loss: 0.6522 - val_accuracy: 0.7087\n",
      "Epoch 171/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.7248 - accuracy: 0.6782\n",
      "Epoch 171: val_loss did not improve from 0.56725\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.7247 - accuracy: 0.6783 - val_loss: 0.6553 - val_accuracy: 0.7038\n",
      "Epoch 172/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6821 - accuracy: 0.6919\n",
      "Epoch 172: val_loss did not improve from 0.56725\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.6821 - accuracy: 0.6919 - val_loss: 0.5955 - val_accuracy: 0.7321\n",
      "Epoch 173/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6843 - accuracy: 0.6966\n",
      "Epoch 173: val_loss did not improve from 0.56725\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.6849 - accuracy: 0.6964 - val_loss: 0.6113 - val_accuracy: 0.7302\n",
      "Epoch 174/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6929 - accuracy: 0.6909\n",
      "Epoch 174: val_loss did not improve from 0.56725\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.6931 - accuracy: 0.6907 - val_loss: 0.6639 - val_accuracy: 0.6962\n",
      "Epoch 175/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6628 - accuracy: 0.7024\n",
      "Epoch 175: val_loss did not improve from 0.56725\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.6633 - accuracy: 0.7021 - val_loss: 0.6032 - val_accuracy: 0.7239\n",
      "Epoch 176/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.7172 - accuracy: 0.6840\n",
      "Epoch 176: val_loss did not improve from 0.56725\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.7171 - accuracy: 0.6839 - val_loss: 0.6981 - val_accuracy: 0.6906\n",
      "Epoch 177/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.7095 - accuracy: 0.6882\n",
      "Epoch 177: val_loss did not improve from 0.56725\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.7092 - accuracy: 0.6885 - val_loss: 0.5782 - val_accuracy: 0.7499\n",
      "Epoch 178/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6745 - accuracy: 0.7028\n",
      "Epoch 178: val_loss did not improve from 0.56725\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.6757 - accuracy: 0.7024 - val_loss: 0.5839 - val_accuracy: 0.7357\n",
      "Epoch 179/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6785 - accuracy: 0.6927\n",
      "Epoch 179: val_loss did not improve from 0.56725\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.6781 - accuracy: 0.6927 - val_loss: 0.5833 - val_accuracy: 0.7473\n",
      "Epoch 180/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6623 - accuracy: 0.7013\n",
      "Epoch 180: val_loss did not improve from 0.56725\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.6620 - accuracy: 0.7014 - val_loss: 0.6017 - val_accuracy: 0.7240\n",
      "Epoch 181/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6597 - accuracy: 0.7059\n",
      "Epoch 181: val_loss did not improve from 0.56725\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.6600 - accuracy: 0.7054 - val_loss: 0.5847 - val_accuracy: 0.7265\n",
      "Epoch 182/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6512 - accuracy: 0.7041\n",
      "Epoch 182: val_loss improved from 0.56725 to 0.56542, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.6513 - accuracy: 0.7041 - val_loss: 0.5654 - val_accuracy: 0.7424\n",
      "Epoch 183/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6412 - accuracy: 0.7115\n",
      "Epoch 183: val_loss improved from 0.56542 to 0.54024, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.6408 - accuracy: 0.7118 - val_loss: 0.5402 - val_accuracy: 0.7499\n",
      "Epoch 184/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6487 - accuracy: 0.7088\n",
      "Epoch 184: val_loss did not improve from 0.54024\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.6485 - accuracy: 0.7091 - val_loss: 0.5706 - val_accuracy: 0.7463\n",
      "Epoch 185/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6552 - accuracy: 0.7113\n",
      "Epoch 185: val_loss did not improve from 0.54024\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.6552 - accuracy: 0.7114 - val_loss: 0.6289 - val_accuracy: 0.7135\n",
      "Epoch 186/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6939 - accuracy: 0.6947\n",
      "Epoch 186: val_loss did not improve from 0.54024\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.6933 - accuracy: 0.6950 - val_loss: 0.5941 - val_accuracy: 0.7277\n",
      "Epoch 187/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.7163 - accuracy: 0.6923\n",
      "Epoch 187: val_loss did not improve from 0.54024\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.7164 - accuracy: 0.6922 - val_loss: 0.6075 - val_accuracy: 0.7250\n",
      "Epoch 188/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6738 - accuracy: 0.6984\n",
      "Epoch 188: val_loss did not improve from 0.54024\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.6737 - accuracy: 0.6983 - val_loss: 0.5471 - val_accuracy: 0.7565\n",
      "Epoch 189/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6339 - accuracy: 0.7121\n",
      "Epoch 189: val_loss did not improve from 0.54024\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.6338 - accuracy: 0.7121 - val_loss: 0.5896 - val_accuracy: 0.7258\n",
      "Epoch 190/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6488 - accuracy: 0.7105\n",
      "Epoch 190: val_loss did not improve from 0.54024\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.6490 - accuracy: 0.7103 - val_loss: 0.6135 - val_accuracy: 0.7276\n",
      "Epoch 191/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6457 - accuracy: 0.7097\n",
      "Epoch 191: val_loss did not improve from 0.54024\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.6460 - accuracy: 0.7095 - val_loss: 0.5558 - val_accuracy: 0.7400\n",
      "Epoch 192/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6222 - accuracy: 0.7238\n",
      "Epoch 192: val_loss did not improve from 0.54024\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.6228 - accuracy: 0.7238 - val_loss: 0.5729 - val_accuracy: 0.7246\n",
      "Epoch 193/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.7801 - accuracy: 0.6725\n",
      "Epoch 193: val_loss did not improve from 0.54024\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.7792 - accuracy: 0.6730 - val_loss: 0.6306 - val_accuracy: 0.7318\n",
      "Epoch 194/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6426 - accuracy: 0.7127\n",
      "Epoch 194: val_loss did not improve from 0.54024\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.6426 - accuracy: 0.7129 - val_loss: 0.5544 - val_accuracy: 0.7583\n",
      "Epoch 195/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6341 - accuracy: 0.7155\n",
      "Epoch 195: val_loss did not improve from 0.54024\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.6338 - accuracy: 0.7155 - val_loss: 0.5534 - val_accuracy: 0.7495\n",
      "Epoch 196/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6598 - accuracy: 0.7082\n",
      "Epoch 196: val_loss did not improve from 0.54024\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.6597 - accuracy: 0.7082 - val_loss: 0.5489 - val_accuracy: 0.7487\n",
      "Epoch 197/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6405 - accuracy: 0.7163\n",
      "Epoch 197: val_loss did not improve from 0.54024\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.6409 - accuracy: 0.7161 - val_loss: 0.5581 - val_accuracy: 0.7373\n",
      "Epoch 198/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6214 - accuracy: 0.7175\n",
      "Epoch 198: val_loss did not improve from 0.54024\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.6216 - accuracy: 0.7174 - val_loss: 0.5814 - val_accuracy: 0.7269\n",
      "Epoch 199/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6189 - accuracy: 0.7225\n",
      "Epoch 199: val_loss did not improve from 0.54024\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.6191 - accuracy: 0.7223 - val_loss: 0.6166 - val_accuracy: 0.7263\n",
      "Epoch 200/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6781 - accuracy: 0.7047\n",
      "Epoch 200: val_loss did not improve from 0.54024\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.6781 - accuracy: 0.7047 - val_loss: 0.5495 - val_accuracy: 0.7479\n",
      "Epoch 201/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6291 - accuracy: 0.7258\n",
      "Epoch 201: val_loss did not improve from 0.54024\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.6289 - accuracy: 0.7258 - val_loss: 0.5420 - val_accuracy: 0.7649\n",
      "Epoch 202/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6370 - accuracy: 0.7171\n",
      "Epoch 202: val_loss did not improve from 0.54024\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.6367 - accuracy: 0.7171 - val_loss: 0.5406 - val_accuracy: 0.7558\n",
      "Epoch 203/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6166 - accuracy: 0.7242\n",
      "Epoch 203: val_loss improved from 0.54024 to 0.53050, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.6167 - accuracy: 0.7239 - val_loss: 0.5305 - val_accuracy: 0.7639\n",
      "Epoch 204/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6376 - accuracy: 0.7243\n",
      "Epoch 204: val_loss improved from 0.53050 to 0.52344, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.6373 - accuracy: 0.7243 - val_loss: 0.5234 - val_accuracy: 0.7607\n",
      "Epoch 205/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6073 - accuracy: 0.7265\n",
      "Epoch 205: val_loss improved from 0.52344 to 0.51638, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 19ms/step - loss: 0.6073 - accuracy: 0.7264 - val_loss: 0.5164 - val_accuracy: 0.7638\n",
      "Epoch 206/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5882 - accuracy: 0.7353\n",
      "Epoch 206: val_loss did not improve from 0.51638\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.5880 - accuracy: 0.7354 - val_loss: 0.5190 - val_accuracy: 0.7692\n",
      "Epoch 207/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6049 - accuracy: 0.7288\n",
      "Epoch 207: val_loss did not improve from 0.51638\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.6048 - accuracy: 0.7288 - val_loss: 0.5472 - val_accuracy: 0.7540\n",
      "Epoch 208/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6739 - accuracy: 0.7056\n",
      "Epoch 208: val_loss did not improve from 0.51638\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.6739 - accuracy: 0.7054 - val_loss: 0.5593 - val_accuracy: 0.7530\n",
      "Epoch 209/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6501 - accuracy: 0.7209\n",
      "Epoch 209: val_loss did not improve from 0.51638\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.6507 - accuracy: 0.7205 - val_loss: 0.6588 - val_accuracy: 0.7259\n",
      "Epoch 210/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.8167 - accuracy: 0.6623\n",
      "Epoch 210: val_loss did not improve from 0.51638\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.8163 - accuracy: 0.6624 - val_loss: 0.6060 - val_accuracy: 0.7386\n",
      "Epoch 211/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6293 - accuracy: 0.7258\n",
      "Epoch 211: val_loss did not improve from 0.51638\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.6287 - accuracy: 0.7262 - val_loss: 0.5442 - val_accuracy: 0.7549\n",
      "Epoch 212/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6323 - accuracy: 0.7160\n",
      "Epoch 212: val_loss improved from 0.51638 to 0.51284, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.6324 - accuracy: 0.7158 - val_loss: 0.5128 - val_accuracy: 0.7770\n",
      "Epoch 213/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5952 - accuracy: 0.7330\n",
      "Epoch 213: val_loss did not improve from 0.51284\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.5944 - accuracy: 0.7335 - val_loss: 0.5194 - val_accuracy: 0.7659\n",
      "Epoch 214/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5962 - accuracy: 0.7394\n",
      "Epoch 214: val_loss did not improve from 0.51284\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.5958 - accuracy: 0.7395 - val_loss: 0.5390 - val_accuracy: 0.7515\n",
      "Epoch 215/1000\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.6180 - accuracy: 0.7258\n",
      "Epoch 215: val_loss improved from 0.51284 to 0.48641, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 19ms/step - loss: 0.6168 - accuracy: 0.7258 - val_loss: 0.4864 - val_accuracy: 0.7947\n",
      "Epoch 216/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6184 - accuracy: 0.7268\n",
      "Epoch 216: val_loss did not improve from 0.48641\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.6183 - accuracy: 0.7269 - val_loss: 0.5243 - val_accuracy: 0.7609\n",
      "Epoch 217/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6116 - accuracy: 0.7330\n",
      "Epoch 217: val_loss did not improve from 0.48641\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.6112 - accuracy: 0.7333 - val_loss: 0.5802 - val_accuracy: 0.7378\n",
      "Epoch 218/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6371 - accuracy: 0.7223\n",
      "Epoch 218: val_loss did not improve from 0.48641\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.6368 - accuracy: 0.7222 - val_loss: 0.4865 - val_accuracy: 0.7885\n",
      "Epoch 219/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6104 - accuracy: 0.7333\n",
      "Epoch 219: val_loss did not improve from 0.48641\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.6102 - accuracy: 0.7333 - val_loss: 0.5056 - val_accuracy: 0.7756\n",
      "Epoch 220/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5862 - accuracy: 0.7409\n",
      "Epoch 220: val_loss did not improve from 0.48641\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.5860 - accuracy: 0.7410 - val_loss: 0.4895 - val_accuracy: 0.7823\n",
      "Epoch 221/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6112 - accuracy: 0.7341\n",
      "Epoch 221: val_loss did not improve from 0.48641\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.6110 - accuracy: 0.7340 - val_loss: 0.4925 - val_accuracy: 0.7959\n",
      "Epoch 222/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5940 - accuracy: 0.7392\n",
      "Epoch 222: val_loss did not improve from 0.48641\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.5948 - accuracy: 0.7386 - val_loss: 0.5195 - val_accuracy: 0.7754\n",
      "Epoch 223/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.7114 - accuracy: 0.7030\n",
      "Epoch 223: val_loss did not improve from 0.48641\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.7124 - accuracy: 0.7028 - val_loss: 0.5140 - val_accuracy: 0.7806\n",
      "Epoch 224/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5919 - accuracy: 0.7435\n",
      "Epoch 224: val_loss did not improve from 0.48641\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.5924 - accuracy: 0.7436 - val_loss: 0.5130 - val_accuracy: 0.7751\n",
      "Epoch 225/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5744 - accuracy: 0.7497\n",
      "Epoch 225: val_loss improved from 0.48641 to 0.48573, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 19ms/step - loss: 0.5743 - accuracy: 0.7498 - val_loss: 0.4857 - val_accuracy: 0.7812\n",
      "Epoch 226/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5795 - accuracy: 0.7516\n",
      "Epoch 226: val_loss improved from 0.48573 to 0.48067, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.5809 - accuracy: 0.7509 - val_loss: 0.4807 - val_accuracy: 0.7957\n",
      "Epoch 227/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5902 - accuracy: 0.7487\n",
      "Epoch 227: val_loss did not improve from 0.48067\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.5898 - accuracy: 0.7490 - val_loss: 0.5141 - val_accuracy: 0.7711\n",
      "Epoch 228/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6936 - accuracy: 0.7155\n",
      "Epoch 228: val_loss did not improve from 0.48067\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.6937 - accuracy: 0.7154 - val_loss: 0.7172 - val_accuracy: 0.7222\n",
      "Epoch 229/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.7010 - accuracy: 0.7083\n",
      "Epoch 229: val_loss did not improve from 0.48067\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.7006 - accuracy: 0.7084 - val_loss: 0.5557 - val_accuracy: 0.7646\n",
      "Epoch 230/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5915 - accuracy: 0.7406\n",
      "Epoch 230: val_loss did not improve from 0.48067\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.5907 - accuracy: 0.7410 - val_loss: 0.5156 - val_accuracy: 0.7639\n",
      "Epoch 231/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5874 - accuracy: 0.7409\n",
      "Epoch 231: val_loss did not improve from 0.48067\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.5867 - accuracy: 0.7413 - val_loss: 0.5046 - val_accuracy: 0.7827\n",
      "Epoch 232/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6156 - accuracy: 0.7329\n",
      "Epoch 232: val_loss did not improve from 0.48067\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.6156 - accuracy: 0.7329 - val_loss: 0.5096 - val_accuracy: 0.7701\n",
      "Epoch 233/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5583 - accuracy: 0.7543\n",
      "Epoch 233: val_loss improved from 0.48067 to 0.47333, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 19ms/step - loss: 0.5594 - accuracy: 0.7539 - val_loss: 0.4733 - val_accuracy: 0.7896\n",
      "Epoch 234/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5763 - accuracy: 0.7428\n",
      "Epoch 234: val_loss did not improve from 0.47333\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.5766 - accuracy: 0.7425 - val_loss: 0.4818 - val_accuracy: 0.7938\n",
      "Epoch 235/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5844 - accuracy: 0.7492\n",
      "Epoch 235: val_loss did not improve from 0.47333\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.5845 - accuracy: 0.7490 - val_loss: 0.4954 - val_accuracy: 0.7870\n",
      "Epoch 236/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5563 - accuracy: 0.7566\n",
      "Epoch 236: val_loss did not improve from 0.47333\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.5561 - accuracy: 0.7568 - val_loss: 0.4776 - val_accuracy: 0.7882\n",
      "Epoch 237/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5710 - accuracy: 0.7545\n",
      "Epoch 237: val_loss improved from 0.47333 to 0.46060, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.5707 - accuracy: 0.7548 - val_loss: 0.4606 - val_accuracy: 0.8141\n",
      "Epoch 238/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5708 - accuracy: 0.7568\n",
      "Epoch 238: val_loss did not improve from 0.46060\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.5727 - accuracy: 0.7563 - val_loss: 0.4811 - val_accuracy: 0.7971\n",
      "Epoch 239/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6195 - accuracy: 0.7372\n",
      "Epoch 239: val_loss did not improve from 0.46060\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.6190 - accuracy: 0.7374 - val_loss: 0.4875 - val_accuracy: 0.7928\n",
      "Epoch 240/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5678 - accuracy: 0.7575\n",
      "Epoch 240: val_loss did not improve from 0.46060\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.5680 - accuracy: 0.7574 - val_loss: 0.4822 - val_accuracy: 0.7980\n",
      "Epoch 241/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5584 - accuracy: 0.7586\n",
      "Epoch 241: val_loss did not improve from 0.46060\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.5583 - accuracy: 0.7587 - val_loss: 0.4721 - val_accuracy: 0.7971\n",
      "Epoch 242/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5821 - accuracy: 0.7521\n",
      "Epoch 242: val_loss improved from 0.46060 to 0.45073, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.5823 - accuracy: 0.7520 - val_loss: 0.4507 - val_accuracy: 0.8065\n",
      "Epoch 243/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5383 - accuracy: 0.7658\n",
      "Epoch 243: val_loss improved from 0.45073 to 0.44071, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.5384 - accuracy: 0.7657 - val_loss: 0.4407 - val_accuracy: 0.8124\n",
      "Epoch 244/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5293 - accuracy: 0.7704\n",
      "Epoch 244: val_loss improved from 0.44071 to 0.41295, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.5290 - accuracy: 0.7705 - val_loss: 0.4130 - val_accuracy: 0.8275\n",
      "Epoch 245/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5374 - accuracy: 0.7689\n",
      "Epoch 245: val_loss did not improve from 0.41295\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.5380 - accuracy: 0.7687 - val_loss: 0.4675 - val_accuracy: 0.7950\n",
      "Epoch 246/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5488 - accuracy: 0.7655\n",
      "Epoch 246: val_loss did not improve from 0.41295\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.5500 - accuracy: 0.7650 - val_loss: 0.4835 - val_accuracy: 0.7888\n",
      "Epoch 247/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5837 - accuracy: 0.7586\n",
      "Epoch 247: val_loss did not improve from 0.41295\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.5836 - accuracy: 0.7585 - val_loss: 0.4641 - val_accuracy: 0.8118\n",
      "Epoch 248/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6150 - accuracy: 0.7509\n",
      "Epoch 248: val_loss did not improve from 0.41295\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.6146 - accuracy: 0.7510 - val_loss: 0.4686 - val_accuracy: 0.7963\n",
      "Epoch 249/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5308 - accuracy: 0.7754\n",
      "Epoch 249: val_loss did not improve from 0.41295\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.5312 - accuracy: 0.7752 - val_loss: 0.4397 - val_accuracy: 0.8189\n",
      "Epoch 250/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5243 - accuracy: 0.7787\n",
      "Epoch 250: val_loss did not improve from 0.41295\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.5239 - accuracy: 0.7788 - val_loss: 0.4425 - val_accuracy: 0.8154\n",
      "Epoch 251/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5389 - accuracy: 0.7647\n",
      "Epoch 251: val_loss did not improve from 0.41295\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.5384 - accuracy: 0.7650 - val_loss: 0.4277 - val_accuracy: 0.8249\n",
      "Epoch 252/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5316 - accuracy: 0.7716\n",
      "Epoch 252: val_loss did not improve from 0.41295\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.5321 - accuracy: 0.7715 - val_loss: 0.4521 - val_accuracy: 0.8099\n",
      "Epoch 253/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5282 - accuracy: 0.7751\n",
      "Epoch 253: val_loss did not improve from 0.41295\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.5281 - accuracy: 0.7752 - val_loss: 0.4140 - val_accuracy: 0.8213\n",
      "Epoch 254/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5208 - accuracy: 0.7801\n",
      "Epoch 254: val_loss did not improve from 0.41295\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.5208 - accuracy: 0.7802 - val_loss: 0.4637 - val_accuracy: 0.7999\n",
      "Epoch 255/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5226 - accuracy: 0.7781\n",
      "Epoch 255: val_loss did not improve from 0.41295\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.5229 - accuracy: 0.7777 - val_loss: 0.4258 - val_accuracy: 0.8179\n",
      "Epoch 256/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5220 - accuracy: 0.7816\n",
      "Epoch 256: val_loss improved from 0.41295 to 0.40921, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.5224 - accuracy: 0.7812 - val_loss: 0.4092 - val_accuracy: 0.8255\n",
      "Epoch 257/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5779 - accuracy: 0.7636\n",
      "Epoch 257: val_loss did not improve from 0.40921\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.5784 - accuracy: 0.7634 - val_loss: 0.4654 - val_accuracy: 0.7961\n",
      "Epoch 258/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5623 - accuracy: 0.7627\n",
      "Epoch 258: val_loss did not improve from 0.40921\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.5622 - accuracy: 0.7625 - val_loss: 0.4354 - val_accuracy: 0.8275\n",
      "Epoch 259/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5416 - accuracy: 0.7760\n",
      "Epoch 259: val_loss did not improve from 0.40921\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.5416 - accuracy: 0.7760 - val_loss: 0.4639 - val_accuracy: 0.7972\n",
      "Epoch 260/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5328 - accuracy: 0.7740\n",
      "Epoch 260: val_loss improved from 0.40921 to 0.39341, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.5329 - accuracy: 0.7740 - val_loss: 0.3934 - val_accuracy: 0.8413\n",
      "Epoch 261/1000\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.5494 - accuracy: 0.7670\n",
      "Epoch 261: val_loss did not improve from 0.39341\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.5496 - accuracy: 0.7664 - val_loss: 0.4142 - val_accuracy: 0.8412\n",
      "Epoch 262/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5565 - accuracy: 0.7698\n",
      "Epoch 262: val_loss did not improve from 0.39341\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.5572 - accuracy: 0.7694 - val_loss: 0.4942 - val_accuracy: 0.7898\n",
      "Epoch 263/1000\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.5812 - accuracy: 0.7623\n",
      "Epoch 263: val_loss did not improve from 0.39341\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.5822 - accuracy: 0.7614 - val_loss: 0.4653 - val_accuracy: 0.8036\n",
      "Epoch 264/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5730 - accuracy: 0.7615\n",
      "Epoch 264: val_loss did not improve from 0.39341\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.5731 - accuracy: 0.7615 - val_loss: 0.4456 - val_accuracy: 0.8081\n",
      "Epoch 265/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5509 - accuracy: 0.7712\n",
      "Epoch 265: val_loss did not improve from 0.39341\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.5512 - accuracy: 0.7711 - val_loss: 0.4365 - val_accuracy: 0.8233\n",
      "Epoch 266/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5228 - accuracy: 0.7819\n",
      "Epoch 266: val_loss did not improve from 0.39341\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.5228 - accuracy: 0.7819 - val_loss: 0.4375 - val_accuracy: 0.8119\n",
      "Epoch 267/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5082 - accuracy: 0.7859\n",
      "Epoch 267: val_loss did not improve from 0.39341\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.5081 - accuracy: 0.7859 - val_loss: 0.4020 - val_accuracy: 0.8401\n",
      "Epoch 268/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5479 - accuracy: 0.7695\n",
      "Epoch 268: val_loss did not improve from 0.39341\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.5484 - accuracy: 0.7692 - val_loss: 0.4407 - val_accuracy: 0.8155\n",
      "Epoch 269/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5218 - accuracy: 0.7818\n",
      "Epoch 269: val_loss did not improve from 0.39341\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.5220 - accuracy: 0.7816 - val_loss: 0.4167 - val_accuracy: 0.8214\n",
      "Epoch 270/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5175 - accuracy: 0.7762\n",
      "Epoch 270: val_loss did not improve from 0.39341\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.5176 - accuracy: 0.7761 - val_loss: 0.4164 - val_accuracy: 0.8215\n",
      "Epoch 271/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5458 - accuracy: 0.7719\n",
      "Epoch 271: val_loss did not improve from 0.39341\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.5459 - accuracy: 0.7719 - val_loss: 0.4000 - val_accuracy: 0.8403\n",
      "Epoch 272/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5058 - accuracy: 0.7867\n",
      "Epoch 272: val_loss improved from 0.39341 to 0.38896, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.5057 - accuracy: 0.7868 - val_loss: 0.3890 - val_accuracy: 0.8370\n",
      "Epoch 273/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5243 - accuracy: 0.7833\n",
      "Epoch 273: val_loss did not improve from 0.38896\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.5236 - accuracy: 0.7836 - val_loss: 0.4015 - val_accuracy: 0.8311\n",
      "Epoch 274/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5501 - accuracy: 0.7733\n",
      "Epoch 274: val_loss did not improve from 0.38896\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.5499 - accuracy: 0.7734 - val_loss: 0.5616 - val_accuracy: 0.7759\n",
      "Epoch 275/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5628 - accuracy: 0.7661\n",
      "Epoch 275: val_loss did not improve from 0.38896\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.5627 - accuracy: 0.7661 - val_loss: 0.4482 - val_accuracy: 0.8149\n",
      "Epoch 276/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5035 - accuracy: 0.7864\n",
      "Epoch 276: val_loss improved from 0.38896 to 0.37880, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.5032 - accuracy: 0.7866 - val_loss: 0.3788 - val_accuracy: 0.8447\n",
      "Epoch 277/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5086 - accuracy: 0.7916\n",
      "Epoch 277: val_loss did not improve from 0.37880\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.5084 - accuracy: 0.7918 - val_loss: 0.3809 - val_accuracy: 0.8452\n",
      "Epoch 278/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5516 - accuracy: 0.7739\n",
      "Epoch 278: val_loss improved from 0.37880 to 0.36874, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.5517 - accuracy: 0.7736 - val_loss: 0.3687 - val_accuracy: 0.8514\n",
      "Epoch 279/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5040 - accuracy: 0.7945\n",
      "Epoch 279: val_loss improved from 0.36874 to 0.35813, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.5039 - accuracy: 0.7945 - val_loss: 0.3581 - val_accuracy: 0.8556\n",
      "Epoch 280/1000\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.4785 - accuracy: 0.8002\n",
      "Epoch 280: val_loss did not improve from 0.35813\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4785 - accuracy: 0.8002 - val_loss: 0.3735 - val_accuracy: 0.8475\n",
      "Epoch 281/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4778 - accuracy: 0.7966\n",
      "Epoch 281: val_loss did not improve from 0.35813\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4781 - accuracy: 0.7964 - val_loss: 0.3714 - val_accuracy: 0.8527\n",
      "Epoch 282/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4835 - accuracy: 0.7964\n",
      "Epoch 282: val_loss improved from 0.35813 to 0.35609, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.4834 - accuracy: 0.7963 - val_loss: 0.3561 - val_accuracy: 0.8542\n",
      "Epoch 283/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5183 - accuracy: 0.7937\n",
      "Epoch 283: val_loss did not improve from 0.35609\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.5187 - accuracy: 0.7935 - val_loss: 0.3936 - val_accuracy: 0.8418\n",
      "Epoch 284/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4900 - accuracy: 0.7973\n",
      "Epoch 284: val_loss improved from 0.35609 to 0.34982, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.4894 - accuracy: 0.7976 - val_loss: 0.3498 - val_accuracy: 0.8550\n",
      "Epoch 285/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4653 - accuracy: 0.8065\n",
      "Epoch 285: val_loss did not improve from 0.34982\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4651 - accuracy: 0.8065 - val_loss: 0.3508 - val_accuracy: 0.8535\n",
      "Epoch 286/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4911 - accuracy: 0.8006\n",
      "Epoch 286: val_loss did not improve from 0.34982\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4912 - accuracy: 0.8006 - val_loss: 0.4207 - val_accuracy: 0.8269\n",
      "Epoch 287/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4861 - accuracy: 0.7940\n",
      "Epoch 287: val_loss did not improve from 0.34982\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4861 - accuracy: 0.7940 - val_loss: 0.3750 - val_accuracy: 0.8469\n",
      "Epoch 288/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5122 - accuracy: 0.7900\n",
      "Epoch 288: val_loss improved from 0.34982 to 0.34705, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.5121 - accuracy: 0.7899 - val_loss: 0.3471 - val_accuracy: 0.8609\n",
      "Epoch 289/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4623 - accuracy: 0.8082\n",
      "Epoch 289: val_loss did not improve from 0.34705\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4627 - accuracy: 0.8079 - val_loss: 0.3743 - val_accuracy: 0.8446\n",
      "Epoch 290/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4864 - accuracy: 0.7945\n",
      "Epoch 290: val_loss did not improve from 0.34705\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4873 - accuracy: 0.7942 - val_loss: 0.3839 - val_accuracy: 0.8490\n",
      "Epoch 291/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5930 - accuracy: 0.7605\n",
      "Epoch 291: val_loss did not improve from 0.34705\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.5933 - accuracy: 0.7606 - val_loss: 0.4193 - val_accuracy: 0.8387\n",
      "Epoch 292/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4994 - accuracy: 0.7924\n",
      "Epoch 292: val_loss did not improve from 0.34705\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4989 - accuracy: 0.7926 - val_loss: 0.3838 - val_accuracy: 0.8387\n",
      "Epoch 293/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4712 - accuracy: 0.8035\n",
      "Epoch 293: val_loss did not improve from 0.34705\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.4708 - accuracy: 0.8037 - val_loss: 0.3647 - val_accuracy: 0.8504\n",
      "Epoch 294/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4532 - accuracy: 0.8127\n",
      "Epoch 294: val_loss improved from 0.34705 to 0.34627, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.4526 - accuracy: 0.8129 - val_loss: 0.3463 - val_accuracy: 0.8570\n",
      "Epoch 295/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4708 - accuracy: 0.8086\n",
      "Epoch 295: val_loss improved from 0.34627 to 0.34602, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.4710 - accuracy: 0.8084 - val_loss: 0.3460 - val_accuracy: 0.8677\n",
      "Epoch 296/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4610 - accuracy: 0.8125\n",
      "Epoch 296: val_loss improved from 0.34602 to 0.33254, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 19ms/step - loss: 0.4608 - accuracy: 0.8124 - val_loss: 0.3325 - val_accuracy: 0.8712\n",
      "Epoch 297/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4863 - accuracy: 0.7999\n",
      "Epoch 297: val_loss did not improve from 0.33254\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4860 - accuracy: 0.7999 - val_loss: 0.3503 - val_accuracy: 0.8609\n",
      "Epoch 298/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4598 - accuracy: 0.8133\n",
      "Epoch 298: val_loss did not improve from 0.33254\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4600 - accuracy: 0.8133 - val_loss: 0.3524 - val_accuracy: 0.8554\n",
      "Epoch 299/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4727 - accuracy: 0.8049\n",
      "Epoch 299: val_loss did not improve from 0.33254\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4726 - accuracy: 0.8050 - val_loss: 0.3427 - val_accuracy: 0.8635\n",
      "Epoch 300/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4732 - accuracy: 0.8052\n",
      "Epoch 300: val_loss did not improve from 0.33254\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4729 - accuracy: 0.8053 - val_loss: 0.3472 - val_accuracy: 0.8587\n",
      "Epoch 301/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4526 - accuracy: 0.8127\n",
      "Epoch 301: val_loss did not improve from 0.33254\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4538 - accuracy: 0.8122 - val_loss: 0.4281 - val_accuracy: 0.8308\n",
      "Epoch 302/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.9280 - accuracy: 0.6556\n",
      "Epoch 302: val_loss did not improve from 0.33254\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.9274 - accuracy: 0.6558 - val_loss: 0.5433 - val_accuracy: 0.7906\n",
      "Epoch 303/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6172 - accuracy: 0.7514\n",
      "Epoch 303: val_loss did not improve from 0.33254\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.6166 - accuracy: 0.7517 - val_loss: 0.5566 - val_accuracy: 0.7843\n",
      "Epoch 304/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5535 - accuracy: 0.7792\n",
      "Epoch 304: val_loss did not improve from 0.33254\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.5532 - accuracy: 0.7793 - val_loss: 0.3886 - val_accuracy: 0.8437\n",
      "Epoch 305/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5745 - accuracy: 0.7655\n",
      "Epoch 305: val_loss did not improve from 0.33254\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.5742 - accuracy: 0.7656 - val_loss: 0.4388 - val_accuracy: 0.8291\n",
      "Epoch 306/1000\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.5329 - accuracy: 0.7820\n",
      "Epoch 306: val_loss did not improve from 0.33254\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.5315 - accuracy: 0.7827 - val_loss: 0.4176 - val_accuracy: 0.8318\n",
      "Epoch 307/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5049 - accuracy: 0.7931\n",
      "Epoch 307: val_loss did not improve from 0.33254\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.5045 - accuracy: 0.7934 - val_loss: 0.3422 - val_accuracy: 0.8623\n",
      "Epoch 308/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4721 - accuracy: 0.8079\n",
      "Epoch 308: val_loss did not improve from 0.33254\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.4716 - accuracy: 0.8081 - val_loss: 0.3955 - val_accuracy: 0.8367\n",
      "Epoch 309/1000\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.4746 - accuracy: 0.8076\n",
      "Epoch 309: val_loss did not improve from 0.33254\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.4750 - accuracy: 0.8071 - val_loss: 0.3417 - val_accuracy: 0.8633\n",
      "Epoch 310/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4420 - accuracy: 0.8197\n",
      "Epoch 310: val_loss did not improve from 0.33254\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4430 - accuracy: 0.8196 - val_loss: 0.3926 - val_accuracy: 0.8408\n",
      "Epoch 311/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5228 - accuracy: 0.7892\n",
      "Epoch 311: val_loss did not improve from 0.33254\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.5249 - accuracy: 0.7884 - val_loss: 0.3736 - val_accuracy: 0.8488\n",
      "Epoch 312/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4772 - accuracy: 0.8032\n",
      "Epoch 312: val_loss did not improve from 0.33254\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4772 - accuracy: 0.8032 - val_loss: 0.3604 - val_accuracy: 0.8510\n",
      "Epoch 313/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4414 - accuracy: 0.8184\n",
      "Epoch 313: val_loss improved from 0.33254 to 0.30115, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 19ms/step - loss: 0.4410 - accuracy: 0.8187 - val_loss: 0.3012 - val_accuracy: 0.8765\n",
      "Epoch 314/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4381 - accuracy: 0.8165\n",
      "Epoch 314: val_loss did not improve from 0.30115\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4378 - accuracy: 0.8166 - val_loss: 0.3234 - val_accuracy: 0.8683\n",
      "Epoch 315/1000\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.4526 - accuracy: 0.8139\n",
      "Epoch 315: val_loss did not improve from 0.30115\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4514 - accuracy: 0.8147 - val_loss: 0.3366 - val_accuracy: 0.8595\n",
      "Epoch 316/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4981 - accuracy: 0.8022\n",
      "Epoch 316: val_loss did not improve from 0.30115\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4978 - accuracy: 0.8023 - val_loss: 0.3754 - val_accuracy: 0.8454\n",
      "Epoch 317/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4430 - accuracy: 0.8169\n",
      "Epoch 317: val_loss did not improve from 0.30115\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4434 - accuracy: 0.8168 - val_loss: 0.3408 - val_accuracy: 0.8576\n",
      "Epoch 318/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4363 - accuracy: 0.8232\n",
      "Epoch 318: val_loss did not improve from 0.30115\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4364 - accuracy: 0.8230 - val_loss: 0.3657 - val_accuracy: 0.8496\n",
      "Epoch 319/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4421 - accuracy: 0.8215\n",
      "Epoch 319: val_loss did not improve from 0.30115\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4419 - accuracy: 0.8216 - val_loss: 0.3818 - val_accuracy: 0.8441\n",
      "Epoch 320/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4323 - accuracy: 0.8260\n",
      "Epoch 320: val_loss did not improve from 0.30115\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4323 - accuracy: 0.8260 - val_loss: 0.3046 - val_accuracy: 0.8842\n",
      "Epoch 321/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4204 - accuracy: 0.8280\n",
      "Epoch 321: val_loss did not improve from 0.30115\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4205 - accuracy: 0.8279 - val_loss: 0.3172 - val_accuracy: 0.8704\n",
      "Epoch 322/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4357 - accuracy: 0.8224\n",
      "Epoch 322: val_loss did not improve from 0.30115\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4363 - accuracy: 0.8221 - val_loss: 0.3549 - val_accuracy: 0.8566\n",
      "Epoch 323/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5452 - accuracy: 0.7878\n",
      "Epoch 323: val_loss did not improve from 0.30115\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.5448 - accuracy: 0.7880 - val_loss: 0.3226 - val_accuracy: 0.8683\n",
      "Epoch 324/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4305 - accuracy: 0.8274\n",
      "Epoch 324: val_loss did not improve from 0.30115\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4305 - accuracy: 0.8275 - val_loss: 0.3396 - val_accuracy: 0.8718\n",
      "Epoch 325/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4250 - accuracy: 0.8300\n",
      "Epoch 325: val_loss did not improve from 0.30115\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4254 - accuracy: 0.8299 - val_loss: 0.3016 - val_accuracy: 0.8766\n",
      "Epoch 326/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4262 - accuracy: 0.8307\n",
      "Epoch 326: val_loss did not improve from 0.30115\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4260 - accuracy: 0.8308 - val_loss: 0.3335 - val_accuracy: 0.8646\n",
      "Epoch 327/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4358 - accuracy: 0.8240\n",
      "Epoch 327: val_loss did not improve from 0.30115\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4357 - accuracy: 0.8240 - val_loss: 0.3102 - val_accuracy: 0.8699\n",
      "Epoch 328/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4382 - accuracy: 0.8253\n",
      "Epoch 328: val_loss did not improve from 0.30115\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4378 - accuracy: 0.8255 - val_loss: 0.3387 - val_accuracy: 0.8660\n",
      "Epoch 329/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4390 - accuracy: 0.8236\n",
      "Epoch 329: val_loss did not improve from 0.30115\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4395 - accuracy: 0.8233 - val_loss: 0.3185 - val_accuracy: 0.8751\n",
      "Epoch 330/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4174 - accuracy: 0.8313\n",
      "Epoch 330: val_loss improved from 0.30115 to 0.29957, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.4170 - accuracy: 0.8314 - val_loss: 0.2996 - val_accuracy: 0.8851\n",
      "Epoch 331/1000\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.4584 - accuracy: 0.8201\n",
      "Epoch 331: val_loss did not improve from 0.29957\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4584 - accuracy: 0.8201 - val_loss: 0.3494 - val_accuracy: 0.8625\n",
      "Epoch 332/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4430 - accuracy: 0.8224\n",
      "Epoch 332: val_loss did not improve from 0.29957\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4429 - accuracy: 0.8225 - val_loss: 0.3817 - val_accuracy: 0.8514\n",
      "Epoch 333/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4538 - accuracy: 0.8190\n",
      "Epoch 333: val_loss did not improve from 0.29957\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4537 - accuracy: 0.8191 - val_loss: 0.3762 - val_accuracy: 0.8586\n",
      "Epoch 334/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4328 - accuracy: 0.8237\n",
      "Epoch 334: val_loss improved from 0.29957 to 0.29089, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.4324 - accuracy: 0.8239 - val_loss: 0.2909 - val_accuracy: 0.8889\n",
      "Epoch 335/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4128 - accuracy: 0.8323\n",
      "Epoch 335: val_loss did not improve from 0.29089\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4128 - accuracy: 0.8323 - val_loss: 0.2915 - val_accuracy: 0.8769\n",
      "Epoch 336/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4079 - accuracy: 0.8349\n",
      "Epoch 336: val_loss did not improve from 0.29089\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4080 - accuracy: 0.8348 - val_loss: 0.3135 - val_accuracy: 0.8716\n",
      "Epoch 337/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5005 - accuracy: 0.8039\n",
      "Epoch 337: val_loss did not improve from 0.29089\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.5001 - accuracy: 0.8042 - val_loss: 0.3922 - val_accuracy: 0.8398\n",
      "Epoch 338/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4346 - accuracy: 0.8186\n",
      "Epoch 338: val_loss did not improve from 0.29089\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4349 - accuracy: 0.8185 - val_loss: 0.3094 - val_accuracy: 0.8769\n",
      "Epoch 339/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4368 - accuracy: 0.8230\n",
      "Epoch 339: val_loss did not improve from 0.29089\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4366 - accuracy: 0.8232 - val_loss: 0.2927 - val_accuracy: 0.8782\n",
      "Epoch 340/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4429 - accuracy: 0.8220\n",
      "Epoch 340: val_loss did not improve from 0.29089\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4433 - accuracy: 0.8218 - val_loss: 0.3058 - val_accuracy: 0.8759\n",
      "Epoch 341/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4087 - accuracy: 0.8379\n",
      "Epoch 341: val_loss did not improve from 0.29089\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4085 - accuracy: 0.8381 - val_loss: 0.2958 - val_accuracy: 0.8866\n",
      "Epoch 342/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4899 - accuracy: 0.8113\n",
      "Epoch 342: val_loss did not improve from 0.29089\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4901 - accuracy: 0.8113 - val_loss: 0.3387 - val_accuracy: 0.8657\n",
      "Epoch 343/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4256 - accuracy: 0.8285\n",
      "Epoch 343: val_loss did not improve from 0.29089\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4255 - accuracy: 0.8285 - val_loss: 0.2929 - val_accuracy: 0.8850\n",
      "Epoch 344/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4405 - accuracy: 0.8226\n",
      "Epoch 344: val_loss did not improve from 0.29089\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4406 - accuracy: 0.8225 - val_loss: 0.3172 - val_accuracy: 0.8669\n",
      "Epoch 345/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4488 - accuracy: 0.8199\n",
      "Epoch 345: val_loss did not improve from 0.29089\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4488 - accuracy: 0.8200 - val_loss: 0.5934 - val_accuracy: 0.7904\n",
      "Epoch 346/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4428 - accuracy: 0.8267\n",
      "Epoch 346: val_loss did not improve from 0.29089\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4431 - accuracy: 0.8266 - val_loss: 0.3559 - val_accuracy: 0.8580\n",
      "Epoch 347/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4298 - accuracy: 0.8314\n",
      "Epoch 347: val_loss did not improve from 0.29089\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4297 - accuracy: 0.8314 - val_loss: 0.3348 - val_accuracy: 0.8712\n",
      "Epoch 348/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4178 - accuracy: 0.8291\n",
      "Epoch 348: val_loss did not improve from 0.29089\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4175 - accuracy: 0.8292 - val_loss: 0.3153 - val_accuracy: 0.8782\n",
      "Epoch 349/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4206 - accuracy: 0.8293\n",
      "Epoch 349: val_loss improved from 0.29089 to 0.28237, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.4206 - accuracy: 0.8293 - val_loss: 0.2824 - val_accuracy: 0.8879\n",
      "Epoch 350/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4124 - accuracy: 0.8325\n",
      "Epoch 350: val_loss did not improve from 0.28237\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4120 - accuracy: 0.8327 - val_loss: 0.2918 - val_accuracy: 0.8833\n",
      "Epoch 351/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4059 - accuracy: 0.8354\n",
      "Epoch 351: val_loss improved from 0.28237 to 0.27367, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.4064 - accuracy: 0.8353 - val_loss: 0.2737 - val_accuracy: 0.8919\n",
      "Epoch 352/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3993 - accuracy: 0.8389\n",
      "Epoch 352: val_loss did not improve from 0.27367\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3993 - accuracy: 0.8390 - val_loss: 0.3097 - val_accuracy: 0.8749\n",
      "Epoch 353/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4001 - accuracy: 0.8388\n",
      "Epoch 353: val_loss did not improve from 0.27367\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3999 - accuracy: 0.8389 - val_loss: 0.2834 - val_accuracy: 0.8876\n",
      "Epoch 354/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3810 - accuracy: 0.8472\n",
      "Epoch 354: val_loss did not improve from 0.27367\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3811 - accuracy: 0.8472 - val_loss: 0.2829 - val_accuracy: 0.8849\n",
      "Epoch 355/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4121 - accuracy: 0.8356\n",
      "Epoch 355: val_loss did not improve from 0.27367\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4129 - accuracy: 0.8352 - val_loss: 0.3103 - val_accuracy: 0.8750\n",
      "Epoch 356/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4171 - accuracy: 0.8359\n",
      "Epoch 356: val_loss did not improve from 0.27367\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.4171 - accuracy: 0.8359 - val_loss: 0.3622 - val_accuracy: 0.8532\n",
      "Epoch 357/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4312 - accuracy: 0.8293\n",
      "Epoch 357: val_loss did not improve from 0.27367\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.4309 - accuracy: 0.8296 - val_loss: 0.3123 - val_accuracy: 0.8813\n",
      "Epoch 358/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3985 - accuracy: 0.8403\n",
      "Epoch 358: val_loss did not improve from 0.27367\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3986 - accuracy: 0.8403 - val_loss: 0.2888 - val_accuracy: 0.8853\n",
      "Epoch 359/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4824 - accuracy: 0.8151\n",
      "Epoch 359: val_loss did not improve from 0.27367\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4826 - accuracy: 0.8149 - val_loss: 0.3181 - val_accuracy: 0.8745\n",
      "Epoch 360/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4161 - accuracy: 0.8359\n",
      "Epoch 360: val_loss improved from 0.27367 to 0.27057, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 19ms/step - loss: 0.4161 - accuracy: 0.8357 - val_loss: 0.2706 - val_accuracy: 0.8903\n",
      "Epoch 361/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3963 - accuracy: 0.8423\n",
      "Epoch 361: val_loss did not improve from 0.27057\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3960 - accuracy: 0.8425 - val_loss: 0.2784 - val_accuracy: 0.8875\n",
      "Epoch 362/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3829 - accuracy: 0.8444\n",
      "Epoch 362: val_loss did not improve from 0.27057\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.3831 - accuracy: 0.8443 - val_loss: 0.2743 - val_accuracy: 0.8912\n",
      "Epoch 363/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4066 - accuracy: 0.8394\n",
      "Epoch 363: val_loss did not improve from 0.27057\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4067 - accuracy: 0.8393 - val_loss: 0.3055 - val_accuracy: 0.8787\n",
      "Epoch 364/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3797 - accuracy: 0.8473\n",
      "Epoch 364: val_loss improved from 0.27057 to 0.25704, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 19ms/step - loss: 0.3801 - accuracy: 0.8471 - val_loss: 0.2570 - val_accuracy: 0.8949\n",
      "Epoch 365/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3790 - accuracy: 0.8522\n",
      "Epoch 365: val_loss did not improve from 0.25704\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3791 - accuracy: 0.8521 - val_loss: 0.2790 - val_accuracy: 0.8897\n",
      "Epoch 366/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3739 - accuracy: 0.8489\n",
      "Epoch 366: val_loss improved from 0.25704 to 0.25538, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.3735 - accuracy: 0.8491 - val_loss: 0.2554 - val_accuracy: 0.8950\n",
      "Epoch 367/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3920 - accuracy: 0.8444\n",
      "Epoch 367: val_loss did not improve from 0.25538\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3923 - accuracy: 0.8442 - val_loss: 0.2697 - val_accuracy: 0.8943\n",
      "Epoch 368/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3984 - accuracy: 0.8404\n",
      "Epoch 368: val_loss did not improve from 0.25538\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3984 - accuracy: 0.8404 - val_loss: 0.2838 - val_accuracy: 0.8880\n",
      "Epoch 369/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4138 - accuracy: 0.8339\n",
      "Epoch 369: val_loss did not improve from 0.25538\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4139 - accuracy: 0.8337 - val_loss: 0.2660 - val_accuracy: 0.8934\n",
      "Epoch 370/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3942 - accuracy: 0.8416\n",
      "Epoch 370: val_loss did not improve from 0.25538\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3940 - accuracy: 0.8417 - val_loss: 0.2954 - val_accuracy: 0.8854\n",
      "Epoch 371/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3803 - accuracy: 0.8447\n",
      "Epoch 371: val_loss did not improve from 0.25538\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3800 - accuracy: 0.8449 - val_loss: 0.2651 - val_accuracy: 0.8957\n",
      "Epoch 372/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3633 - accuracy: 0.8530\n",
      "Epoch 372: val_loss improved from 0.25538 to 0.23732, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 19ms/step - loss: 0.3635 - accuracy: 0.8529 - val_loss: 0.2373 - val_accuracy: 0.9105\n",
      "Epoch 373/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3659 - accuracy: 0.8523\n",
      "Epoch 373: val_loss did not improve from 0.23732\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3659 - accuracy: 0.8524 - val_loss: 0.2818 - val_accuracy: 0.8935\n",
      "Epoch 374/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4096 - accuracy: 0.8354\n",
      "Epoch 374: val_loss did not improve from 0.23732\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4099 - accuracy: 0.8353 - val_loss: 0.3065 - val_accuracy: 0.8809\n",
      "Epoch 375/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4127 - accuracy: 0.8386\n",
      "Epoch 375: val_loss did not improve from 0.23732\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4130 - accuracy: 0.8386 - val_loss: 0.2612 - val_accuracy: 0.9025\n",
      "Epoch 376/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4048 - accuracy: 0.8385\n",
      "Epoch 376: val_loss did not improve from 0.23732\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4046 - accuracy: 0.8385 - val_loss: 0.2688 - val_accuracy: 0.8997\n",
      "Epoch 377/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4166 - accuracy: 0.8369\n",
      "Epoch 377: val_loss did not improve from 0.23732\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4168 - accuracy: 0.8369 - val_loss: 0.3409 - val_accuracy: 0.8698\n",
      "Epoch 378/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3877 - accuracy: 0.8442\n",
      "Epoch 378: val_loss did not improve from 0.23732\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3877 - accuracy: 0.8443 - val_loss: 0.2593 - val_accuracy: 0.8955\n",
      "Epoch 379/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3785 - accuracy: 0.8481\n",
      "Epoch 379: val_loss did not improve from 0.23732\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3788 - accuracy: 0.8480 - val_loss: 0.4508 - val_accuracy: 0.8195\n",
      "Epoch 380/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4888 - accuracy: 0.8156\n",
      "Epoch 380: val_loss did not improve from 0.23732\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4885 - accuracy: 0.8156 - val_loss: 0.3683 - val_accuracy: 0.8560\n",
      "Epoch 381/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3936 - accuracy: 0.8430\n",
      "Epoch 381: val_loss did not improve from 0.23732\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3935 - accuracy: 0.8430 - val_loss: 0.2827 - val_accuracy: 0.8792\n",
      "Epoch 382/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3695 - accuracy: 0.8531\n",
      "Epoch 382: val_loss did not improve from 0.23732\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3693 - accuracy: 0.8532 - val_loss: 0.2491 - val_accuracy: 0.9063\n",
      "Epoch 383/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3659 - accuracy: 0.8514\n",
      "Epoch 383: val_loss did not improve from 0.23732\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3656 - accuracy: 0.8516 - val_loss: 0.2446 - val_accuracy: 0.9028\n",
      "Epoch 384/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3613 - accuracy: 0.8570\n",
      "Epoch 384: val_loss did not improve from 0.23732\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3611 - accuracy: 0.8571 - val_loss: 0.3165 - val_accuracy: 0.8825\n",
      "Epoch 385/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3753 - accuracy: 0.8504\n",
      "Epoch 385: val_loss improved from 0.23732 to 0.22497, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.3756 - accuracy: 0.8504 - val_loss: 0.2250 - val_accuracy: 0.9133\n",
      "Epoch 386/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3686 - accuracy: 0.8538\n",
      "Epoch 386: val_loss did not improve from 0.22497\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3697 - accuracy: 0.8531 - val_loss: 0.2488 - val_accuracy: 0.9086\n",
      "Epoch 387/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4623 - accuracy: 0.8199\n",
      "Epoch 387: val_loss did not improve from 0.22497\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4619 - accuracy: 0.8201 - val_loss: 0.3112 - val_accuracy: 0.8830\n",
      "Epoch 388/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4390 - accuracy: 0.8306\n",
      "Epoch 388: val_loss did not improve from 0.22497\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4387 - accuracy: 0.8307 - val_loss: 0.2944 - val_accuracy: 0.8932\n",
      "Epoch 389/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3796 - accuracy: 0.8487\n",
      "Epoch 389: val_loss did not improve from 0.22497\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3795 - accuracy: 0.8488 - val_loss: 0.2534 - val_accuracy: 0.8982\n",
      "Epoch 390/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3728 - accuracy: 0.8502\n",
      "Epoch 390: val_loss did not improve from 0.22497\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3725 - accuracy: 0.8503 - val_loss: 0.2369 - val_accuracy: 0.9040\n",
      "Epoch 391/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3654 - accuracy: 0.8543\n",
      "Epoch 391: val_loss did not improve from 0.22497\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3658 - accuracy: 0.8541 - val_loss: 0.3118 - val_accuracy: 0.8815\n",
      "Epoch 392/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4402 - accuracy: 0.8349\n",
      "Epoch 392: val_loss did not improve from 0.22497\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4397 - accuracy: 0.8352 - val_loss: 0.2510 - val_accuracy: 0.9040\n",
      "Epoch 393/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3779 - accuracy: 0.8504\n",
      "Epoch 393: val_loss did not improve from 0.22497\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3786 - accuracy: 0.8503 - val_loss: 0.3130 - val_accuracy: 0.8785\n",
      "Epoch 394/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3774 - accuracy: 0.8481\n",
      "Epoch 394: val_loss did not improve from 0.22497\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3774 - accuracy: 0.8481 - val_loss: 0.2547 - val_accuracy: 0.8974\n",
      "Epoch 395/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3854 - accuracy: 0.8447\n",
      "Epoch 395: val_loss did not improve from 0.22497\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3853 - accuracy: 0.8447 - val_loss: 0.2507 - val_accuracy: 0.9055\n",
      "Epoch 396/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3841 - accuracy: 0.8454\n",
      "Epoch 396: val_loss did not improve from 0.22497\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.3848 - accuracy: 0.8451 - val_loss: 0.2629 - val_accuracy: 0.8954\n",
      "Epoch 397/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3606 - accuracy: 0.8568\n",
      "Epoch 397: val_loss did not improve from 0.22497\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3607 - accuracy: 0.8566 - val_loss: 0.2463 - val_accuracy: 0.9047\n",
      "Epoch 398/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3660 - accuracy: 0.8553\n",
      "Epoch 398: val_loss did not improve from 0.22497\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3657 - accuracy: 0.8554 - val_loss: 0.2477 - val_accuracy: 0.9005\n",
      "Epoch 399/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4865 - accuracy: 0.8215\n",
      "Epoch 399: val_loss did not improve from 0.22497\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4861 - accuracy: 0.8217 - val_loss: 0.2667 - val_accuracy: 0.9067\n",
      "Epoch 400/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3611 - accuracy: 0.8583\n",
      "Epoch 400: val_loss did not improve from 0.22497\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3611 - accuracy: 0.8583 - val_loss: 0.2474 - val_accuracy: 0.9023\n",
      "Epoch 401/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3694 - accuracy: 0.8550\n",
      "Epoch 401: val_loss did not improve from 0.22497\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3693 - accuracy: 0.8550 - val_loss: 0.2291 - val_accuracy: 0.9095\n",
      "Epoch 402/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3586 - accuracy: 0.8589\n",
      "Epoch 402: val_loss did not improve from 0.22497\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3589 - accuracy: 0.8587 - val_loss: 0.3301 - val_accuracy: 0.8865\n",
      "Epoch 403/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3651 - accuracy: 0.8533\n",
      "Epoch 403: val_loss did not improve from 0.22497\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3653 - accuracy: 0.8533 - val_loss: 0.2444 - val_accuracy: 0.9107\n",
      "Epoch 404/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3848 - accuracy: 0.8493\n",
      "Epoch 404: val_loss improved from 0.22497 to 0.22290, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 19ms/step - loss: 0.3851 - accuracy: 0.8491 - val_loss: 0.2229 - val_accuracy: 0.9159\n",
      "Epoch 405/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3774 - accuracy: 0.8559\n",
      "Epoch 405: val_loss did not improve from 0.22290\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3770 - accuracy: 0.8560 - val_loss: 0.2420 - val_accuracy: 0.9034\n",
      "Epoch 406/1000\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.3769 - accuracy: 0.8518\n",
      "Epoch 406: val_loss did not improve from 0.22290\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3769 - accuracy: 0.8518 - val_loss: 0.2446 - val_accuracy: 0.9051\n",
      "Epoch 407/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3451 - accuracy: 0.8634\n",
      "Epoch 407: val_loss did not improve from 0.22290\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3448 - accuracy: 0.8636 - val_loss: 0.2471 - val_accuracy: 0.8956\n",
      "Epoch 408/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3331 - accuracy: 0.8663\n",
      "Epoch 408: val_loss did not improve from 0.22290\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3328 - accuracy: 0.8664 - val_loss: 0.2234 - val_accuracy: 0.9095\n",
      "Epoch 409/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3406 - accuracy: 0.8640\n",
      "Epoch 409: val_loss did not improve from 0.22290\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3415 - accuracy: 0.8637 - val_loss: 0.2341 - val_accuracy: 0.9093\n",
      "Epoch 410/1000\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.3493 - accuracy: 0.8635\n",
      "Epoch 410: val_loss did not improve from 0.22290\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.3510 - accuracy: 0.8627 - val_loss: 0.2296 - val_accuracy: 0.9067\n",
      "Epoch 411/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3740 - accuracy: 0.8481\n",
      "Epoch 411: val_loss did not improve from 0.22290\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3743 - accuracy: 0.8481 - val_loss: 0.2785 - val_accuracy: 0.8930\n",
      "Epoch 412/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4066 - accuracy: 0.8399\n",
      "Epoch 412: val_loss did not improve from 0.22290\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4066 - accuracy: 0.8400 - val_loss: 0.2917 - val_accuracy: 0.8887\n",
      "Epoch 413/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3883 - accuracy: 0.8471\n",
      "Epoch 413: val_loss did not improve from 0.22290\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3885 - accuracy: 0.8468 - val_loss: 0.2519 - val_accuracy: 0.8944\n",
      "Epoch 414/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4111 - accuracy: 0.8411\n",
      "Epoch 414: val_loss did not improve from 0.22290\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4111 - accuracy: 0.8412 - val_loss: 0.3226 - val_accuracy: 0.8777\n",
      "Epoch 415/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3601 - accuracy: 0.8588\n",
      "Epoch 415: val_loss did not improve from 0.22290\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3601 - accuracy: 0.8587 - val_loss: 0.2382 - val_accuracy: 0.9107\n",
      "Epoch 416/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3582 - accuracy: 0.8552\n",
      "Epoch 416: val_loss did not improve from 0.22290\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3585 - accuracy: 0.8553 - val_loss: 0.2407 - val_accuracy: 0.9068\n",
      "Epoch 417/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3735 - accuracy: 0.8531\n",
      "Epoch 417: val_loss improved from 0.22290 to 0.21926, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.3730 - accuracy: 0.8533 - val_loss: 0.2193 - val_accuracy: 0.9128\n",
      "Epoch 418/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3540 - accuracy: 0.8619\n",
      "Epoch 418: val_loss did not improve from 0.21926\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3539 - accuracy: 0.8620 - val_loss: 0.2349 - val_accuracy: 0.9081\n",
      "Epoch 419/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3725 - accuracy: 0.8545\n",
      "Epoch 419: val_loss did not improve from 0.21926\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3727 - accuracy: 0.8544 - val_loss: 0.2502 - val_accuracy: 0.9100\n",
      "Epoch 420/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3491 - accuracy: 0.8621\n",
      "Epoch 420: val_loss did not improve from 0.21926\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.3489 - accuracy: 0.8622 - val_loss: 0.2232 - val_accuracy: 0.9130\n",
      "Epoch 421/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3702 - accuracy: 0.8524\n",
      "Epoch 421: val_loss did not improve from 0.21926\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3699 - accuracy: 0.8525 - val_loss: 0.2437 - val_accuracy: 0.9036\n",
      "Epoch 422/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3705 - accuracy: 0.8541\n",
      "Epoch 422: val_loss did not improve from 0.21926\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3702 - accuracy: 0.8543 - val_loss: 0.2247 - val_accuracy: 0.9111\n",
      "Epoch 423/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3392 - accuracy: 0.8674\n",
      "Epoch 423: val_loss did not improve from 0.21926\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3391 - accuracy: 0.8674 - val_loss: 0.2346 - val_accuracy: 0.9066\n",
      "Epoch 424/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3319 - accuracy: 0.8682\n",
      "Epoch 424: val_loss did not improve from 0.21926\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.3317 - accuracy: 0.8683 - val_loss: 0.2527 - val_accuracy: 0.8975\n",
      "Epoch 425/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4101 - accuracy: 0.8420\n",
      "Epoch 425: val_loss did not improve from 0.21926\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4102 - accuracy: 0.8419 - val_loss: 0.2431 - val_accuracy: 0.9013\n",
      "Epoch 426/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3326 - accuracy: 0.8687\n",
      "Epoch 426: val_loss did not improve from 0.21926\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3325 - accuracy: 0.8689 - val_loss: 0.2361 - val_accuracy: 0.9094\n",
      "Epoch 427/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3360 - accuracy: 0.8649\n",
      "Epoch 427: val_loss did not improve from 0.21926\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3362 - accuracy: 0.8648 - val_loss: 0.2487 - val_accuracy: 0.9041\n",
      "Epoch 428/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3287 - accuracy: 0.8698\n",
      "Epoch 428: val_loss improved from 0.21926 to 0.21922, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.3283 - accuracy: 0.8700 - val_loss: 0.2192 - val_accuracy: 0.9115\n",
      "Epoch 429/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3527 - accuracy: 0.8593\n",
      "Epoch 429: val_loss did not improve from 0.21922\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3526 - accuracy: 0.8595 - val_loss: 0.2967 - val_accuracy: 0.8881\n",
      "Epoch 430/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3387 - accuracy: 0.8648\n",
      "Epoch 430: val_loss did not improve from 0.21922\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3389 - accuracy: 0.8646 - val_loss: 0.2919 - val_accuracy: 0.8892\n",
      "Epoch 431/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3899 - accuracy: 0.8511\n",
      "Epoch 431: val_loss did not improve from 0.21922\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3894 - accuracy: 0.8512 - val_loss: 0.2371 - val_accuracy: 0.9099\n",
      "Epoch 432/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3490 - accuracy: 0.8634\n",
      "Epoch 432: val_loss did not improve from 0.21922\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3490 - accuracy: 0.8632 - val_loss: 0.2294 - val_accuracy: 0.9145\n",
      "Epoch 433/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3492 - accuracy: 0.8625\n",
      "Epoch 433: val_loss did not improve from 0.21922\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3491 - accuracy: 0.8625 - val_loss: 0.2421 - val_accuracy: 0.9014\n",
      "Epoch 434/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3230 - accuracy: 0.8715\n",
      "Epoch 434: val_loss improved from 0.21922 to 0.21559, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.3237 - accuracy: 0.8712 - val_loss: 0.2156 - val_accuracy: 0.9134\n",
      "Epoch 435/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3289 - accuracy: 0.8703\n",
      "Epoch 435: val_loss improved from 0.21559 to 0.21382, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.3292 - accuracy: 0.8701 - val_loss: 0.2138 - val_accuracy: 0.9175\n",
      "Epoch 436/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3577 - accuracy: 0.8625\n",
      "Epoch 436: val_loss did not improve from 0.21382\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3582 - accuracy: 0.8624 - val_loss: 0.2497 - val_accuracy: 0.9038\n",
      "Epoch 437/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5630 - accuracy: 0.8059\n",
      "Epoch 437: val_loss did not improve from 0.21382\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.5627 - accuracy: 0.8058 - val_loss: 0.2592 - val_accuracy: 0.8959\n",
      "Epoch 438/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3661 - accuracy: 0.8602\n",
      "Epoch 438: val_loss did not improve from 0.21382\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3660 - accuracy: 0.8604 - val_loss: 0.3026 - val_accuracy: 0.8861\n",
      "Epoch 439/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3705 - accuracy: 0.8561\n",
      "Epoch 439: val_loss improved from 0.21382 to 0.20586, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.3703 - accuracy: 0.8562 - val_loss: 0.2059 - val_accuracy: 0.9248\n",
      "Epoch 440/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3612 - accuracy: 0.8610\n",
      "Epoch 440: val_loss did not improve from 0.20586\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3615 - accuracy: 0.8609 - val_loss: 0.2626 - val_accuracy: 0.9077\n",
      "Epoch 441/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3489 - accuracy: 0.8627\n",
      "Epoch 441: val_loss did not improve from 0.20586\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3485 - accuracy: 0.8629 - val_loss: 0.2216 - val_accuracy: 0.9150\n",
      "Epoch 442/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3337 - accuracy: 0.8680\n",
      "Epoch 442: val_loss did not improve from 0.20586\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3336 - accuracy: 0.8681 - val_loss: 0.2513 - val_accuracy: 0.9008\n",
      "Epoch 443/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3277 - accuracy: 0.8715\n",
      "Epoch 443: val_loss improved from 0.20586 to 0.20518, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.3282 - accuracy: 0.8714 - val_loss: 0.2052 - val_accuracy: 0.9192\n",
      "Epoch 444/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3350 - accuracy: 0.8691\n",
      "Epoch 444: val_loss did not improve from 0.20518\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3350 - accuracy: 0.8692 - val_loss: 0.2901 - val_accuracy: 0.8931\n",
      "Epoch 445/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3425 - accuracy: 0.8687\n",
      "Epoch 445: val_loss improved from 0.20518 to 0.20479, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.3428 - accuracy: 0.8685 - val_loss: 0.2048 - val_accuracy: 0.9213\n",
      "Epoch 446/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3311 - accuracy: 0.8701\n",
      "Epoch 446: val_loss did not improve from 0.20479\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3316 - accuracy: 0.8699 - val_loss: 0.2293 - val_accuracy: 0.9094\n",
      "Epoch 447/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4345 - accuracy: 0.8374\n",
      "Epoch 447: val_loss did not improve from 0.20479\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4340 - accuracy: 0.8376 - val_loss: 0.2298 - val_accuracy: 0.9124\n",
      "Epoch 448/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3563 - accuracy: 0.8599\n",
      "Epoch 448: val_loss did not improve from 0.20479\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3561 - accuracy: 0.8600 - val_loss: 0.2158 - val_accuracy: 0.9116\n",
      "Epoch 449/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3727 - accuracy: 0.8572\n",
      "Epoch 449: val_loss did not improve from 0.20479\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3724 - accuracy: 0.8574 - val_loss: 0.2141 - val_accuracy: 0.9156\n",
      "Epoch 450/1000\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.3389 - accuracy: 0.8678\n",
      "Epoch 450: val_loss did not improve from 0.20479\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.3392 - accuracy: 0.8674 - val_loss: 0.2159 - val_accuracy: 0.9203\n",
      "Epoch 451/1000\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.3386 - accuracy: 0.8684\n",
      "Epoch 451: val_loss did not improve from 0.20479\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.3380 - accuracy: 0.8685 - val_loss: 0.2130 - val_accuracy: 0.9195\n",
      "Epoch 452/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3899 - accuracy: 0.8540\n",
      "Epoch 452: val_loss did not improve from 0.20479\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3895 - accuracy: 0.8542 - val_loss: 0.2240 - val_accuracy: 0.9151\n",
      "Epoch 453/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3450 - accuracy: 0.8623\n",
      "Epoch 453: val_loss did not improve from 0.20479\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3448 - accuracy: 0.8624 - val_loss: 0.2063 - val_accuracy: 0.9216\n",
      "Epoch 454/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3372 - accuracy: 0.8696\n",
      "Epoch 454: val_loss did not improve from 0.20479\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3367 - accuracy: 0.8699 - val_loss: 0.2152 - val_accuracy: 0.9239\n",
      "Epoch 455/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3255 - accuracy: 0.8730\n",
      "Epoch 455: val_loss did not improve from 0.20479\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3251 - accuracy: 0.8732 - val_loss: 0.2088 - val_accuracy: 0.9211\n",
      "Epoch 456/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3500 - accuracy: 0.8623\n",
      "Epoch 456: val_loss did not improve from 0.20479\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3499 - accuracy: 0.8623 - val_loss: 0.2117 - val_accuracy: 0.9163\n",
      "Epoch 457/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3275 - accuracy: 0.8696\n",
      "Epoch 457: val_loss did not improve from 0.20479\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3273 - accuracy: 0.8698 - val_loss: 0.2442 - val_accuracy: 0.9063\n",
      "Epoch 458/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3142 - accuracy: 0.8756\n",
      "Epoch 458: val_loss improved from 0.20479 to 0.19598, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 19ms/step - loss: 0.3139 - accuracy: 0.8758 - val_loss: 0.1960 - val_accuracy: 0.9270\n",
      "Epoch 459/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3059 - accuracy: 0.8776\n",
      "Epoch 459: val_loss improved from 0.19598 to 0.18656, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.3057 - accuracy: 0.8777 - val_loss: 0.1866 - val_accuracy: 0.9283\n",
      "Epoch 460/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3065 - accuracy: 0.8802\n",
      "Epoch 460: val_loss did not improve from 0.18656\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3066 - accuracy: 0.8802 - val_loss: 0.2412 - val_accuracy: 0.9105\n",
      "Epoch 461/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3280 - accuracy: 0.8734\n",
      "Epoch 461: val_loss did not improve from 0.18656\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3279 - accuracy: 0.8736 - val_loss: 0.2070 - val_accuracy: 0.9154\n",
      "Epoch 462/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3192 - accuracy: 0.8770\n",
      "Epoch 462: val_loss did not improve from 0.18656\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3193 - accuracy: 0.8769 - val_loss: 0.2003 - val_accuracy: 0.9233\n",
      "Epoch 463/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3173 - accuracy: 0.8770\n",
      "Epoch 463: val_loss did not improve from 0.18656\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3169 - accuracy: 0.8772 - val_loss: 0.2022 - val_accuracy: 0.9234\n",
      "Epoch 464/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3081 - accuracy: 0.8790\n",
      "Epoch 464: val_loss did not improve from 0.18656\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3082 - accuracy: 0.8790 - val_loss: 0.2020 - val_accuracy: 0.9183\n",
      "Epoch 465/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3099 - accuracy: 0.8793\n",
      "Epoch 465: val_loss did not improve from 0.18656\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3099 - accuracy: 0.8794 - val_loss: 0.2033 - val_accuracy: 0.9217\n",
      "Epoch 466/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3495 - accuracy: 0.8669\n",
      "Epoch 466: val_loss did not improve from 0.18656\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3493 - accuracy: 0.8669 - val_loss: 0.3321 - val_accuracy: 0.8874\n",
      "Epoch 467/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3443 - accuracy: 0.8653\n",
      "Epoch 467: val_loss did not improve from 0.18656\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3443 - accuracy: 0.8654 - val_loss: 0.3453 - val_accuracy: 0.8645\n",
      "Epoch 468/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3458 - accuracy: 0.8670\n",
      "Epoch 468: val_loss did not improve from 0.18656\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3455 - accuracy: 0.8670 - val_loss: 0.1982 - val_accuracy: 0.9248\n",
      "Epoch 469/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3243 - accuracy: 0.8724\n",
      "Epoch 469: val_loss did not improve from 0.18656\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3241 - accuracy: 0.8725 - val_loss: 0.2077 - val_accuracy: 0.9242\n",
      "Epoch 470/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3336 - accuracy: 0.8710\n",
      "Epoch 470: val_loss did not improve from 0.18656\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3332 - accuracy: 0.8712 - val_loss: 0.2363 - val_accuracy: 0.9136\n",
      "Epoch 471/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3182 - accuracy: 0.8780\n",
      "Epoch 471: val_loss did not improve from 0.18656\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3179 - accuracy: 0.8781 - val_loss: 0.2090 - val_accuracy: 0.9156\n",
      "Epoch 472/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3127 - accuracy: 0.8773\n",
      "Epoch 472: val_loss did not improve from 0.18656\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3126 - accuracy: 0.8774 - val_loss: 0.2116 - val_accuracy: 0.9173\n",
      "Epoch 473/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3138 - accuracy: 0.8782\n",
      "Epoch 473: val_loss did not improve from 0.18656\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3147 - accuracy: 0.8778 - val_loss: 0.2102 - val_accuracy: 0.9222\n",
      "Epoch 474/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3249 - accuracy: 0.8749\n",
      "Epoch 474: val_loss did not improve from 0.18656\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3252 - accuracy: 0.8747 - val_loss: 0.1901 - val_accuracy: 0.9250\n",
      "Epoch 475/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3802 - accuracy: 0.8532\n",
      "Epoch 475: val_loss did not improve from 0.18656\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3800 - accuracy: 0.8532 - val_loss: 0.2369 - val_accuracy: 0.9095\n",
      "Epoch 476/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3233 - accuracy: 0.8731\n",
      "Epoch 476: val_loss did not improve from 0.18656\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3235 - accuracy: 0.8732 - val_loss: 0.2020 - val_accuracy: 0.9230\n",
      "Epoch 477/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3093 - accuracy: 0.8769\n",
      "Epoch 477: val_loss did not improve from 0.18656\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3089 - accuracy: 0.8770 - val_loss: 0.2127 - val_accuracy: 0.9200\n",
      "Epoch 478/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2929 - accuracy: 0.8864\n",
      "Epoch 478: val_loss did not improve from 0.18656\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2925 - accuracy: 0.8866 - val_loss: 0.1948 - val_accuracy: 0.9204\n",
      "Epoch 479/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2973 - accuracy: 0.8816\n",
      "Epoch 479: val_loss did not improve from 0.18656\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.2975 - accuracy: 0.8814 - val_loss: 0.1897 - val_accuracy: 0.9289\n",
      "Epoch 480/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3204 - accuracy: 0.8765\n",
      "Epoch 480: val_loss did not improve from 0.18656\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3204 - accuracy: 0.8764 - val_loss: 0.4209 - val_accuracy: 0.8693\n",
      "Epoch 481/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3469 - accuracy: 0.8682\n",
      "Epoch 481: val_loss did not improve from 0.18656\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3471 - accuracy: 0.8681 - val_loss: 0.2011 - val_accuracy: 0.9232\n",
      "Epoch 482/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3502 - accuracy: 0.8693\n",
      "Epoch 482: val_loss did not improve from 0.18656\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3502 - accuracy: 0.8693 - val_loss: 0.2027 - val_accuracy: 0.9237\n",
      "Epoch 483/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3204 - accuracy: 0.8737\n",
      "Epoch 483: val_loss did not improve from 0.18656\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3208 - accuracy: 0.8734 - val_loss: 0.2119 - val_accuracy: 0.9202\n",
      "Epoch 484/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3040 - accuracy: 0.8788\n",
      "Epoch 484: val_loss did not improve from 0.18656\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3038 - accuracy: 0.8789 - val_loss: 0.2014 - val_accuracy: 0.9220\n",
      "Epoch 485/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2984 - accuracy: 0.8827\n",
      "Epoch 485: val_loss did not improve from 0.18656\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2982 - accuracy: 0.8828 - val_loss: 0.2026 - val_accuracy: 0.9224\n",
      "Epoch 486/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3255 - accuracy: 0.8730\n",
      "Epoch 486: val_loss did not improve from 0.18656\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3253 - accuracy: 0.8731 - val_loss: 0.2032 - val_accuracy: 0.9265\n",
      "Epoch 487/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3170 - accuracy: 0.8828\n",
      "Epoch 487: val_loss did not improve from 0.18656\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3170 - accuracy: 0.8828 - val_loss: 0.2293 - val_accuracy: 0.9117\n",
      "Epoch 488/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3376 - accuracy: 0.8722\n",
      "Epoch 488: val_loss did not improve from 0.18656\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3377 - accuracy: 0.8723 - val_loss: 0.2053 - val_accuracy: 0.9232\n",
      "Epoch 489/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2986 - accuracy: 0.8829\n",
      "Epoch 489: val_loss did not improve from 0.18656\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2989 - accuracy: 0.8826 - val_loss: 0.1961 - val_accuracy: 0.9237\n",
      "Epoch 490/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3866 - accuracy: 0.8567\n",
      "Epoch 490: val_loss did not improve from 0.18656\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3865 - accuracy: 0.8567 - val_loss: 0.1944 - val_accuracy: 0.9283\n",
      "Epoch 491/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3077 - accuracy: 0.8784\n",
      "Epoch 491: val_loss improved from 0.18656 to 0.17930, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.3076 - accuracy: 0.8784 - val_loss: 0.1793 - val_accuracy: 0.9303\n",
      "Epoch 492/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2839 - accuracy: 0.8863\n",
      "Epoch 492: val_loss did not improve from 0.17930\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2839 - accuracy: 0.8863 - val_loss: 0.1866 - val_accuracy: 0.9247\n",
      "Epoch 493/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2967 - accuracy: 0.8842\n",
      "Epoch 493: val_loss did not improve from 0.17930\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2967 - accuracy: 0.8842 - val_loss: 0.1878 - val_accuracy: 0.9292\n",
      "Epoch 494/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3132 - accuracy: 0.8769\n",
      "Epoch 494: val_loss did not improve from 0.17930\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3135 - accuracy: 0.8767 - val_loss: 0.2030 - val_accuracy: 0.9235\n",
      "Epoch 495/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2975 - accuracy: 0.8832\n",
      "Epoch 495: val_loss did not improve from 0.17930\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2975 - accuracy: 0.8832 - val_loss: 0.1906 - val_accuracy: 0.9229\n",
      "Epoch 496/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2825 - accuracy: 0.8892\n",
      "Epoch 496: val_loss improved from 0.17930 to 0.17867, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.2824 - accuracy: 0.8893 - val_loss: 0.1787 - val_accuracy: 0.9313\n",
      "Epoch 497/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3111 - accuracy: 0.8790\n",
      "Epoch 497: val_loss did not improve from 0.17867\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3109 - accuracy: 0.8791 - val_loss: 0.1884 - val_accuracy: 0.9298\n",
      "Epoch 498/1000\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.2920 - accuracy: 0.8888\n",
      "Epoch 498: val_loss improved from 0.17867 to 0.17481, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 19ms/step - loss: 0.2933 - accuracy: 0.8878 - val_loss: 0.1748 - val_accuracy: 0.9310\n",
      "Epoch 499/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2811 - accuracy: 0.8897\n",
      "Epoch 499: val_loss improved from 0.17481 to 0.17426, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.2810 - accuracy: 0.8897 - val_loss: 0.1743 - val_accuracy: 0.9297\n",
      "Epoch 500/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2807 - accuracy: 0.8913\n",
      "Epoch 500: val_loss did not improve from 0.17426\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2813 - accuracy: 0.8910 - val_loss: 0.1755 - val_accuracy: 0.9330\n",
      "Epoch 501/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3047 - accuracy: 0.8800\n",
      "Epoch 501: val_loss improved from 0.17426 to 0.17419, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 19ms/step - loss: 0.3047 - accuracy: 0.8800 - val_loss: 0.1742 - val_accuracy: 0.9315\n",
      "Epoch 502/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2881 - accuracy: 0.8854\n",
      "Epoch 502: val_loss did not improve from 0.17419\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2880 - accuracy: 0.8855 - val_loss: 0.2078 - val_accuracy: 0.9171\n",
      "Epoch 503/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3059 - accuracy: 0.8820\n",
      "Epoch 503: val_loss did not improve from 0.17419\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3062 - accuracy: 0.8819 - val_loss: 0.2164 - val_accuracy: 0.9239\n",
      "Epoch 504/1000\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.3064 - accuracy: 0.8793\n",
      "Epoch 504: val_loss did not improve from 0.17419\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3060 - accuracy: 0.8793 - val_loss: 0.1786 - val_accuracy: 0.9320\n",
      "Epoch 505/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2846 - accuracy: 0.8898\n",
      "Epoch 505: val_loss did not improve from 0.17419\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2843 - accuracy: 0.8899 - val_loss: 0.1822 - val_accuracy: 0.9278\n",
      "Epoch 506/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2774 - accuracy: 0.8883\n",
      "Epoch 506: val_loss did not improve from 0.17419\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2771 - accuracy: 0.8884 - val_loss: 0.1802 - val_accuracy: 0.9302\n",
      "Epoch 507/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3155 - accuracy: 0.8802\n",
      "Epoch 507: val_loss did not improve from 0.17419\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.3152 - accuracy: 0.8803 - val_loss: 0.1943 - val_accuracy: 0.9267\n",
      "Epoch 508/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2952 - accuracy: 0.8870\n",
      "Epoch 508: val_loss did not improve from 0.17419\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2951 - accuracy: 0.8871 - val_loss: 0.2066 - val_accuracy: 0.9239\n",
      "Epoch 509/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2955 - accuracy: 0.8850\n",
      "Epoch 509: val_loss did not improve from 0.17419\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2952 - accuracy: 0.8851 - val_loss: 0.1770 - val_accuracy: 0.9322\n",
      "Epoch 510/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3234 - accuracy: 0.8768\n",
      "Epoch 510: val_loss did not improve from 0.17419\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3243 - accuracy: 0.8764 - val_loss: 0.2024 - val_accuracy: 0.9214\n",
      "Epoch 511/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3137 - accuracy: 0.8800\n",
      "Epoch 511: val_loss did not improve from 0.17419\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3132 - accuracy: 0.8802 - val_loss: 0.1814 - val_accuracy: 0.9327\n",
      "Epoch 512/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3060 - accuracy: 0.8825\n",
      "Epoch 512: val_loss did not improve from 0.17419\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3057 - accuracy: 0.8826 - val_loss: 0.1831 - val_accuracy: 0.9281\n",
      "Epoch 513/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2926 - accuracy: 0.8878\n",
      "Epoch 513: val_loss did not improve from 0.17419\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2922 - accuracy: 0.8880 - val_loss: 0.1862 - val_accuracy: 0.9283\n",
      "Epoch 514/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2959 - accuracy: 0.8859\n",
      "Epoch 514: val_loss did not improve from 0.17419\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2963 - accuracy: 0.8856 - val_loss: 0.1865 - val_accuracy: 0.9286\n",
      "Epoch 515/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2751 - accuracy: 0.8927\n",
      "Epoch 515: val_loss did not improve from 0.17419\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2749 - accuracy: 0.8927 - val_loss: 0.1775 - val_accuracy: 0.9303\n",
      "Epoch 516/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2802 - accuracy: 0.8897\n",
      "Epoch 516: val_loss did not improve from 0.17419\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2799 - accuracy: 0.8899 - val_loss: 0.2367 - val_accuracy: 0.9112\n",
      "Epoch 517/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4434 - accuracy: 0.8426\n",
      "Epoch 517: val_loss did not improve from 0.17419\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4431 - accuracy: 0.8427 - val_loss: 0.2173 - val_accuracy: 0.9216\n",
      "Epoch 518/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3168 - accuracy: 0.8793\n",
      "Epoch 518: val_loss did not improve from 0.17419\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3165 - accuracy: 0.8794 - val_loss: 0.1806 - val_accuracy: 0.9310\n",
      "Epoch 519/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2692 - accuracy: 0.8958\n",
      "Epoch 519: val_loss improved from 0.17419 to 0.15884, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.2696 - accuracy: 0.8956 - val_loss: 0.1588 - val_accuracy: 0.9409\n",
      "Epoch 520/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2806 - accuracy: 0.8874\n",
      "Epoch 520: val_loss did not improve from 0.15884\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2807 - accuracy: 0.8873 - val_loss: 0.1703 - val_accuracy: 0.9339\n",
      "Epoch 521/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3255 - accuracy: 0.8738\n",
      "Epoch 521: val_loss did not improve from 0.15884\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3252 - accuracy: 0.8739 - val_loss: 0.1873 - val_accuracy: 0.9295\n",
      "Epoch 522/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3016 - accuracy: 0.8826\n",
      "Epoch 522: val_loss did not improve from 0.15884\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3017 - accuracy: 0.8825 - val_loss: 0.1785 - val_accuracy: 0.9316\n",
      "Epoch 523/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2698 - accuracy: 0.8970\n",
      "Epoch 523: val_loss did not improve from 0.15884\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2701 - accuracy: 0.8968 - val_loss: 0.1603 - val_accuracy: 0.9400\n",
      "Epoch 524/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2755 - accuracy: 0.8913\n",
      "Epoch 524: val_loss did not improve from 0.15884\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2756 - accuracy: 0.8913 - val_loss: 0.1823 - val_accuracy: 0.9300\n",
      "Epoch 525/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3001 - accuracy: 0.8844\n",
      "Epoch 525: val_loss did not improve from 0.15884\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2999 - accuracy: 0.8845 - val_loss: 0.1733 - val_accuracy: 0.9336\n",
      "Epoch 526/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2911 - accuracy: 0.8875\n",
      "Epoch 526: val_loss did not improve from 0.15884\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2916 - accuracy: 0.8874 - val_loss: 0.2143 - val_accuracy: 0.9181\n",
      "Epoch 527/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3062 - accuracy: 0.8851\n",
      "Epoch 527: val_loss did not improve from 0.15884\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3058 - accuracy: 0.8852 - val_loss: 0.1963 - val_accuracy: 0.9203\n",
      "Epoch 528/1000\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.3375 - accuracy: 0.8709\n",
      "Epoch 528: val_loss did not improve from 0.15884\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.3375 - accuracy: 0.8704 - val_loss: 0.1897 - val_accuracy: 0.9281\n",
      "Epoch 529/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2927 - accuracy: 0.8866\n",
      "Epoch 529: val_loss did not improve from 0.15884\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2924 - accuracy: 0.8868 - val_loss: 0.2011 - val_accuracy: 0.9215\n",
      "Epoch 530/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2838 - accuracy: 0.8890\n",
      "Epoch 530: val_loss did not improve from 0.15884\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2837 - accuracy: 0.8891 - val_loss: 0.1792 - val_accuracy: 0.9291\n",
      "Epoch 531/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3017 - accuracy: 0.8848\n",
      "Epoch 531: val_loss did not improve from 0.15884\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3022 - accuracy: 0.8846 - val_loss: 0.2038 - val_accuracy: 0.9186\n",
      "Epoch 532/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3627 - accuracy: 0.8636\n",
      "Epoch 532: val_loss did not improve from 0.15884\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3628 - accuracy: 0.8636 - val_loss: 0.1976 - val_accuracy: 0.9262\n",
      "Epoch 533/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2983 - accuracy: 0.8839\n",
      "Epoch 533: val_loss did not improve from 0.15884\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2984 - accuracy: 0.8838 - val_loss: 0.1858 - val_accuracy: 0.9308\n",
      "Epoch 534/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3128 - accuracy: 0.8789\n",
      "Epoch 534: val_loss did not improve from 0.15884\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3124 - accuracy: 0.8790 - val_loss: 0.1714 - val_accuracy: 0.9349\n",
      "Epoch 535/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2887 - accuracy: 0.8904\n",
      "Epoch 535: val_loss did not improve from 0.15884\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2907 - accuracy: 0.8899 - val_loss: 0.2337 - val_accuracy: 0.9124\n",
      "Epoch 536/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.7432 - accuracy: 0.7306\n",
      "Epoch 536: val_loss did not improve from 0.15884\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.7424 - accuracy: 0.7310 - val_loss: 0.3241 - val_accuracy: 0.8785\n",
      "Epoch 537/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4168 - accuracy: 0.8427\n",
      "Epoch 537: val_loss did not improve from 0.15884\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.4167 - accuracy: 0.8427 - val_loss: 0.2196 - val_accuracy: 0.9174\n",
      "Epoch 538/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3284 - accuracy: 0.8730\n",
      "Epoch 538: val_loss did not improve from 0.15884\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3282 - accuracy: 0.8732 - val_loss: 0.1876 - val_accuracy: 0.9265\n",
      "Epoch 539/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3153 - accuracy: 0.8796\n",
      "Epoch 539: val_loss did not improve from 0.15884\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3149 - accuracy: 0.8797 - val_loss: 0.2059 - val_accuracy: 0.9232\n",
      "Epoch 540/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3017 - accuracy: 0.8828\n",
      "Epoch 540: val_loss did not improve from 0.15884\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3016 - accuracy: 0.8828 - val_loss: 0.2154 - val_accuracy: 0.9118\n",
      "Epoch 541/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3071 - accuracy: 0.8814\n",
      "Epoch 541: val_loss did not improve from 0.15884\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3072 - accuracy: 0.8814 - val_loss: 0.2219 - val_accuracy: 0.9148\n",
      "Epoch 542/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3439 - accuracy: 0.8719\n",
      "Epoch 542: val_loss did not improve from 0.15884\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3438 - accuracy: 0.8719 - val_loss: 0.2050 - val_accuracy: 0.9226\n",
      "Epoch 543/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2981 - accuracy: 0.8849\n",
      "Epoch 543: val_loss did not improve from 0.15884\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2982 - accuracy: 0.8849 - val_loss: 0.1744 - val_accuracy: 0.9314\n",
      "Epoch 544/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2922 - accuracy: 0.8863\n",
      "Epoch 544: val_loss did not improve from 0.15884\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2923 - accuracy: 0.8862 - val_loss: 0.1718 - val_accuracy: 0.9322\n",
      "Epoch 545/1000\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.2944 - accuracy: 0.8856\n",
      "Epoch 545: val_loss did not improve from 0.15884\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.2945 - accuracy: 0.8857 - val_loss: 0.1741 - val_accuracy: 0.9364\n",
      "Epoch 546/1000\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.3071 - accuracy: 0.8822\n",
      "Epoch 546: val_loss did not improve from 0.15884\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.3091 - accuracy: 0.8816 - val_loss: 0.1715 - val_accuracy: 0.9346\n",
      "Epoch 547/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2745 - accuracy: 0.8931\n",
      "Epoch 547: val_loss did not improve from 0.15884\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2748 - accuracy: 0.8929 - val_loss: 0.1649 - val_accuracy: 0.9351\n",
      "Epoch 548/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2666 - accuracy: 0.8956\n",
      "Epoch 548: val_loss did not improve from 0.15884\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2666 - accuracy: 0.8956 - val_loss: 0.1626 - val_accuracy: 0.9382\n",
      "Epoch 549/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2606 - accuracy: 0.8997\n",
      "Epoch 549: val_loss did not improve from 0.15884\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2606 - accuracy: 0.8997 - val_loss: 0.1618 - val_accuracy: 0.9362\n",
      "Epoch 550/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2783 - accuracy: 0.8937\n",
      "Epoch 550: val_loss did not improve from 0.15884\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2784 - accuracy: 0.8936 - val_loss: 0.1872 - val_accuracy: 0.9293\n",
      "Epoch 551/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2864 - accuracy: 0.8900\n",
      "Epoch 551: val_loss improved from 0.15884 to 0.15584, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.2866 - accuracy: 0.8897 - val_loss: 0.1558 - val_accuracy: 0.9419\n",
      "Epoch 552/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2706 - accuracy: 0.8962\n",
      "Epoch 552: val_loss did not improve from 0.15584\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2706 - accuracy: 0.8962 - val_loss: 0.1662 - val_accuracy: 0.9377\n",
      "Epoch 553/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2887 - accuracy: 0.8932\n",
      "Epoch 553: val_loss did not improve from 0.15584\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2890 - accuracy: 0.8929 - val_loss: 0.1758 - val_accuracy: 0.9325\n",
      "Epoch 554/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2832 - accuracy: 0.8910\n",
      "Epoch 554: val_loss did not improve from 0.15584\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2840 - accuracy: 0.8907 - val_loss: 0.1835 - val_accuracy: 0.9326\n",
      "Epoch 555/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3290 - accuracy: 0.8748\n",
      "Epoch 555: val_loss did not improve from 0.15584\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3289 - accuracy: 0.8749 - val_loss: 0.2169 - val_accuracy: 0.9160\n",
      "Epoch 556/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2844 - accuracy: 0.8910\n",
      "Epoch 556: val_loss did not improve from 0.15584\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.2843 - accuracy: 0.8910 - val_loss: 0.1776 - val_accuracy: 0.9316\n",
      "Epoch 557/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2988 - accuracy: 0.8868\n",
      "Epoch 557: val_loss did not improve from 0.15584\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2989 - accuracy: 0.8868 - val_loss: 0.1773 - val_accuracy: 0.9326\n",
      "Epoch 558/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2544 - accuracy: 0.9018\n",
      "Epoch 558: val_loss improved from 0.15584 to 0.14261, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 19ms/step - loss: 0.2547 - accuracy: 0.9017 - val_loss: 0.1426 - val_accuracy: 0.9483\n",
      "Epoch 559/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2568 - accuracy: 0.9009\n",
      "Epoch 559: val_loss did not improve from 0.14261\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2568 - accuracy: 0.9009 - val_loss: 0.1669 - val_accuracy: 0.9336\n",
      "Epoch 560/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3617 - accuracy: 0.8678\n",
      "Epoch 560: val_loss did not improve from 0.14261\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3626 - accuracy: 0.8674 - val_loss: 0.3522 - val_accuracy: 0.8996\n",
      "Epoch 561/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3388 - accuracy: 0.8757\n",
      "Epoch 561: val_loss did not improve from 0.14261\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.3383 - accuracy: 0.8759 - val_loss: 0.1673 - val_accuracy: 0.9388\n",
      "Epoch 562/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2776 - accuracy: 0.8931\n",
      "Epoch 562: val_loss did not improve from 0.14261\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.2779 - accuracy: 0.8930 - val_loss: 0.1636 - val_accuracy: 0.9407\n",
      "Epoch 563/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2723 - accuracy: 0.8955\n",
      "Epoch 563: val_loss did not improve from 0.14261\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.2720 - accuracy: 0.8957 - val_loss: 0.1702 - val_accuracy: 0.9363\n",
      "Epoch 564/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2701 - accuracy: 0.8946\n",
      "Epoch 564: val_loss did not improve from 0.14261\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2698 - accuracy: 0.8947 - val_loss: 0.1679 - val_accuracy: 0.9362\n",
      "Epoch 565/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3480 - accuracy: 0.8737\n",
      "Epoch 565: val_loss did not improve from 0.14261\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3481 - accuracy: 0.8737 - val_loss: 0.1908 - val_accuracy: 0.9304\n",
      "Epoch 566/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2766 - accuracy: 0.8938\n",
      "Epoch 566: val_loss did not improve from 0.14261\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2766 - accuracy: 0.8939 - val_loss: 0.1645 - val_accuracy: 0.9367\n",
      "Epoch 567/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2687 - accuracy: 0.8968\n",
      "Epoch 567: val_loss did not improve from 0.14261\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2687 - accuracy: 0.8968 - val_loss: 0.1671 - val_accuracy: 0.9400\n",
      "Epoch 568/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2686 - accuracy: 0.8952\n",
      "Epoch 568: val_loss did not improve from 0.14261\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2684 - accuracy: 0.8953 - val_loss: 0.1527 - val_accuracy: 0.9394\n",
      "Epoch 569/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2676 - accuracy: 0.8979\n",
      "Epoch 569: val_loss did not improve from 0.14261\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2678 - accuracy: 0.8979 - val_loss: 0.1927 - val_accuracy: 0.9310\n",
      "Epoch 570/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2583 - accuracy: 0.9027\n",
      "Epoch 570: val_loss did not improve from 0.14261\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2580 - accuracy: 0.9029 - val_loss: 0.1677 - val_accuracy: 0.9355\n",
      "Epoch 571/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2560 - accuracy: 0.8994\n",
      "Epoch 571: val_loss did not improve from 0.14261\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2558 - accuracy: 0.8995 - val_loss: 0.1576 - val_accuracy: 0.9415\n",
      "Epoch 572/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2504 - accuracy: 0.9011\n",
      "Epoch 572: val_loss did not improve from 0.14261\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2504 - accuracy: 0.9012 - val_loss: 0.1675 - val_accuracy: 0.9377\n",
      "Epoch 573/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3094 - accuracy: 0.8831\n",
      "Epoch 573: val_loss did not improve from 0.14261\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3101 - accuracy: 0.8828 - val_loss: 0.1907 - val_accuracy: 0.9288\n",
      "Epoch 574/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2939 - accuracy: 0.8899\n",
      "Epoch 574: val_loss did not improve from 0.14261\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2939 - accuracy: 0.8899 - val_loss: 0.1745 - val_accuracy: 0.9282\n",
      "Epoch 575/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2546 - accuracy: 0.9006\n",
      "Epoch 575: val_loss did not improve from 0.14261\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2548 - accuracy: 0.9006 - val_loss: 0.1596 - val_accuracy: 0.9390\n",
      "Epoch 576/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2645 - accuracy: 0.8955\n",
      "Epoch 576: val_loss did not improve from 0.14261\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2647 - accuracy: 0.8954 - val_loss: 0.1700 - val_accuracy: 0.9334\n",
      "Epoch 577/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2623 - accuracy: 0.8993\n",
      "Epoch 577: val_loss did not improve from 0.14261\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2621 - accuracy: 0.8995 - val_loss: 0.1643 - val_accuracy: 0.9403\n",
      "Epoch 578/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2549 - accuracy: 0.9022\n",
      "Epoch 578: val_loss did not improve from 0.14261\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2548 - accuracy: 0.9022 - val_loss: 0.1698 - val_accuracy: 0.9348\n",
      "Epoch 579/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2888 - accuracy: 0.8928\n",
      "Epoch 579: val_loss did not improve from 0.14261\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2885 - accuracy: 0.8929 - val_loss: 0.2168 - val_accuracy: 0.9212\n",
      "Epoch 580/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2644 - accuracy: 0.9000\n",
      "Epoch 580: val_loss did not improve from 0.14261\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2642 - accuracy: 0.9001 - val_loss: 0.1510 - val_accuracy: 0.9424\n",
      "Epoch 581/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2415 - accuracy: 0.9060\n",
      "Epoch 581: val_loss did not improve from 0.14261\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2413 - accuracy: 0.9061 - val_loss: 0.1652 - val_accuracy: 0.9363\n",
      "Epoch 582/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2579 - accuracy: 0.8996\n",
      "Epoch 582: val_loss improved from 0.14261 to 0.14125, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.2584 - accuracy: 0.8992 - val_loss: 0.1413 - val_accuracy: 0.9438\n",
      "Epoch 583/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2594 - accuracy: 0.8998\n",
      "Epoch 583: val_loss did not improve from 0.14125\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2594 - accuracy: 0.8998 - val_loss: 0.1594 - val_accuracy: 0.9430\n",
      "Epoch 584/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2671 - accuracy: 0.8978\n",
      "Epoch 584: val_loss did not improve from 0.14125\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2669 - accuracy: 0.8979 - val_loss: 0.1582 - val_accuracy: 0.9395\n",
      "Epoch 585/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2916 - accuracy: 0.8897\n",
      "Epoch 585: val_loss did not improve from 0.14125\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2915 - accuracy: 0.8898 - val_loss: 0.1635 - val_accuracy: 0.9362\n",
      "Epoch 586/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2657 - accuracy: 0.8981\n",
      "Epoch 586: val_loss did not improve from 0.14125\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2655 - accuracy: 0.8982 - val_loss: 0.1635 - val_accuracy: 0.9403\n",
      "Epoch 587/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2587 - accuracy: 0.9007\n",
      "Epoch 587: val_loss did not improve from 0.14125\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2587 - accuracy: 0.9007 - val_loss: 0.1649 - val_accuracy: 0.9409\n",
      "Epoch 588/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2902 - accuracy: 0.8878\n",
      "Epoch 588: val_loss did not improve from 0.14125\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.2901 - accuracy: 0.8878 - val_loss: 0.1844 - val_accuracy: 0.9298\n",
      "Epoch 589/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2681 - accuracy: 0.8948\n",
      "Epoch 589: val_loss did not improve from 0.14125\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2676 - accuracy: 0.8950 - val_loss: 0.1641 - val_accuracy: 0.9379\n",
      "Epoch 590/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2687 - accuracy: 0.8968\n",
      "Epoch 590: val_loss did not improve from 0.14125\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2685 - accuracy: 0.8970 - val_loss: 0.1592 - val_accuracy: 0.9388\n",
      "Epoch 591/1000\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.2447 - accuracy: 0.9063\n",
      "Epoch 591: val_loss did not improve from 0.14125\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.2447 - accuracy: 0.9062 - val_loss: 0.1566 - val_accuracy: 0.9402\n",
      "Epoch 592/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2516 - accuracy: 0.9041\n",
      "Epoch 592: val_loss did not improve from 0.14125\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2513 - accuracy: 0.9042 - val_loss: 0.1552 - val_accuracy: 0.9388\n",
      "Epoch 593/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2460 - accuracy: 0.9037\n",
      "Epoch 593: val_loss did not improve from 0.14125\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2459 - accuracy: 0.9037 - val_loss: 0.1431 - val_accuracy: 0.9468\n",
      "Epoch 594/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2396 - accuracy: 0.9068\n",
      "Epoch 594: val_loss did not improve from 0.14125\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2394 - accuracy: 0.9069 - val_loss: 0.1583 - val_accuracy: 0.9411\n",
      "Epoch 595/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2502 - accuracy: 0.9027\n",
      "Epoch 595: val_loss did not improve from 0.14125\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2502 - accuracy: 0.9028 - val_loss: 0.1725 - val_accuracy: 0.9349\n",
      "Epoch 596/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3182 - accuracy: 0.8851\n",
      "Epoch 596: val_loss did not improve from 0.14125\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3181 - accuracy: 0.8852 - val_loss: 0.1726 - val_accuracy: 0.9393\n",
      "Epoch 597/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3368 - accuracy: 0.8796\n",
      "Epoch 597: val_loss did not improve from 0.14125\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3369 - accuracy: 0.8795 - val_loss: 0.1619 - val_accuracy: 0.9379\n",
      "Epoch 598/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2625 - accuracy: 0.8998\n",
      "Epoch 598: val_loss did not improve from 0.14125\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2623 - accuracy: 0.8998 - val_loss: 0.1525 - val_accuracy: 0.9423\n",
      "Epoch 599/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2485 - accuracy: 0.9041\n",
      "Epoch 599: val_loss did not improve from 0.14125\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2489 - accuracy: 0.9038 - val_loss: 0.1559 - val_accuracy: 0.9419\n",
      "Epoch 600/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2481 - accuracy: 0.9034\n",
      "Epoch 600: val_loss did not improve from 0.14125\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2479 - accuracy: 0.9035 - val_loss: 0.1898 - val_accuracy: 0.9316\n",
      "Epoch 601/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2720 - accuracy: 0.8982\n",
      "Epoch 601: val_loss did not improve from 0.14125\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2719 - accuracy: 0.8983 - val_loss: 0.1424 - val_accuracy: 0.9449\n",
      "Epoch 602/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2407 - accuracy: 0.9053\n",
      "Epoch 602: val_loss did not improve from 0.14125\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2406 - accuracy: 0.9053 - val_loss: 0.1441 - val_accuracy: 0.9435\n",
      "Epoch 603/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2454 - accuracy: 0.9065\n",
      "Epoch 603: val_loss did not improve from 0.14125\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2458 - accuracy: 0.9063 - val_loss: 0.1523 - val_accuracy: 0.9420\n",
      "Epoch 604/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2806 - accuracy: 0.8933\n",
      "Epoch 604: val_loss did not improve from 0.14125\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.2810 - accuracy: 0.8931 - val_loss: 0.1723 - val_accuracy: 0.9327\n",
      "Epoch 605/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2708 - accuracy: 0.8969\n",
      "Epoch 605: val_loss did not improve from 0.14125\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2707 - accuracy: 0.8969 - val_loss: 0.1539 - val_accuracy: 0.9425\n",
      "Epoch 606/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2534 - accuracy: 0.9025\n",
      "Epoch 606: val_loss did not improve from 0.14125\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2532 - accuracy: 0.9026 - val_loss: 0.1473 - val_accuracy: 0.9428\n",
      "Epoch 607/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2649 - accuracy: 0.9011\n",
      "Epoch 607: val_loss did not improve from 0.14125\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2649 - accuracy: 0.9010 - val_loss: 0.1555 - val_accuracy: 0.9439\n",
      "Epoch 608/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2567 - accuracy: 0.9017\n",
      "Epoch 608: val_loss did not improve from 0.14125\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2568 - accuracy: 0.9017 - val_loss: 0.1655 - val_accuracy: 0.9335\n",
      "Epoch 609/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2581 - accuracy: 0.9006\n",
      "Epoch 609: val_loss did not improve from 0.14125\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.2585 - accuracy: 0.9004 - val_loss: 0.1701 - val_accuracy: 0.9378\n",
      "Epoch 610/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2763 - accuracy: 0.8956\n",
      "Epoch 610: val_loss did not improve from 0.14125\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2761 - accuracy: 0.8956 - val_loss: 0.1586 - val_accuracy: 0.9422\n",
      "Epoch 611/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2577 - accuracy: 0.8993\n",
      "Epoch 611: val_loss did not improve from 0.14125\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2575 - accuracy: 0.8994 - val_loss: 0.1426 - val_accuracy: 0.9472\n",
      "Epoch 612/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2404 - accuracy: 0.9082\n",
      "Epoch 612: val_loss did not improve from 0.14125\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2406 - accuracy: 0.9080 - val_loss: 0.1455 - val_accuracy: 0.9463\n",
      "Epoch 613/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2622 - accuracy: 0.9018\n",
      "Epoch 613: val_loss did not improve from 0.14125\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2622 - accuracy: 0.9019 - val_loss: 0.4570 - val_accuracy: 0.8745\n",
      "Epoch 614/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3030 - accuracy: 0.8895\n",
      "Epoch 614: val_loss did not improve from 0.14125\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3027 - accuracy: 0.8896 - val_loss: 0.1874 - val_accuracy: 0.9257\n",
      "Epoch 615/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2779 - accuracy: 0.8974\n",
      "Epoch 615: val_loss did not improve from 0.14125\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2779 - accuracy: 0.8974 - val_loss: 0.1510 - val_accuracy: 0.9456\n",
      "Epoch 616/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2790 - accuracy: 0.8929\n",
      "Epoch 616: val_loss did not improve from 0.14125\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2789 - accuracy: 0.8930 - val_loss: 0.1603 - val_accuracy: 0.9396\n",
      "Epoch 617/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2480 - accuracy: 0.9063\n",
      "Epoch 617: val_loss improved from 0.14125 to 0.13765, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.2479 - accuracy: 0.9063 - val_loss: 0.1376 - val_accuracy: 0.9496\n",
      "Epoch 618/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2316 - accuracy: 0.9111\n",
      "Epoch 618: val_loss improved from 0.13765 to 0.13110, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.2316 - accuracy: 0.9110 - val_loss: 0.1311 - val_accuracy: 0.9509\n",
      "Epoch 619/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2696 - accuracy: 0.8987\n",
      "Epoch 619: val_loss did not improve from 0.13110\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2696 - accuracy: 0.8988 - val_loss: 0.1772 - val_accuracy: 0.9361\n",
      "Epoch 620/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2782 - accuracy: 0.8969\n",
      "Epoch 620: val_loss did not improve from 0.13110\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2783 - accuracy: 0.8968 - val_loss: 0.1519 - val_accuracy: 0.9428\n",
      "Epoch 621/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2333 - accuracy: 0.9091\n",
      "Epoch 621: val_loss did not improve from 0.13110\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2333 - accuracy: 0.9091 - val_loss: 0.1455 - val_accuracy: 0.9452\n",
      "Epoch 622/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2437 - accuracy: 0.9064\n",
      "Epoch 622: val_loss did not improve from 0.13110\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2435 - accuracy: 0.9065 - val_loss: 0.1483 - val_accuracy: 0.9445\n",
      "Epoch 623/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3316 - accuracy: 0.8785\n",
      "Epoch 623: val_loss did not improve from 0.13110\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.3317 - accuracy: 0.8784 - val_loss: 0.2060 - val_accuracy: 0.9249\n",
      "Epoch 624/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2689 - accuracy: 0.8962\n",
      "Epoch 624: val_loss did not improve from 0.13110\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2690 - accuracy: 0.8961 - val_loss: 0.1764 - val_accuracy: 0.9323\n",
      "Epoch 625/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2530 - accuracy: 0.9056\n",
      "Epoch 625: val_loss did not improve from 0.13110\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2532 - accuracy: 0.9054 - val_loss: 0.1510 - val_accuracy: 0.9399\n",
      "Epoch 626/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2360 - accuracy: 0.9104\n",
      "Epoch 626: val_loss did not improve from 0.13110\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.2359 - accuracy: 0.9104 - val_loss: 0.1365 - val_accuracy: 0.9472\n",
      "Epoch 627/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2358 - accuracy: 0.9108\n",
      "Epoch 627: val_loss did not improve from 0.13110\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2358 - accuracy: 0.9108 - val_loss: 0.1462 - val_accuracy: 0.9428\n",
      "Epoch 628/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2318 - accuracy: 0.9110\n",
      "Epoch 628: val_loss did not improve from 0.13110\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2317 - accuracy: 0.9110 - val_loss: 0.1431 - val_accuracy: 0.9475\n",
      "Epoch 629/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2649 - accuracy: 0.8993\n",
      "Epoch 629: val_loss did not improve from 0.13110\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2647 - accuracy: 0.8994 - val_loss: 0.1442 - val_accuracy: 0.9451\n",
      "Epoch 630/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2375 - accuracy: 0.9086\n",
      "Epoch 630: val_loss did not improve from 0.13110\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2376 - accuracy: 0.9085 - val_loss: 0.1501 - val_accuracy: 0.9423\n",
      "Epoch 631/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2672 - accuracy: 0.8993\n",
      "Epoch 631: val_loss did not improve from 0.13110\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2672 - accuracy: 0.8993 - val_loss: 0.1823 - val_accuracy: 0.9340\n",
      "Epoch 632/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2480 - accuracy: 0.9031\n",
      "Epoch 632: val_loss did not improve from 0.13110\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2478 - accuracy: 0.9032 - val_loss: 0.1388 - val_accuracy: 0.9471\n",
      "Epoch 633/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2617 - accuracy: 0.8986\n",
      "Epoch 633: val_loss did not improve from 0.13110\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2617 - accuracy: 0.8985 - val_loss: 0.1649 - val_accuracy: 0.9376\n",
      "Epoch 634/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2801 - accuracy: 0.8920\n",
      "Epoch 634: val_loss did not improve from 0.13110\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2798 - accuracy: 0.8922 - val_loss: 0.1619 - val_accuracy: 0.9394\n",
      "Epoch 635/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2467 - accuracy: 0.9038\n",
      "Epoch 635: val_loss did not improve from 0.13110\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2485 - accuracy: 0.9036 - val_loss: 0.1453 - val_accuracy: 0.9469\n",
      "Epoch 636/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3207 - accuracy: 0.8825\n",
      "Epoch 636: val_loss did not improve from 0.13110\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3207 - accuracy: 0.8826 - val_loss: 0.1515 - val_accuracy: 0.9444\n",
      "Epoch 637/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2384 - accuracy: 0.9080\n",
      "Epoch 637: val_loss did not improve from 0.13110\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2385 - accuracy: 0.9080 - val_loss: 0.1619 - val_accuracy: 0.9335\n",
      "Epoch 638/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2421 - accuracy: 0.9082\n",
      "Epoch 638: val_loss did not improve from 0.13110\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2419 - accuracy: 0.9082 - val_loss: 0.1376 - val_accuracy: 0.9478\n",
      "Epoch 639/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2386 - accuracy: 0.9087\n",
      "Epoch 639: val_loss did not improve from 0.13110\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2383 - accuracy: 0.9088 - val_loss: 0.1401 - val_accuracy: 0.9452\n",
      "Epoch 640/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2811 - accuracy: 0.8952\n",
      "Epoch 640: val_loss did not improve from 0.13110\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2808 - accuracy: 0.8954 - val_loss: 0.1763 - val_accuracy: 0.9358\n",
      "Epoch 641/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2614 - accuracy: 0.9007\n",
      "Epoch 641: val_loss did not improve from 0.13110\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.2618 - accuracy: 0.9004 - val_loss: 0.1602 - val_accuracy: 0.9394\n",
      "Epoch 642/1000\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.2566 - accuracy: 0.9004\n",
      "Epoch 642: val_loss did not improve from 0.13110\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.2556 - accuracy: 0.9010 - val_loss: 0.1408 - val_accuracy: 0.9462\n",
      "Epoch 643/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2790 - accuracy: 0.8963\n",
      "Epoch 643: val_loss did not improve from 0.13110\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2787 - accuracy: 0.8965 - val_loss: 0.1603 - val_accuracy: 0.9408\n",
      "Epoch 644/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2617 - accuracy: 0.8997\n",
      "Epoch 644: val_loss did not improve from 0.13110\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2617 - accuracy: 0.8998 - val_loss: 0.1366 - val_accuracy: 0.9495\n",
      "Epoch 645/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2358 - accuracy: 0.9083\n",
      "Epoch 645: val_loss did not improve from 0.13110\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2354 - accuracy: 0.9084 - val_loss: 0.1500 - val_accuracy: 0.9417\n",
      "Epoch 646/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2383 - accuracy: 0.9084\n",
      "Epoch 646: val_loss did not improve from 0.13110\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2381 - accuracy: 0.9085 - val_loss: 0.1419 - val_accuracy: 0.9478\n",
      "Epoch 647/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2247 - accuracy: 0.9148\n",
      "Epoch 647: val_loss did not improve from 0.13110\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2247 - accuracy: 0.9148 - val_loss: 0.1369 - val_accuracy: 0.9470\n",
      "Epoch 648/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2319 - accuracy: 0.9110\n",
      "Epoch 648: val_loss did not improve from 0.13110\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2319 - accuracy: 0.9109 - val_loss: 0.2617 - val_accuracy: 0.9105\n",
      "Epoch 649/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2912 - accuracy: 0.8915\n",
      "Epoch 649: val_loss did not improve from 0.13110\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2913 - accuracy: 0.8914 - val_loss: 0.1483 - val_accuracy: 0.9436\n",
      "Epoch 650/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2386 - accuracy: 0.9080\n",
      "Epoch 650: val_loss did not improve from 0.13110\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2387 - accuracy: 0.9080 - val_loss: 0.1370 - val_accuracy: 0.9519\n",
      "Epoch 651/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2470 - accuracy: 0.9064\n",
      "Epoch 651: val_loss did not improve from 0.13110\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2469 - accuracy: 0.9065 - val_loss: 0.1507 - val_accuracy: 0.9431\n",
      "Epoch 652/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2447 - accuracy: 0.9054\n",
      "Epoch 652: val_loss did not improve from 0.13110\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2444 - accuracy: 0.9055 - val_loss: 0.1578 - val_accuracy: 0.9389\n",
      "Epoch 653/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2869 - accuracy: 0.8959\n",
      "Epoch 653: val_loss did not improve from 0.13110\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2867 - accuracy: 0.8960 - val_loss: 0.1709 - val_accuracy: 0.9370\n",
      "Epoch 654/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2500 - accuracy: 0.9030\n",
      "Epoch 654: val_loss did not improve from 0.13110\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2502 - accuracy: 0.9028 - val_loss: 0.1522 - val_accuracy: 0.9384\n",
      "Epoch 655/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2404 - accuracy: 0.9081\n",
      "Epoch 655: val_loss did not improve from 0.13110\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2410 - accuracy: 0.9079 - val_loss: 0.1379 - val_accuracy: 0.9479\n",
      "Epoch 656/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2576 - accuracy: 0.9024\n",
      "Epoch 656: val_loss did not improve from 0.13110\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2575 - accuracy: 0.9024 - val_loss: 0.1317 - val_accuracy: 0.9504\n",
      "Epoch 657/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2248 - accuracy: 0.9131\n",
      "Epoch 657: val_loss did not improve from 0.13110\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2248 - accuracy: 0.9131 - val_loss: 0.1337 - val_accuracy: 0.9508\n",
      "Epoch 658/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2471 - accuracy: 0.9067\n",
      "Epoch 658: val_loss did not improve from 0.13110\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2470 - accuracy: 0.9068 - val_loss: 0.1680 - val_accuracy: 0.9340\n",
      "Epoch 659/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2430 - accuracy: 0.9088\n",
      "Epoch 659: val_loss did not improve from 0.13110\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2431 - accuracy: 0.9087 - val_loss: 0.1347 - val_accuracy: 0.9527\n",
      "Epoch 660/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2646 - accuracy: 0.9005\n",
      "Epoch 660: val_loss did not improve from 0.13110\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2642 - accuracy: 0.9006 - val_loss: 0.1960 - val_accuracy: 0.9277\n",
      "Epoch 661/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2484 - accuracy: 0.9068\n",
      "Epoch 661: val_loss did not improve from 0.13110\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2483 - accuracy: 0.9068 - val_loss: 0.1553 - val_accuracy: 0.9419\n",
      "Epoch 662/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2249 - accuracy: 0.9138\n",
      "Epoch 662: val_loss did not improve from 0.13110\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2246 - accuracy: 0.9140 - val_loss: 0.1381 - val_accuracy: 0.9475\n",
      "Epoch 663/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2671 - accuracy: 0.9003\n",
      "Epoch 663: val_loss did not improve from 0.13110\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2668 - accuracy: 0.9004 - val_loss: 0.1947 - val_accuracy: 0.9306\n",
      "Epoch 664/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2416 - accuracy: 0.9073\n",
      "Epoch 664: val_loss improved from 0.13110 to 0.13015, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.2422 - accuracy: 0.9072 - val_loss: 0.1302 - val_accuracy: 0.9490\n",
      "Epoch 665/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2529 - accuracy: 0.9043\n",
      "Epoch 665: val_loss did not improve from 0.13015\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2527 - accuracy: 0.9043 - val_loss: 0.1495 - val_accuracy: 0.9409\n",
      "Epoch 666/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2472 - accuracy: 0.9044\n",
      "Epoch 666: val_loss did not improve from 0.13015\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2475 - accuracy: 0.9043 - val_loss: 0.1576 - val_accuracy: 0.9403\n",
      "Epoch 667/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2362 - accuracy: 0.9093\n",
      "Epoch 667: val_loss did not improve from 0.13015\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2359 - accuracy: 0.9094 - val_loss: 0.1517 - val_accuracy: 0.9459\n",
      "Epoch 668/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2438 - accuracy: 0.9067\n",
      "Epoch 668: val_loss did not improve from 0.13015\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2435 - accuracy: 0.9068 - val_loss: 0.1373 - val_accuracy: 0.9501\n",
      "Epoch 669/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2308 - accuracy: 0.9128\n",
      "Epoch 669: val_loss improved from 0.13015 to 0.12886, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.2305 - accuracy: 0.9129 - val_loss: 0.1289 - val_accuracy: 0.9530\n",
      "Epoch 670/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2386 - accuracy: 0.9062\n",
      "Epoch 670: val_loss did not improve from 0.12886\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2383 - accuracy: 0.9062 - val_loss: 0.1373 - val_accuracy: 0.9456\n",
      "Epoch 671/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2220 - accuracy: 0.9139\n",
      "Epoch 671: val_loss did not improve from 0.12886\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2220 - accuracy: 0.9139 - val_loss: 0.1293 - val_accuracy: 0.9503\n",
      "Epoch 672/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2325 - accuracy: 0.9135\n",
      "Epoch 672: val_loss did not improve from 0.12886\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2325 - accuracy: 0.9135 - val_loss: 0.1470 - val_accuracy: 0.9466\n",
      "Epoch 673/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2312 - accuracy: 0.9099\n",
      "Epoch 673: val_loss did not improve from 0.12886\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2310 - accuracy: 0.9100 - val_loss: 0.1352 - val_accuracy: 0.9502\n",
      "Epoch 674/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2318 - accuracy: 0.9130\n",
      "Epoch 674: val_loss did not improve from 0.12886\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2317 - accuracy: 0.9131 - val_loss: 0.1401 - val_accuracy: 0.9474\n",
      "Epoch 675/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2383 - accuracy: 0.9094\n",
      "Epoch 675: val_loss did not improve from 0.12886\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2381 - accuracy: 0.9095 - val_loss: 0.1319 - val_accuracy: 0.9515\n",
      "Epoch 676/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2172 - accuracy: 0.9161\n",
      "Epoch 676: val_loss did not improve from 0.12886\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2173 - accuracy: 0.9159 - val_loss: 0.1297 - val_accuracy: 0.9503\n",
      "Epoch 677/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2336 - accuracy: 0.9114\n",
      "Epoch 677: val_loss did not improve from 0.12886\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2338 - accuracy: 0.9113 - val_loss: 0.1381 - val_accuracy: 0.9456\n",
      "Epoch 678/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2758 - accuracy: 0.8977\n",
      "Epoch 678: val_loss improved from 0.12886 to 0.12717, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.2758 - accuracy: 0.8977 - val_loss: 0.1272 - val_accuracy: 0.9519\n",
      "Epoch 679/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2389 - accuracy: 0.9086\n",
      "Epoch 679: val_loss did not improve from 0.12717\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2385 - accuracy: 0.9087 - val_loss: 0.1317 - val_accuracy: 0.9512\n",
      "Epoch 680/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2131 - accuracy: 0.9179\n",
      "Epoch 680: val_loss did not improve from 0.12717\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2129 - accuracy: 0.9179 - val_loss: 0.1352 - val_accuracy: 0.9458\n",
      "Epoch 681/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2442 - accuracy: 0.9068\n",
      "Epoch 681: val_loss did not improve from 0.12717\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2440 - accuracy: 0.9069 - val_loss: 0.1298 - val_accuracy: 0.9522\n",
      "Epoch 682/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2572 - accuracy: 0.9032\n",
      "Epoch 682: val_loss did not improve from 0.12717\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2571 - accuracy: 0.9033 - val_loss: 0.1473 - val_accuracy: 0.9439\n",
      "Epoch 683/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2387 - accuracy: 0.9086\n",
      "Epoch 683: val_loss did not improve from 0.12717\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2390 - accuracy: 0.9085 - val_loss: 0.1438 - val_accuracy: 0.9450\n",
      "Epoch 684/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2573 - accuracy: 0.9044\n",
      "Epoch 684: val_loss did not improve from 0.12717\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2572 - accuracy: 0.9044 - val_loss: 0.1544 - val_accuracy: 0.9420\n",
      "Epoch 685/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2421 - accuracy: 0.9072\n",
      "Epoch 685: val_loss did not improve from 0.12717\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2419 - accuracy: 0.9073 - val_loss: 0.1593 - val_accuracy: 0.9368\n",
      "Epoch 686/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2263 - accuracy: 0.9135\n",
      "Epoch 686: val_loss did not improve from 0.12717\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2262 - accuracy: 0.9135 - val_loss: 0.1279 - val_accuracy: 0.9534\n",
      "Epoch 687/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3462 - accuracy: 0.8796\n",
      "Epoch 687: val_loss did not improve from 0.12717\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3459 - accuracy: 0.8797 - val_loss: 0.1618 - val_accuracy: 0.9415\n",
      "Epoch 688/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2445 - accuracy: 0.9093\n",
      "Epoch 688: val_loss did not improve from 0.12717\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2445 - accuracy: 0.9093 - val_loss: 0.1405 - val_accuracy: 0.9455\n",
      "Epoch 689/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2283 - accuracy: 0.9148\n",
      "Epoch 689: val_loss did not improve from 0.12717\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2281 - accuracy: 0.9148 - val_loss: 0.1281 - val_accuracy: 0.9534\n",
      "Epoch 690/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2274 - accuracy: 0.9122\n",
      "Epoch 690: val_loss did not improve from 0.12717\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2272 - accuracy: 0.9123 - val_loss: 0.1328 - val_accuracy: 0.9508\n",
      "Epoch 691/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2302 - accuracy: 0.9137\n",
      "Epoch 691: val_loss did not improve from 0.12717\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2301 - accuracy: 0.9138 - val_loss: 0.1344 - val_accuracy: 0.9491\n",
      "Epoch 692/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2306 - accuracy: 0.9123\n",
      "Epoch 692: val_loss did not improve from 0.12717\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2305 - accuracy: 0.9123 - val_loss: 0.1336 - val_accuracy: 0.9519\n",
      "Epoch 693/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2244 - accuracy: 0.9142\n",
      "Epoch 693: val_loss did not improve from 0.12717\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2244 - accuracy: 0.9142 - val_loss: 0.1364 - val_accuracy: 0.9478\n",
      "Epoch 694/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2161 - accuracy: 0.9174\n",
      "Epoch 694: val_loss improved from 0.12717 to 0.12414, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.2161 - accuracy: 0.9174 - val_loss: 0.1241 - val_accuracy: 0.9533\n",
      "Epoch 695/1000\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.2024 - accuracy: 0.9223\n",
      "Epoch 695: val_loss did not improve from 0.12414\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2026 - accuracy: 0.9220 - val_loss: 0.1550 - val_accuracy: 0.9414\n",
      "Epoch 696/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2284 - accuracy: 0.9126\n",
      "Epoch 696: val_loss did not improve from 0.12414\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2286 - accuracy: 0.9124 - val_loss: 0.1453 - val_accuracy: 0.9474\n",
      "Epoch 697/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2301 - accuracy: 0.9143\n",
      "Epoch 697: val_loss did not improve from 0.12414\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2301 - accuracy: 0.9142 - val_loss: 0.2049 - val_accuracy: 0.9273\n",
      "Epoch 698/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3684 - accuracy: 0.8762\n",
      "Epoch 698: val_loss did not improve from 0.12414\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3688 - accuracy: 0.8761 - val_loss: 0.1360 - val_accuracy: 0.9506\n",
      "Epoch 699/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3125 - accuracy: 0.8883\n",
      "Epoch 699: val_loss did not improve from 0.12414\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3123 - accuracy: 0.8884 - val_loss: 0.1412 - val_accuracy: 0.9469\n",
      "Epoch 700/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2538 - accuracy: 0.9058\n",
      "Epoch 700: val_loss did not improve from 0.12414\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2537 - accuracy: 0.9058 - val_loss: 0.1484 - val_accuracy: 0.9417\n",
      "Epoch 701/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2338 - accuracy: 0.9114\n",
      "Epoch 701: val_loss did not improve from 0.12414\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2336 - accuracy: 0.9115 - val_loss: 0.1315 - val_accuracy: 0.9488\n",
      "Epoch 702/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2270 - accuracy: 0.9125\n",
      "Epoch 702: val_loss did not improve from 0.12414\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2268 - accuracy: 0.9126 - val_loss: 0.1525 - val_accuracy: 0.9397\n",
      "Epoch 703/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2487 - accuracy: 0.9080\n",
      "Epoch 703: val_loss did not improve from 0.12414\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2486 - accuracy: 0.9080 - val_loss: 0.1372 - val_accuracy: 0.9509\n",
      "Epoch 704/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2352 - accuracy: 0.9126\n",
      "Epoch 704: val_loss did not improve from 0.12414\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2354 - accuracy: 0.9126 - val_loss: 0.1321 - val_accuracy: 0.9528\n",
      "Epoch 705/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2303 - accuracy: 0.9122\n",
      "Epoch 705: val_loss did not improve from 0.12414\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2305 - accuracy: 0.9121 - val_loss: 0.1555 - val_accuracy: 0.9418\n",
      "Epoch 706/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2253 - accuracy: 0.9153\n",
      "Epoch 706: val_loss did not improve from 0.12414\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2252 - accuracy: 0.9154 - val_loss: 0.1334 - val_accuracy: 0.9493\n",
      "Epoch 707/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2465 - accuracy: 0.9055\n",
      "Epoch 707: val_loss did not improve from 0.12414\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2464 - accuracy: 0.9055 - val_loss: 0.1486 - val_accuracy: 0.9429\n",
      "Epoch 708/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2327 - accuracy: 0.9116\n",
      "Epoch 708: val_loss did not improve from 0.12414\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2328 - accuracy: 0.9115 - val_loss: 0.1297 - val_accuracy: 0.9513\n",
      "Epoch 709/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2368 - accuracy: 0.9124\n",
      "Epoch 709: val_loss did not improve from 0.12414\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2367 - accuracy: 0.9124 - val_loss: 0.1278 - val_accuracy: 0.9522\n",
      "Epoch 710/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2164 - accuracy: 0.9168\n",
      "Epoch 710: val_loss did not improve from 0.12414\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2166 - accuracy: 0.9168 - val_loss: 0.1420 - val_accuracy: 0.9472\n",
      "Epoch 711/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2076 - accuracy: 0.9206\n",
      "Epoch 711: val_loss did not improve from 0.12414\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.2081 - accuracy: 0.9204 - val_loss: 0.1263 - val_accuracy: 0.9504\n",
      "Epoch 712/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2496 - accuracy: 0.9063\n",
      "Epoch 712: val_loss improved from 0.12414 to 0.12169, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.2497 - accuracy: 0.9063 - val_loss: 0.1217 - val_accuracy: 0.9537\n",
      "Epoch 713/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2033 - accuracy: 0.9210\n",
      "Epoch 713: val_loss improved from 0.12169 to 0.11264, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 19ms/step - loss: 0.2031 - accuracy: 0.9211 - val_loss: 0.1126 - val_accuracy: 0.9590\n",
      "Epoch 714/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2146 - accuracy: 0.9180\n",
      "Epoch 714: val_loss did not improve from 0.11264\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2146 - accuracy: 0.9181 - val_loss: 0.1363 - val_accuracy: 0.9512\n",
      "Epoch 715/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2072 - accuracy: 0.9213\n",
      "Epoch 715: val_loss improved from 0.11264 to 0.11063, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.2074 - accuracy: 0.9212 - val_loss: 0.1106 - val_accuracy: 0.9592\n",
      "Epoch 716/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1982 - accuracy: 0.9243\n",
      "Epoch 716: val_loss did not improve from 0.11063\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.1981 - accuracy: 0.9243 - val_loss: 0.1374 - val_accuracy: 0.9448\n",
      "Epoch 717/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2859 - accuracy: 0.8981\n",
      "Epoch 717: val_loss did not improve from 0.11063\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2863 - accuracy: 0.8978 - val_loss: 0.1566 - val_accuracy: 0.9427\n",
      "Epoch 718/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2252 - accuracy: 0.9157\n",
      "Epoch 718: val_loss did not improve from 0.11063\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2255 - accuracy: 0.9155 - val_loss: 0.1430 - val_accuracy: 0.9493\n",
      "Epoch 719/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2355 - accuracy: 0.9144\n",
      "Epoch 719: val_loss did not improve from 0.11063\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2353 - accuracy: 0.9145 - val_loss: 0.1373 - val_accuracy: 0.9491\n",
      "Epoch 720/1000\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.2093 - accuracy: 0.9188\n",
      "Epoch 720: val_loss did not improve from 0.11063\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2093 - accuracy: 0.9188 - val_loss: 0.1277 - val_accuracy: 0.9517\n",
      "Epoch 721/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2022 - accuracy: 0.9224\n",
      "Epoch 721: val_loss did not improve from 0.11063\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2021 - accuracy: 0.9224 - val_loss: 0.1302 - val_accuracy: 0.9509\n",
      "Epoch 722/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2138 - accuracy: 0.9185\n",
      "Epoch 722: val_loss did not improve from 0.11063\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2139 - accuracy: 0.9185 - val_loss: 0.1196 - val_accuracy: 0.9535\n",
      "Epoch 723/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2239 - accuracy: 0.9136\n",
      "Epoch 723: val_loss did not improve from 0.11063\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2239 - accuracy: 0.9137 - val_loss: 0.1273 - val_accuracy: 0.9525\n",
      "Epoch 724/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2279 - accuracy: 0.9139\n",
      "Epoch 724: val_loss did not improve from 0.11063\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2277 - accuracy: 0.9140 - val_loss: 0.1189 - val_accuracy: 0.9553\n",
      "Epoch 725/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2094 - accuracy: 0.9212\n",
      "Epoch 725: val_loss did not improve from 0.11063\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2097 - accuracy: 0.9210 - val_loss: 0.1172 - val_accuracy: 0.9566\n",
      "Epoch 726/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2203 - accuracy: 0.9159\n",
      "Epoch 726: val_loss did not improve from 0.11063\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2217 - accuracy: 0.9156 - val_loss: 0.1217 - val_accuracy: 0.9532\n",
      "Epoch 727/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2986 - accuracy: 0.8954\n",
      "Epoch 727: val_loss did not improve from 0.11063\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2982 - accuracy: 0.8956 - val_loss: 0.1757 - val_accuracy: 0.9376\n",
      "Epoch 728/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2439 - accuracy: 0.9074\n",
      "Epoch 728: val_loss did not improve from 0.11063\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2438 - accuracy: 0.9074 - val_loss: 0.1371 - val_accuracy: 0.9514\n",
      "Epoch 729/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2192 - accuracy: 0.9174\n",
      "Epoch 729: val_loss did not improve from 0.11063\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2195 - accuracy: 0.9173 - val_loss: 0.1423 - val_accuracy: 0.9445\n",
      "Epoch 730/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2225 - accuracy: 0.9143\n",
      "Epoch 730: val_loss did not improve from 0.11063\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2226 - accuracy: 0.9143 - val_loss: 0.4399 - val_accuracy: 0.8725\n",
      "Epoch 731/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2578 - accuracy: 0.9065\n",
      "Epoch 731: val_loss did not improve from 0.11063\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2577 - accuracy: 0.9066 - val_loss: 0.1288 - val_accuracy: 0.9509\n",
      "Epoch 732/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2146 - accuracy: 0.9185\n",
      "Epoch 732: val_loss did not improve from 0.11063\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2144 - accuracy: 0.9186 - val_loss: 0.1408 - val_accuracy: 0.9484\n",
      "Epoch 733/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2032 - accuracy: 0.9232\n",
      "Epoch 733: val_loss did not improve from 0.11063\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2030 - accuracy: 0.9233 - val_loss: 0.1202 - val_accuracy: 0.9574\n",
      "Epoch 734/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1982 - accuracy: 0.9234\n",
      "Epoch 734: val_loss did not improve from 0.11063\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1981 - accuracy: 0.9235 - val_loss: 0.1238 - val_accuracy: 0.9519\n",
      "Epoch 735/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2074 - accuracy: 0.9199\n",
      "Epoch 735: val_loss did not improve from 0.11063\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2071 - accuracy: 0.9200 - val_loss: 0.1224 - val_accuracy: 0.9591\n",
      "Epoch 736/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2218 - accuracy: 0.9161\n",
      "Epoch 736: val_loss did not improve from 0.11063\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.2232 - accuracy: 0.9160 - val_loss: 0.1240 - val_accuracy: 0.9532\n",
      "Epoch 737/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2275 - accuracy: 0.9142\n",
      "Epoch 737: val_loss did not improve from 0.11063\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2272 - accuracy: 0.9143 - val_loss: 0.1234 - val_accuracy: 0.9578\n",
      "Epoch 738/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2133 - accuracy: 0.9186\n",
      "Epoch 738: val_loss did not improve from 0.11063\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2136 - accuracy: 0.9186 - val_loss: 0.1248 - val_accuracy: 0.9525\n",
      "Epoch 739/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2376 - accuracy: 0.9112\n",
      "Epoch 739: val_loss did not improve from 0.11063\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2395 - accuracy: 0.9109 - val_loss: 0.1650 - val_accuracy: 0.9406\n",
      "Epoch 740/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.9650 - accuracy: 0.6383\n",
      "Epoch 740: val_loss did not improve from 0.11063\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.9649 - accuracy: 0.6388 - val_loss: 0.7396 - val_accuracy: 0.7114\n",
      "Epoch 741/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6094 - accuracy: 0.7648\n",
      "Epoch 741: val_loss did not improve from 0.11063\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.6089 - accuracy: 0.7650 - val_loss: 0.3738 - val_accuracy: 0.8704\n",
      "Epoch 742/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5131 - accuracy: 0.8172\n",
      "Epoch 742: val_loss did not improve from 0.11063\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.5127 - accuracy: 0.8173 - val_loss: 0.2863 - val_accuracy: 0.9001\n",
      "Epoch 743/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3496 - accuracy: 0.8746\n",
      "Epoch 743: val_loss did not improve from 0.11063\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3493 - accuracy: 0.8747 - val_loss: 0.2023 - val_accuracy: 0.9279\n",
      "Epoch 744/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3505 - accuracy: 0.8706\n",
      "Epoch 744: val_loss did not improve from 0.11063\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3506 - accuracy: 0.8706 - val_loss: 0.2438 - val_accuracy: 0.9172\n",
      "Epoch 745/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3107 - accuracy: 0.8875\n",
      "Epoch 745: val_loss did not improve from 0.11063\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3108 - accuracy: 0.8874 - val_loss: 0.1600 - val_accuracy: 0.9455\n",
      "Epoch 746/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3396 - accuracy: 0.8742\n",
      "Epoch 746: val_loss did not improve from 0.11063\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3402 - accuracy: 0.8740 - val_loss: 0.1658 - val_accuracy: 0.9397\n",
      "Epoch 747/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3096 - accuracy: 0.8866\n",
      "Epoch 747: val_loss did not improve from 0.11063\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3096 - accuracy: 0.8865 - val_loss: 0.1673 - val_accuracy: 0.9379\n",
      "Epoch 748/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2577 - accuracy: 0.9030\n",
      "Epoch 748: val_loss did not improve from 0.11063\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2575 - accuracy: 0.9031 - val_loss: 0.1427 - val_accuracy: 0.9478\n",
      "Epoch 749/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2569 - accuracy: 0.9037\n",
      "Epoch 749: val_loss did not improve from 0.11063\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2572 - accuracy: 0.9037 - val_loss: 0.1349 - val_accuracy: 0.9504\n",
      "Epoch 750/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2259 - accuracy: 0.9139\n",
      "Epoch 750: val_loss did not improve from 0.11063\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2257 - accuracy: 0.9140 - val_loss: 0.1327 - val_accuracy: 0.9488\n",
      "Epoch 751/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2022 - accuracy: 0.9215\n",
      "Epoch 751: val_loss did not improve from 0.11063\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2019 - accuracy: 0.9216 - val_loss: 0.1231 - val_accuracy: 0.9537\n",
      "Epoch 752/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2206 - accuracy: 0.9159\n",
      "Epoch 752: val_loss did not improve from 0.11063\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2205 - accuracy: 0.9159 - val_loss: 0.1289 - val_accuracy: 0.9522\n",
      "Epoch 753/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2141 - accuracy: 0.9177\n",
      "Epoch 753: val_loss did not improve from 0.11063\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2143 - accuracy: 0.9176 - val_loss: 0.1273 - val_accuracy: 0.9505\n",
      "Epoch 754/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2291 - accuracy: 0.9131\n",
      "Epoch 754: val_loss did not improve from 0.11063\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2290 - accuracy: 0.9131 - val_loss: 0.1284 - val_accuracy: 0.9524\n",
      "Epoch 755/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2046 - accuracy: 0.9217\n",
      "Epoch 755: val_loss did not improve from 0.11063\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2046 - accuracy: 0.9216 - val_loss: 0.1204 - val_accuracy: 0.9578\n",
      "Epoch 756/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2103 - accuracy: 0.9193\n",
      "Epoch 756: val_loss did not improve from 0.11063\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2105 - accuracy: 0.9193 - val_loss: 0.1263 - val_accuracy: 0.9525\n",
      "Epoch 757/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2127 - accuracy: 0.9201\n",
      "Epoch 757: val_loss did not improve from 0.11063\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.2126 - accuracy: 0.9202 - val_loss: 0.1444 - val_accuracy: 0.9494\n",
      "Epoch 758/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2204 - accuracy: 0.9174\n",
      "Epoch 758: val_loss did not improve from 0.11063\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2207 - accuracy: 0.9174 - val_loss: 0.1521 - val_accuracy: 0.9447\n",
      "Epoch 759/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2198 - accuracy: 0.9165\n",
      "Epoch 759: val_loss did not improve from 0.11063\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2200 - accuracy: 0.9164 - val_loss: 0.1511 - val_accuracy: 0.9410\n",
      "Epoch 760/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2059 - accuracy: 0.9213\n",
      "Epoch 760: val_loss did not improve from 0.11063\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2059 - accuracy: 0.9213 - val_loss: 0.1135 - val_accuracy: 0.9570\n",
      "Epoch 761/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1972 - accuracy: 0.9250\n",
      "Epoch 761: val_loss did not improve from 0.11063\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1972 - accuracy: 0.9251 - val_loss: 0.1130 - val_accuracy: 0.9588\n",
      "Epoch 762/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2466 - accuracy: 0.9082\n",
      "Epoch 762: val_loss did not improve from 0.11063\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2465 - accuracy: 0.9083 - val_loss: 0.1281 - val_accuracy: 0.9508\n",
      "Epoch 763/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2151 - accuracy: 0.9181\n",
      "Epoch 763: val_loss did not improve from 0.11063\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2150 - accuracy: 0.9181 - val_loss: 0.1241 - val_accuracy: 0.9543\n",
      "Epoch 764/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1916 - accuracy: 0.9263\n",
      "Epoch 764: val_loss did not improve from 0.11063\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1917 - accuracy: 0.9263 - val_loss: 0.1174 - val_accuracy: 0.9530\n",
      "Epoch 765/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2055 - accuracy: 0.9224\n",
      "Epoch 765: val_loss did not improve from 0.11063\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2051 - accuracy: 0.9225 - val_loss: 0.1586 - val_accuracy: 0.9434\n",
      "Epoch 766/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3737 - accuracy: 0.8769\n",
      "Epoch 766: val_loss did not improve from 0.11063\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3732 - accuracy: 0.8771 - val_loss: 0.1514 - val_accuracy: 0.9436\n",
      "Epoch 767/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2478 - accuracy: 0.9091\n",
      "Epoch 767: val_loss did not improve from 0.11063\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2477 - accuracy: 0.9091 - val_loss: 0.1566 - val_accuracy: 0.9451\n",
      "Epoch 768/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2169 - accuracy: 0.9189\n",
      "Epoch 768: val_loss improved from 0.11063 to 0.10826, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 19ms/step - loss: 0.2167 - accuracy: 0.9190 - val_loss: 0.1083 - val_accuracy: 0.9602\n",
      "Epoch 769/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2070 - accuracy: 0.9211\n",
      "Epoch 769: val_loss did not improve from 0.10826\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2073 - accuracy: 0.9210 - val_loss: 0.1118 - val_accuracy: 0.9593\n",
      "Epoch 770/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2031 - accuracy: 0.9235\n",
      "Epoch 770: val_loss did not improve from 0.10826\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2029 - accuracy: 0.9236 - val_loss: 0.1253 - val_accuracy: 0.9544\n",
      "Epoch 771/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2489 - accuracy: 0.9088\n",
      "Epoch 771: val_loss did not improve from 0.10826\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2486 - accuracy: 0.9089 - val_loss: 0.1121 - val_accuracy: 0.9576\n",
      "Epoch 772/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1956 - accuracy: 0.9251\n",
      "Epoch 772: val_loss did not improve from 0.10826\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1954 - accuracy: 0.9252 - val_loss: 0.1116 - val_accuracy: 0.9568\n",
      "Epoch 773/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1912 - accuracy: 0.9275\n",
      "Epoch 773: val_loss did not improve from 0.10826\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1914 - accuracy: 0.9274 - val_loss: 0.1220 - val_accuracy: 0.9518\n",
      "Epoch 774/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2039 - accuracy: 0.9242\n",
      "Epoch 774: val_loss did not improve from 0.10826\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2041 - accuracy: 0.9242 - val_loss: 0.1159 - val_accuracy: 0.9563\n",
      "Epoch 775/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2038 - accuracy: 0.9232\n",
      "Epoch 775: val_loss did not improve from 0.10826\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2038 - accuracy: 0.9233 - val_loss: 0.1183 - val_accuracy: 0.9574\n",
      "Epoch 776/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2177 - accuracy: 0.9193\n",
      "Epoch 776: val_loss did not improve from 0.10826\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2176 - accuracy: 0.9194 - val_loss: 0.1174 - val_accuracy: 0.9591\n",
      "Epoch 777/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2178 - accuracy: 0.9191\n",
      "Epoch 777: val_loss did not improve from 0.10826\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2177 - accuracy: 0.9191 - val_loss: 0.1277 - val_accuracy: 0.9522\n",
      "Epoch 778/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2014 - accuracy: 0.9230\n",
      "Epoch 778: val_loss did not improve from 0.10826\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2012 - accuracy: 0.9231 - val_loss: 0.1279 - val_accuracy: 0.9516\n",
      "Epoch 779/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1995 - accuracy: 0.9241\n",
      "Epoch 779: val_loss did not improve from 0.10826\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1997 - accuracy: 0.9240 - val_loss: 0.1093 - val_accuracy: 0.9577\n",
      "Epoch 780/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2101 - accuracy: 0.9197\n",
      "Epoch 780: val_loss did not improve from 0.10826\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2102 - accuracy: 0.9196 - val_loss: 0.1335 - val_accuracy: 0.9482\n",
      "Epoch 781/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2395 - accuracy: 0.9137\n",
      "Epoch 781: val_loss did not improve from 0.10826\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.2392 - accuracy: 0.9139 - val_loss: 0.1312 - val_accuracy: 0.9507\n",
      "Epoch 782/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2154 - accuracy: 0.9192\n",
      "Epoch 782: val_loss did not improve from 0.10826\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2156 - accuracy: 0.9191 - val_loss: 0.1268 - val_accuracy: 0.9527\n",
      "Epoch 783/1000\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.2011 - accuracy: 0.9218\n",
      "Epoch 783: val_loss did not improve from 0.10826\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.2007 - accuracy: 0.9219 - val_loss: 0.1148 - val_accuracy: 0.9572\n",
      "Epoch 784/1000\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.2366 - accuracy: 0.9146\n",
      "Epoch 784: val_loss did not improve from 0.10826\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.2372 - accuracy: 0.9142 - val_loss: 0.1499 - val_accuracy: 0.9455\n",
      "Epoch 785/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2466 - accuracy: 0.9098\n",
      "Epoch 785: val_loss did not improve from 0.10826\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2463 - accuracy: 0.9099 - val_loss: 0.1317 - val_accuracy: 0.9509\n",
      "Epoch 786/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2387 - accuracy: 0.9131\n",
      "Epoch 786: val_loss did not improve from 0.10826\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2385 - accuracy: 0.9132 - val_loss: 0.1560 - val_accuracy: 0.9402\n",
      "Epoch 787/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2050 - accuracy: 0.9209\n",
      "Epoch 787: val_loss did not improve from 0.10826\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2049 - accuracy: 0.9210 - val_loss: 0.1117 - val_accuracy: 0.9581\n",
      "Epoch 788/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2160 - accuracy: 0.9175\n",
      "Epoch 788: val_loss did not improve from 0.10826\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2160 - accuracy: 0.9175 - val_loss: 0.1245 - val_accuracy: 0.9546\n",
      "Epoch 789/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1977 - accuracy: 0.9263\n",
      "Epoch 789: val_loss did not improve from 0.10826\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1979 - accuracy: 0.9263 - val_loss: 0.1178 - val_accuracy: 0.9528\n",
      "Epoch 790/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2015 - accuracy: 0.9234\n",
      "Epoch 790: val_loss did not improve from 0.10826\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2013 - accuracy: 0.9235 - val_loss: 0.1157 - val_accuracy: 0.9551\n",
      "Epoch 791/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1952 - accuracy: 0.9258\n",
      "Epoch 791: val_loss did not improve from 0.10826\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1951 - accuracy: 0.9258 - val_loss: 0.1617 - val_accuracy: 0.9433\n",
      "Epoch 792/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2287 - accuracy: 0.9162\n",
      "Epoch 792: val_loss did not improve from 0.10826\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2285 - accuracy: 0.9162 - val_loss: 0.1145 - val_accuracy: 0.9560\n",
      "Epoch 793/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2154 - accuracy: 0.9208\n",
      "Epoch 793: val_loss did not improve from 0.10826\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2152 - accuracy: 0.9209 - val_loss: 0.1445 - val_accuracy: 0.9478\n",
      "Epoch 794/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2207 - accuracy: 0.9193\n",
      "Epoch 794: val_loss did not improve from 0.10826\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2206 - accuracy: 0.9194 - val_loss: 0.1396 - val_accuracy: 0.9495\n",
      "Epoch 795/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2086 - accuracy: 0.9213\n",
      "Epoch 795: val_loss did not improve from 0.10826\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2084 - accuracy: 0.9214 - val_loss: 0.1114 - val_accuracy: 0.9611\n",
      "Epoch 796/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2205 - accuracy: 0.9171\n",
      "Epoch 796: val_loss did not improve from 0.10826\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2204 - accuracy: 0.9171 - val_loss: 0.1378 - val_accuracy: 0.9477\n",
      "Epoch 797/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2305 - accuracy: 0.9147\n",
      "Epoch 797: val_loss did not improve from 0.10826\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2305 - accuracy: 0.9147 - val_loss: 0.1230 - val_accuracy: 0.9531\n",
      "Epoch 798/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2070 - accuracy: 0.9217\n",
      "Epoch 798: val_loss did not improve from 0.10826\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2069 - accuracy: 0.9218 - val_loss: 0.1630 - val_accuracy: 0.9381\n",
      "Epoch 799/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2330 - accuracy: 0.9155\n",
      "Epoch 799: val_loss did not improve from 0.10826\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2329 - accuracy: 0.9155 - val_loss: 0.1228 - val_accuracy: 0.9529\n",
      "Epoch 800/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2035 - accuracy: 0.9242\n",
      "Epoch 800: val_loss did not improve from 0.10826\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2039 - accuracy: 0.9239 - val_loss: 0.1223 - val_accuracy: 0.9542\n",
      "Epoch 801/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2224 - accuracy: 0.9164\n",
      "Epoch 801: val_loss did not improve from 0.10826\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2222 - accuracy: 0.9165 - val_loss: 0.1325 - val_accuracy: 0.9507\n",
      "Epoch 802/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2100 - accuracy: 0.9223\n",
      "Epoch 802: val_loss did not improve from 0.10826\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2098 - accuracy: 0.9224 - val_loss: 0.1181 - val_accuracy: 0.9567\n",
      "Epoch 803/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2881 - accuracy: 0.8990\n",
      "Epoch 803: val_loss did not improve from 0.10826\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2882 - accuracy: 0.8990 - val_loss: 0.1714 - val_accuracy: 0.9432\n",
      "Epoch 804/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2656 - accuracy: 0.9037\n",
      "Epoch 804: val_loss did not improve from 0.10826\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2652 - accuracy: 0.9039 - val_loss: 0.1283 - val_accuracy: 0.9525\n",
      "Epoch 805/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1908 - accuracy: 0.9280\n",
      "Epoch 805: val_loss improved from 0.10826 to 0.10601, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.1908 - accuracy: 0.9280 - val_loss: 0.1060 - val_accuracy: 0.9610\n",
      "Epoch 806/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2226 - accuracy: 0.9203\n",
      "Epoch 806: val_loss did not improve from 0.10601\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2225 - accuracy: 0.9203 - val_loss: 0.1225 - val_accuracy: 0.9554\n",
      "Epoch 807/1000\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.2031 - accuracy: 0.9244\n",
      "Epoch 807: val_loss did not improve from 0.10601\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2024 - accuracy: 0.9247 - val_loss: 0.1251 - val_accuracy: 0.9541\n",
      "Epoch 808/1000\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.2098 - accuracy: 0.9209\n",
      "Epoch 808: val_loss did not improve from 0.10601\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.2097 - accuracy: 0.9211 - val_loss: 0.1119 - val_accuracy: 0.9592\n",
      "Epoch 809/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2044 - accuracy: 0.9225\n",
      "Epoch 809: val_loss did not improve from 0.10601\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2044 - accuracy: 0.9225 - val_loss: 0.1156 - val_accuracy: 0.9567\n",
      "Epoch 810/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2031 - accuracy: 0.9237\n",
      "Epoch 810: val_loss did not improve from 0.10601\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2036 - accuracy: 0.9236 - val_loss: 0.1145 - val_accuracy: 0.9580\n",
      "Epoch 811/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2166 - accuracy: 0.9183\n",
      "Epoch 811: val_loss did not improve from 0.10601\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2166 - accuracy: 0.9183 - val_loss: 0.1216 - val_accuracy: 0.9547\n",
      "Epoch 812/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2020 - accuracy: 0.9225\n",
      "Epoch 812: val_loss did not improve from 0.10601\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2023 - accuracy: 0.9224 - val_loss: 0.1228 - val_accuracy: 0.9547\n",
      "Epoch 813/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2273 - accuracy: 0.9174\n",
      "Epoch 813: val_loss did not improve from 0.10601\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2275 - accuracy: 0.9174 - val_loss: 0.1166 - val_accuracy: 0.9584\n",
      "Epoch 814/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1927 - accuracy: 0.9270\n",
      "Epoch 814: val_loss did not improve from 0.10601\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1930 - accuracy: 0.9268 - val_loss: 0.1136 - val_accuracy: 0.9583\n",
      "Epoch 815/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1971 - accuracy: 0.9236\n",
      "Epoch 815: val_loss did not improve from 0.10601\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1974 - accuracy: 0.9235 - val_loss: 0.1213 - val_accuracy: 0.9541\n",
      "Epoch 816/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2010 - accuracy: 0.9264\n",
      "Epoch 816: val_loss did not improve from 0.10601\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2010 - accuracy: 0.9264 - val_loss: 0.1179 - val_accuracy: 0.9557\n",
      "Epoch 817/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2047 - accuracy: 0.9222\n",
      "Epoch 817: val_loss did not improve from 0.10601\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2046 - accuracy: 0.9222 - val_loss: 0.1191 - val_accuracy: 0.9571\n",
      "Epoch 818/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2241 - accuracy: 0.9187\n",
      "Epoch 818: val_loss did not improve from 0.10601\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2242 - accuracy: 0.9186 - val_loss: 0.1346 - val_accuracy: 0.9471\n",
      "Epoch 819/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2077 - accuracy: 0.9221\n",
      "Epoch 819: val_loss did not improve from 0.10601\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2081 - accuracy: 0.9220 - val_loss: 0.1219 - val_accuracy: 0.9568\n",
      "Epoch 820/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2165 - accuracy: 0.9218\n",
      "Epoch 820: val_loss did not improve from 0.10601\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2165 - accuracy: 0.9219 - val_loss: 0.1420 - val_accuracy: 0.9500\n",
      "Epoch 821/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2031 - accuracy: 0.9242\n",
      "Epoch 821: val_loss did not improve from 0.10601\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2029 - accuracy: 0.9243 - val_loss: 0.1136 - val_accuracy: 0.9575\n",
      "Epoch 822/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2043 - accuracy: 0.9234\n",
      "Epoch 822: val_loss did not improve from 0.10601\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2043 - accuracy: 0.9234 - val_loss: 0.1330 - val_accuracy: 0.9503\n",
      "Epoch 823/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2607 - accuracy: 0.9098\n",
      "Epoch 823: val_loss did not improve from 0.10601\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2609 - accuracy: 0.9097 - val_loss: 0.1131 - val_accuracy: 0.9583\n",
      "Epoch 824/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2163 - accuracy: 0.9173\n",
      "Epoch 824: val_loss did not improve from 0.10601\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2161 - accuracy: 0.9174 - val_loss: 0.1439 - val_accuracy: 0.9466\n",
      "Epoch 825/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2108 - accuracy: 0.9187\n",
      "Epoch 825: val_loss did not improve from 0.10601\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2107 - accuracy: 0.9187 - val_loss: 0.1175 - val_accuracy: 0.9570\n",
      "Epoch 826/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3110 - accuracy: 0.8890\n",
      "Epoch 826: val_loss did not improve from 0.10601\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3108 - accuracy: 0.8890 - val_loss: 0.2543 - val_accuracy: 0.9253\n",
      "Epoch 827/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2437 - accuracy: 0.9125\n",
      "Epoch 827: val_loss did not improve from 0.10601\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2438 - accuracy: 0.9124 - val_loss: 0.1166 - val_accuracy: 0.9572\n",
      "Epoch 828/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2043 - accuracy: 0.9239\n",
      "Epoch 828: val_loss did not improve from 0.10601\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2040 - accuracy: 0.9241 - val_loss: 0.1138 - val_accuracy: 0.9581\n",
      "Epoch 829/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1854 - accuracy: 0.9286\n",
      "Epoch 829: val_loss did not improve from 0.10601\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1858 - accuracy: 0.9283 - val_loss: 0.1106 - val_accuracy: 0.9589\n",
      "Epoch 830/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1979 - accuracy: 0.9257\n",
      "Epoch 830: val_loss improved from 0.10601 to 0.09970, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.1979 - accuracy: 0.9257 - val_loss: 0.0997 - val_accuracy: 0.9637\n",
      "Epoch 831/1000\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.1848 - accuracy: 0.9314\n",
      "Epoch 831: val_loss did not improve from 0.09970\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.1842 - accuracy: 0.9317 - val_loss: 0.1110 - val_accuracy: 0.9570\n",
      "Epoch 832/1000\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.2728 - accuracy: 0.9047\n",
      "Epoch 832: val_loss did not improve from 0.09970\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.2696 - accuracy: 0.9058 - val_loss: 0.1285 - val_accuracy: 0.9542\n",
      "Epoch 833/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2243 - accuracy: 0.9171\n",
      "Epoch 833: val_loss did not improve from 0.09970\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2242 - accuracy: 0.9172 - val_loss: 0.1182 - val_accuracy: 0.9551\n",
      "Epoch 834/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2120 - accuracy: 0.9205\n",
      "Epoch 834: val_loss did not improve from 0.09970\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2121 - accuracy: 0.9204 - val_loss: 0.1174 - val_accuracy: 0.9570\n",
      "Epoch 835/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2075 - accuracy: 0.9218\n",
      "Epoch 835: val_loss did not improve from 0.09970\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2073 - accuracy: 0.9220 - val_loss: 0.1228 - val_accuracy: 0.9535\n",
      "Epoch 836/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3087 - accuracy: 0.8944\n",
      "Epoch 836: val_loss did not improve from 0.09970\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3086 - accuracy: 0.8945 - val_loss: 0.1726 - val_accuracy: 0.9395\n",
      "Epoch 837/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2752 - accuracy: 0.9024\n",
      "Epoch 837: val_loss did not improve from 0.09970\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2753 - accuracy: 0.9024 - val_loss: 0.1451 - val_accuracy: 0.9435\n",
      "Epoch 838/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2134 - accuracy: 0.9206\n",
      "Epoch 838: val_loss did not improve from 0.09970\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2130 - accuracy: 0.9207 - val_loss: 0.1064 - val_accuracy: 0.9585\n",
      "Epoch 839/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1990 - accuracy: 0.9266\n",
      "Epoch 839: val_loss did not improve from 0.09970\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1987 - accuracy: 0.9267 - val_loss: 0.1151 - val_accuracy: 0.9577\n",
      "Epoch 840/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1969 - accuracy: 0.9254\n",
      "Epoch 840: val_loss did not improve from 0.09970\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1967 - accuracy: 0.9255 - val_loss: 0.1107 - val_accuracy: 0.9581\n",
      "Epoch 841/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1934 - accuracy: 0.9280\n",
      "Epoch 841: val_loss did not improve from 0.09970\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1930 - accuracy: 0.9281 - val_loss: 0.1085 - val_accuracy: 0.9585\n",
      "Epoch 842/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1705 - accuracy: 0.9340\n",
      "Epoch 842: val_loss did not improve from 0.09970\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1702 - accuracy: 0.9341 - val_loss: 0.1003 - val_accuracy: 0.9632\n",
      "Epoch 843/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1911 - accuracy: 0.9270\n",
      "Epoch 843: val_loss did not improve from 0.09970\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1910 - accuracy: 0.9270 - val_loss: 0.1102 - val_accuracy: 0.9582\n",
      "Epoch 844/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1811 - accuracy: 0.9304\n",
      "Epoch 844: val_loss did not improve from 0.09970\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1809 - accuracy: 0.9305 - val_loss: 0.1035 - val_accuracy: 0.9608\n",
      "Epoch 845/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1961 - accuracy: 0.9271\n",
      "Epoch 845: val_loss did not improve from 0.09970\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1963 - accuracy: 0.9269 - val_loss: 0.1269 - val_accuracy: 0.9529\n",
      "Epoch 846/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1983 - accuracy: 0.9264\n",
      "Epoch 846: val_loss did not improve from 0.09970\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1983 - accuracy: 0.9263 - val_loss: 0.1236 - val_accuracy: 0.9548\n",
      "Epoch 847/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1901 - accuracy: 0.9303\n",
      "Epoch 847: val_loss did not improve from 0.09970\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1900 - accuracy: 0.9303 - val_loss: 0.1112 - val_accuracy: 0.9564\n",
      "Epoch 848/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2254 - accuracy: 0.9184\n",
      "Epoch 848: val_loss did not improve from 0.09970\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2255 - accuracy: 0.9184 - val_loss: 0.1936 - val_accuracy: 0.9393\n",
      "Epoch 849/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2016 - accuracy: 0.9262\n",
      "Epoch 849: val_loss did not improve from 0.09970\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.2014 - accuracy: 0.9262 - val_loss: 0.1192 - val_accuracy: 0.9571\n",
      "Epoch 850/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1913 - accuracy: 0.9279\n",
      "Epoch 850: val_loss did not improve from 0.09970\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1913 - accuracy: 0.9279 - val_loss: 0.1279 - val_accuracy: 0.9559\n",
      "Epoch 851/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2241 - accuracy: 0.9199\n",
      "Epoch 851: val_loss did not improve from 0.09970\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2241 - accuracy: 0.9200 - val_loss: 0.1163 - val_accuracy: 0.9567\n",
      "Epoch 852/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2415 - accuracy: 0.9113\n",
      "Epoch 852: val_loss did not improve from 0.09970\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2411 - accuracy: 0.9114 - val_loss: 0.1481 - val_accuracy: 0.9479\n",
      "Epoch 853/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2240 - accuracy: 0.9183\n",
      "Epoch 853: val_loss did not improve from 0.09970\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2244 - accuracy: 0.9182 - val_loss: 0.1443 - val_accuracy: 0.9461\n",
      "Epoch 854/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2075 - accuracy: 0.9238\n",
      "Epoch 854: val_loss did not improve from 0.09970\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2077 - accuracy: 0.9237 - val_loss: 0.1182 - val_accuracy: 0.9559\n",
      "Epoch 855/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2157 - accuracy: 0.9204\n",
      "Epoch 855: val_loss did not improve from 0.09970\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2157 - accuracy: 0.9204 - val_loss: 0.1116 - val_accuracy: 0.9577\n",
      "Epoch 856/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1859 - accuracy: 0.9308\n",
      "Epoch 856: val_loss did not improve from 0.09970\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1861 - accuracy: 0.9307 - val_loss: 0.1068 - val_accuracy: 0.9610\n",
      "Epoch 857/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2060 - accuracy: 0.9234\n",
      "Epoch 857: val_loss did not improve from 0.09970\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2060 - accuracy: 0.9233 - val_loss: 0.1169 - val_accuracy: 0.9561\n",
      "Epoch 858/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2042 - accuracy: 0.9215\n",
      "Epoch 858: val_loss did not improve from 0.09970\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2041 - accuracy: 0.9216 - val_loss: 0.1137 - val_accuracy: 0.9545\n",
      "Epoch 859/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1874 - accuracy: 0.9313\n",
      "Epoch 859: val_loss did not improve from 0.09970\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.1878 - accuracy: 0.9312 - val_loss: 0.1002 - val_accuracy: 0.9618\n",
      "Epoch 860/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1962 - accuracy: 0.9270\n",
      "Epoch 860: val_loss did not improve from 0.09970\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1961 - accuracy: 0.9270 - val_loss: 0.1117 - val_accuracy: 0.9583\n",
      "Epoch 861/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2352 - accuracy: 0.9145\n",
      "Epoch 861: val_loss did not improve from 0.09970\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2351 - accuracy: 0.9144 - val_loss: 0.1172 - val_accuracy: 0.9558\n",
      "Epoch 862/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2029 - accuracy: 0.9239\n",
      "Epoch 862: val_loss did not improve from 0.09970\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2028 - accuracy: 0.9239 - val_loss: 0.1130 - val_accuracy: 0.9561\n",
      "Epoch 863/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1893 - accuracy: 0.9278\n",
      "Epoch 863: val_loss did not improve from 0.09970\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1893 - accuracy: 0.9278 - val_loss: 0.1167 - val_accuracy: 0.9566\n",
      "Epoch 864/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1825 - accuracy: 0.9318\n",
      "Epoch 864: val_loss did not improve from 0.09970\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1827 - accuracy: 0.9317 - val_loss: 0.1003 - val_accuracy: 0.9619\n",
      "Epoch 865/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1793 - accuracy: 0.9321\n",
      "Epoch 865: val_loss did not improve from 0.09970\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1795 - accuracy: 0.9321 - val_loss: 0.1041 - val_accuracy: 0.9619\n",
      "Epoch 866/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1720 - accuracy: 0.9346\n",
      "Epoch 866: val_loss improved from 0.09970 to 0.09544, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 19ms/step - loss: 0.1719 - accuracy: 0.9347 - val_loss: 0.0954 - val_accuracy: 0.9637\n",
      "Epoch 867/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1842 - accuracy: 0.9304\n",
      "Epoch 867: val_loss did not improve from 0.09544\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1843 - accuracy: 0.9303 - val_loss: 0.0958 - val_accuracy: 0.9635\n",
      "Epoch 868/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2382 - accuracy: 0.9123\n",
      "Epoch 868: val_loss did not improve from 0.09544\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2384 - accuracy: 0.9122 - val_loss: 0.1699 - val_accuracy: 0.9410\n",
      "Epoch 869/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2592 - accuracy: 0.9081\n",
      "Epoch 869: val_loss did not improve from 0.09544\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2589 - accuracy: 0.9082 - val_loss: 0.1123 - val_accuracy: 0.9603\n",
      "Epoch 870/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2052 - accuracy: 0.9246\n",
      "Epoch 870: val_loss did not improve from 0.09544\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2050 - accuracy: 0.9247 - val_loss: 0.1288 - val_accuracy: 0.9529\n",
      "Epoch 871/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1961 - accuracy: 0.9272\n",
      "Epoch 871: val_loss did not improve from 0.09544\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1960 - accuracy: 0.9272 - val_loss: 0.1157 - val_accuracy: 0.9562\n",
      "Epoch 872/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2008 - accuracy: 0.9270\n",
      "Epoch 872: val_loss did not improve from 0.09544\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2045 - accuracy: 0.9266 - val_loss: 0.2265 - val_accuracy: 0.9232\n",
      "Epoch 873/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2138 - accuracy: 0.9222\n",
      "Epoch 873: val_loss did not improve from 0.09544\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2139 - accuracy: 0.9221 - val_loss: 0.1058 - val_accuracy: 0.9625\n",
      "Epoch 874/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1735 - accuracy: 0.9354\n",
      "Epoch 874: val_loss did not improve from 0.09544\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1738 - accuracy: 0.9354 - val_loss: 0.1001 - val_accuracy: 0.9618\n",
      "Epoch 875/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1870 - accuracy: 0.9301\n",
      "Epoch 875: val_loss did not improve from 0.09544\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1869 - accuracy: 0.9302 - val_loss: 0.1012 - val_accuracy: 0.9621\n",
      "Epoch 876/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1908 - accuracy: 0.9296\n",
      "Epoch 876: val_loss did not improve from 0.09544\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1907 - accuracy: 0.9297 - val_loss: 0.1188 - val_accuracy: 0.9558\n",
      "Epoch 877/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1994 - accuracy: 0.9254\n",
      "Epoch 877: val_loss did not improve from 0.09544\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1994 - accuracy: 0.9254 - val_loss: 0.1226 - val_accuracy: 0.9571\n",
      "Epoch 878/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1803 - accuracy: 0.9315\n",
      "Epoch 878: val_loss did not improve from 0.09544\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1802 - accuracy: 0.9315 - val_loss: 0.1139 - val_accuracy: 0.9571\n",
      "Epoch 879/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1993 - accuracy: 0.9250\n",
      "Epoch 879: val_loss did not improve from 0.09544\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1992 - accuracy: 0.9250 - val_loss: 0.1025 - val_accuracy: 0.9621\n",
      "Epoch 880/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2059 - accuracy: 0.9258\n",
      "Epoch 880: val_loss did not improve from 0.09544\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2058 - accuracy: 0.9258 - val_loss: 0.1316 - val_accuracy: 0.9525\n",
      "Epoch 881/1000\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.2249 - accuracy: 0.9175\n",
      "Epoch 881: val_loss did not improve from 0.09544\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2247 - accuracy: 0.9171 - val_loss: 0.1131 - val_accuracy: 0.9599\n",
      "Epoch 882/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1951 - accuracy: 0.9265\n",
      "Epoch 882: val_loss did not improve from 0.09544\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1951 - accuracy: 0.9265 - val_loss: 0.1277 - val_accuracy: 0.9556\n",
      "Epoch 883/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1933 - accuracy: 0.9285\n",
      "Epoch 883: val_loss did not improve from 0.09544\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1934 - accuracy: 0.9285 - val_loss: 0.1066 - val_accuracy: 0.9608\n",
      "Epoch 884/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1788 - accuracy: 0.9337\n",
      "Epoch 884: val_loss improved from 0.09544 to 0.09454, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.1789 - accuracy: 0.9336 - val_loss: 0.0945 - val_accuracy: 0.9653\n",
      "Epoch 885/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1905 - accuracy: 0.9290\n",
      "Epoch 885: val_loss did not improve from 0.09454\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1904 - accuracy: 0.9290 - val_loss: 0.1179 - val_accuracy: 0.9559\n",
      "Epoch 886/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1764 - accuracy: 0.9326\n",
      "Epoch 886: val_loss did not improve from 0.09454\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1763 - accuracy: 0.9326 - val_loss: 0.1051 - val_accuracy: 0.9601\n",
      "Epoch 887/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2025 - accuracy: 0.9255\n",
      "Epoch 887: val_loss did not improve from 0.09454\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2028 - accuracy: 0.9254 - val_loss: 0.0999 - val_accuracy: 0.9628\n",
      "Epoch 888/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2168 - accuracy: 0.9203\n",
      "Epoch 888: val_loss did not improve from 0.09454\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2171 - accuracy: 0.9201 - val_loss: 0.1045 - val_accuracy: 0.9642\n",
      "Epoch 889/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2276 - accuracy: 0.9201\n",
      "Epoch 889: val_loss did not improve from 0.09454\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2273 - accuracy: 0.9202 - val_loss: 0.1142 - val_accuracy: 0.9581\n",
      "Epoch 890/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2141 - accuracy: 0.9238\n",
      "Epoch 890: val_loss did not improve from 0.09454\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2141 - accuracy: 0.9238 - val_loss: 0.1192 - val_accuracy: 0.9554\n",
      "Epoch 891/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1954 - accuracy: 0.9253\n",
      "Epoch 891: val_loss did not improve from 0.09454\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1953 - accuracy: 0.9253 - val_loss: 0.1060 - val_accuracy: 0.9598\n",
      "Epoch 892/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1798 - accuracy: 0.9311\n",
      "Epoch 892: val_loss did not improve from 0.09454\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1798 - accuracy: 0.9311 - val_loss: 0.1077 - val_accuracy: 0.9608\n",
      "Epoch 893/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1933 - accuracy: 0.9265\n",
      "Epoch 893: val_loss did not improve from 0.09454\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1932 - accuracy: 0.9266 - val_loss: 0.1057 - val_accuracy: 0.9620\n",
      "Epoch 894/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1825 - accuracy: 0.9300\n",
      "Epoch 894: val_loss did not improve from 0.09454\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1822 - accuracy: 0.9301 - val_loss: 0.1079 - val_accuracy: 0.9598\n",
      "Epoch 895/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1884 - accuracy: 0.9298\n",
      "Epoch 895: val_loss did not improve from 0.09454\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1882 - accuracy: 0.9299 - val_loss: 0.0980 - val_accuracy: 0.9639\n",
      "Epoch 896/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1750 - accuracy: 0.9330\n",
      "Epoch 896: val_loss did not improve from 0.09454\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1751 - accuracy: 0.9330 - val_loss: 0.0981 - val_accuracy: 0.9618\n",
      "Epoch 897/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2722 - accuracy: 0.9066\n",
      "Epoch 897: val_loss did not improve from 0.09454\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2721 - accuracy: 0.9067 - val_loss: 0.1851 - val_accuracy: 0.9435\n",
      "Epoch 898/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2341 - accuracy: 0.9133\n",
      "Epoch 898: val_loss did not improve from 0.09454\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2341 - accuracy: 0.9132 - val_loss: 0.1181 - val_accuracy: 0.9582\n",
      "Epoch 899/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1871 - accuracy: 0.9303\n",
      "Epoch 899: val_loss did not improve from 0.09454\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1870 - accuracy: 0.9303 - val_loss: 0.1063 - val_accuracy: 0.9633\n",
      "Epoch 900/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1875 - accuracy: 0.9283\n",
      "Epoch 900: val_loss did not improve from 0.09454\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1874 - accuracy: 0.9284 - val_loss: 0.1098 - val_accuracy: 0.9588\n",
      "Epoch 901/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2052 - accuracy: 0.9241\n",
      "Epoch 901: val_loss did not improve from 0.09454\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2053 - accuracy: 0.9241 - val_loss: 0.1317 - val_accuracy: 0.9564\n",
      "Epoch 902/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2348 - accuracy: 0.9172\n",
      "Epoch 902: val_loss did not improve from 0.09454\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2348 - accuracy: 0.9171 - val_loss: 0.1247 - val_accuracy: 0.9547\n",
      "Epoch 903/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2364 - accuracy: 0.9142\n",
      "Epoch 903: val_loss did not improve from 0.09454\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2362 - accuracy: 0.9143 - val_loss: 0.0989 - val_accuracy: 0.9630\n",
      "Epoch 904/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2488 - accuracy: 0.9106\n",
      "Epoch 904: val_loss did not improve from 0.09454\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2488 - accuracy: 0.9105 - val_loss: 0.2143 - val_accuracy: 0.9357\n",
      "Epoch 905/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2235 - accuracy: 0.9185\n",
      "Epoch 905: val_loss did not improve from 0.09454\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2237 - accuracy: 0.9184 - val_loss: 0.1028 - val_accuracy: 0.9609\n",
      "Epoch 906/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1956 - accuracy: 0.9274\n",
      "Epoch 906: val_loss did not improve from 0.09454\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1955 - accuracy: 0.9274 - val_loss: 0.1070 - val_accuracy: 0.9620\n",
      "Epoch 907/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2120 - accuracy: 0.9237\n",
      "Epoch 907: val_loss did not improve from 0.09454\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2118 - accuracy: 0.9237 - val_loss: 0.1067 - val_accuracy: 0.9621\n",
      "Epoch 908/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1888 - accuracy: 0.9294\n",
      "Epoch 908: val_loss did not improve from 0.09454\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.1886 - accuracy: 0.9295 - val_loss: 0.1047 - val_accuracy: 0.9621\n",
      "Epoch 909/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1884 - accuracy: 0.9312\n",
      "Epoch 909: val_loss did not improve from 0.09454\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1884 - accuracy: 0.9312 - val_loss: 0.1055 - val_accuracy: 0.9600\n",
      "Epoch 910/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1864 - accuracy: 0.9300\n",
      "Epoch 910: val_loss did not improve from 0.09454\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1871 - accuracy: 0.9299 - val_loss: 0.1422 - val_accuracy: 0.9535\n",
      "Epoch 911/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1912 - accuracy: 0.9284\n",
      "Epoch 911: val_loss did not improve from 0.09454\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1911 - accuracy: 0.9285 - val_loss: 0.0983 - val_accuracy: 0.9632\n",
      "Epoch 912/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1670 - accuracy: 0.9361\n",
      "Epoch 912: val_loss did not improve from 0.09454\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1676 - accuracy: 0.9359 - val_loss: 0.1019 - val_accuracy: 0.9625\n",
      "Epoch 913/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2052 - accuracy: 0.9234\n",
      "Epoch 913: val_loss did not improve from 0.09454\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2054 - accuracy: 0.9233 - val_loss: 0.1140 - val_accuracy: 0.9617\n",
      "Epoch 914/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1840 - accuracy: 0.9312\n",
      "Epoch 914: val_loss did not improve from 0.09454\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1845 - accuracy: 0.9311 - val_loss: 0.1039 - val_accuracy: 0.9625\n",
      "Epoch 915/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1851 - accuracy: 0.9303\n",
      "Epoch 915: val_loss did not improve from 0.09454\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1849 - accuracy: 0.9303 - val_loss: 0.1051 - val_accuracy: 0.9618\n",
      "Epoch 916/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1854 - accuracy: 0.9299\n",
      "Epoch 916: val_loss did not improve from 0.09454\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1852 - accuracy: 0.9300 - val_loss: 0.1077 - val_accuracy: 0.9625\n",
      "Epoch 917/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1905 - accuracy: 0.9297\n",
      "Epoch 917: val_loss did not improve from 0.09454\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1907 - accuracy: 0.9297 - val_loss: 0.2599 - val_accuracy: 0.9167\n",
      "Epoch 918/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2185 - accuracy: 0.9216\n",
      "Epoch 918: val_loss did not improve from 0.09454\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2186 - accuracy: 0.9215 - val_loss: 0.1045 - val_accuracy: 0.9617\n",
      "Epoch 919/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1651 - accuracy: 0.9366\n",
      "Epoch 919: val_loss did not improve from 0.09454\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1653 - accuracy: 0.9365 - val_loss: 0.1014 - val_accuracy: 0.9632\n",
      "Epoch 920/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1759 - accuracy: 0.9345\n",
      "Epoch 920: val_loss did not improve from 0.09454\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1759 - accuracy: 0.9345 - val_loss: 0.0956 - val_accuracy: 0.9642\n",
      "Epoch 921/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1746 - accuracy: 0.9337\n",
      "Epoch 921: val_loss did not improve from 0.09454\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1745 - accuracy: 0.9337 - val_loss: 0.1005 - val_accuracy: 0.9627\n",
      "Epoch 922/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1626 - accuracy: 0.9380\n",
      "Epoch 922: val_loss did not improve from 0.09454\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1625 - accuracy: 0.9381 - val_loss: 0.0961 - val_accuracy: 0.9653\n",
      "Epoch 923/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1693 - accuracy: 0.9353\n",
      "Epoch 923: val_loss did not improve from 0.09454\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1699 - accuracy: 0.9352 - val_loss: 0.1059 - val_accuracy: 0.9608\n",
      "Epoch 924/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3095 - accuracy: 0.9005\n",
      "Epoch 924: val_loss did not improve from 0.09454\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.3094 - accuracy: 0.9005 - val_loss: 0.1000 - val_accuracy: 0.9636\n",
      "Epoch 925/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2659 - accuracy: 0.9054\n",
      "Epoch 925: val_loss did not improve from 0.09454\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.2658 - accuracy: 0.9054 - val_loss: 0.1002 - val_accuracy: 0.9621\n",
      "Epoch 926/1000\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.1850 - accuracy: 0.9311\n",
      "Epoch 926: val_loss did not improve from 0.09454\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.1850 - accuracy: 0.9312 - val_loss: 0.1095 - val_accuracy: 0.9580\n",
      "Epoch 927/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1851 - accuracy: 0.9316\n",
      "Epoch 927: val_loss did not improve from 0.09454\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.1851 - accuracy: 0.9316 - val_loss: 0.0992 - val_accuracy: 0.9634\n",
      "Epoch 928/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2554 - accuracy: 0.9116\n",
      "Epoch 928: val_loss did not improve from 0.09454\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2553 - accuracy: 0.9116 - val_loss: 0.1155 - val_accuracy: 0.9601\n",
      "Epoch 929/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2179 - accuracy: 0.9214\n",
      "Epoch 929: val_loss did not improve from 0.09454\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2181 - accuracy: 0.9214 - val_loss: 0.1153 - val_accuracy: 0.9586\n",
      "Epoch 930/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1976 - accuracy: 0.9283\n",
      "Epoch 930: val_loss did not improve from 0.09454\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1975 - accuracy: 0.9283 - val_loss: 0.0954 - val_accuracy: 0.9632\n",
      "Epoch 931/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1970 - accuracy: 0.9260\n",
      "Epoch 931: val_loss did not improve from 0.09454\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1967 - accuracy: 0.9261 - val_loss: 0.1027 - val_accuracy: 0.9637\n",
      "Epoch 932/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1936 - accuracy: 0.9277\n",
      "Epoch 932: val_loss did not improve from 0.09454\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1934 - accuracy: 0.9278 - val_loss: 0.1383 - val_accuracy: 0.9522\n",
      "Epoch 933/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2103 - accuracy: 0.9226\n",
      "Epoch 933: val_loss did not improve from 0.09454\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2101 - accuracy: 0.9227 - val_loss: 0.1058 - val_accuracy: 0.9601\n",
      "Epoch 934/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1837 - accuracy: 0.9307\n",
      "Epoch 934: val_loss did not improve from 0.09454\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1836 - accuracy: 0.9307 - val_loss: 0.0986 - val_accuracy: 0.9632\n",
      "Epoch 935/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1723 - accuracy: 0.9345\n",
      "Epoch 935: val_loss improved from 0.09454 to 0.09237, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.1723 - accuracy: 0.9345 - val_loss: 0.0924 - val_accuracy: 0.9646\n",
      "Epoch 936/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1816 - accuracy: 0.9320\n",
      "Epoch 936: val_loss did not improve from 0.09237\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1814 - accuracy: 0.9320 - val_loss: 0.1070 - val_accuracy: 0.9594\n",
      "Epoch 937/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1737 - accuracy: 0.9350\n",
      "Epoch 937: val_loss did not improve from 0.09237\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1735 - accuracy: 0.9351 - val_loss: 0.1001 - val_accuracy: 0.9626\n",
      "Epoch 938/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1724 - accuracy: 0.9356\n",
      "Epoch 938: val_loss did not improve from 0.09237\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1726 - accuracy: 0.9356 - val_loss: 0.1090 - val_accuracy: 0.9620\n",
      "Epoch 939/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2100 - accuracy: 0.9236\n",
      "Epoch 939: val_loss did not improve from 0.09237\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2098 - accuracy: 0.9236 - val_loss: 0.1159 - val_accuracy: 0.9548\n",
      "Epoch 940/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1814 - accuracy: 0.9322\n",
      "Epoch 940: val_loss did not improve from 0.09237\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1813 - accuracy: 0.9322 - val_loss: 0.0987 - val_accuracy: 0.9642\n",
      "Epoch 941/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1668 - accuracy: 0.9376\n",
      "Epoch 941: val_loss improved from 0.09237 to 0.09046, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.1667 - accuracy: 0.9376 - val_loss: 0.0905 - val_accuracy: 0.9670\n",
      "Epoch 942/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1640 - accuracy: 0.9385\n",
      "Epoch 942: val_loss did not improve from 0.09046\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1640 - accuracy: 0.9385 - val_loss: 0.0923 - val_accuracy: 0.9651\n",
      "Epoch 943/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1689 - accuracy: 0.9366\n",
      "Epoch 943: val_loss did not improve from 0.09046\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1688 - accuracy: 0.9367 - val_loss: 0.0931 - val_accuracy: 0.9647\n",
      "Epoch 944/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1783 - accuracy: 0.9331\n",
      "Epoch 944: val_loss did not improve from 0.09046\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1783 - accuracy: 0.9331 - val_loss: 0.0982 - val_accuracy: 0.9636\n",
      "Epoch 945/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1623 - accuracy: 0.9379\n",
      "Epoch 945: val_loss improved from 0.09046 to 0.08781, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.1623 - accuracy: 0.9379 - val_loss: 0.0878 - val_accuracy: 0.9679\n",
      "Epoch 946/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1757 - accuracy: 0.9347\n",
      "Epoch 946: val_loss did not improve from 0.08781\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.1755 - accuracy: 0.9347 - val_loss: 0.0908 - val_accuracy: 0.9675\n",
      "Epoch 947/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1752 - accuracy: 0.9347\n",
      "Epoch 947: val_loss did not improve from 0.08781\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1753 - accuracy: 0.9347 - val_loss: 0.0996 - val_accuracy: 0.9644\n",
      "Epoch 948/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2047 - accuracy: 0.9248\n",
      "Epoch 948: val_loss did not improve from 0.08781\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2045 - accuracy: 0.9249 - val_loss: 0.1035 - val_accuracy: 0.9629\n",
      "Epoch 949/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1720 - accuracy: 0.9353\n",
      "Epoch 949: val_loss did not improve from 0.08781\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1720 - accuracy: 0.9353 - val_loss: 0.0944 - val_accuracy: 0.9641\n",
      "Epoch 950/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1703 - accuracy: 0.9364\n",
      "Epoch 950: val_loss did not improve from 0.08781\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.1703 - accuracy: 0.9364 - val_loss: 0.0911 - val_accuracy: 0.9651\n",
      "Epoch 951/1000\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.1676 - accuracy: 0.9383\n",
      "Epoch 951: val_loss did not improve from 0.08781\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.1670 - accuracy: 0.9385 - val_loss: 0.0914 - val_accuracy: 0.9655\n",
      "Epoch 952/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3645 - accuracy: 0.8764\n",
      "Epoch 952: val_loss did not improve from 0.08781\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3641 - accuracy: 0.8765 - val_loss: 0.1027 - val_accuracy: 0.9633\n",
      "Epoch 953/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1810 - accuracy: 0.9317\n",
      "Epoch 953: val_loss did not improve from 0.08781\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1811 - accuracy: 0.9317 - val_loss: 0.0989 - val_accuracy: 0.9624\n",
      "Epoch 954/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1698 - accuracy: 0.9357\n",
      "Epoch 954: val_loss did not improve from 0.08781\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1698 - accuracy: 0.9358 - val_loss: 0.1083 - val_accuracy: 0.9590\n",
      "Epoch 955/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1986 - accuracy: 0.9289\n",
      "Epoch 955: val_loss did not improve from 0.08781\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1984 - accuracy: 0.9289 - val_loss: 0.0962 - val_accuracy: 0.9647\n",
      "Epoch 956/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2182 - accuracy: 0.9214\n",
      "Epoch 956: val_loss did not improve from 0.08781\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2182 - accuracy: 0.9213 - val_loss: 0.1187 - val_accuracy: 0.9563\n",
      "Epoch 957/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2414 - accuracy: 0.9147\n",
      "Epoch 957: val_loss did not improve from 0.08781\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.2410 - accuracy: 0.9148 - val_loss: 0.1181 - val_accuracy: 0.9583\n",
      "Epoch 958/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1942 - accuracy: 0.9304\n",
      "Epoch 958: val_loss did not improve from 0.08781\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1942 - accuracy: 0.9304 - val_loss: 0.1017 - val_accuracy: 0.9613\n",
      "Epoch 959/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1910 - accuracy: 0.9290\n",
      "Epoch 959: val_loss did not improve from 0.08781\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1910 - accuracy: 0.9290 - val_loss: 0.0981 - val_accuracy: 0.9630\n",
      "Epoch 960/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1897 - accuracy: 0.9308\n",
      "Epoch 960: val_loss did not improve from 0.08781\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1896 - accuracy: 0.9308 - val_loss: 0.0979 - val_accuracy: 0.9627\n",
      "Epoch 961/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1660 - accuracy: 0.9364\n",
      "Epoch 961: val_loss did not improve from 0.08781\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1659 - accuracy: 0.9364 - val_loss: 0.0958 - val_accuracy: 0.9637\n",
      "Epoch 962/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1682 - accuracy: 0.9375\n",
      "Epoch 962: val_loss did not improve from 0.08781\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1679 - accuracy: 0.9376 - val_loss: 0.1024 - val_accuracy: 0.9636\n",
      "Epoch 963/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2015 - accuracy: 0.9269\n",
      "Epoch 963: val_loss did not improve from 0.08781\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2014 - accuracy: 0.9269 - val_loss: 0.0912 - val_accuracy: 0.9651\n",
      "Epoch 964/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2033 - accuracy: 0.9247\n",
      "Epoch 964: val_loss did not improve from 0.08781\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2034 - accuracy: 0.9247 - val_loss: 0.1245 - val_accuracy: 0.9548\n",
      "Epoch 965/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2290 - accuracy: 0.9179\n",
      "Epoch 965: val_loss did not improve from 0.08781\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2288 - accuracy: 0.9180 - val_loss: 0.1314 - val_accuracy: 0.9555\n",
      "Epoch 966/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1921 - accuracy: 0.9298\n",
      "Epoch 966: val_loss did not improve from 0.08781\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1921 - accuracy: 0.9298 - val_loss: 0.0968 - val_accuracy: 0.9641\n",
      "Epoch 967/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1716 - accuracy: 0.9348\n",
      "Epoch 967: val_loss did not improve from 0.08781\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1717 - accuracy: 0.9348 - val_loss: 0.1002 - val_accuracy: 0.9649\n",
      "Epoch 968/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1819 - accuracy: 0.9322\n",
      "Epoch 968: val_loss did not improve from 0.08781\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1821 - accuracy: 0.9320 - val_loss: 0.0952 - val_accuracy: 0.9642\n",
      "Epoch 969/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1962 - accuracy: 0.9290\n",
      "Epoch 969: val_loss did not improve from 0.08781\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1964 - accuracy: 0.9290 - val_loss: 0.1143 - val_accuracy: 0.9576\n",
      "Epoch 970/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1746 - accuracy: 0.9356\n",
      "Epoch 970: val_loss did not improve from 0.08781\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1745 - accuracy: 0.9356 - val_loss: 0.0957 - val_accuracy: 0.9647\n",
      "Epoch 971/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1899 - accuracy: 0.9297\n",
      "Epoch 971: val_loss did not improve from 0.08781\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1897 - accuracy: 0.9298 - val_loss: 0.0921 - val_accuracy: 0.9670\n",
      "Epoch 972/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1727 - accuracy: 0.9343\n",
      "Epoch 972: val_loss did not improve from 0.08781\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1727 - accuracy: 0.9343 - val_loss: 0.0934 - val_accuracy: 0.9657\n",
      "Epoch 973/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1870 - accuracy: 0.9314\n",
      "Epoch 973: val_loss did not improve from 0.08781\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1869 - accuracy: 0.9315 - val_loss: 0.1028 - val_accuracy: 0.9615\n",
      "Epoch 974/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1684 - accuracy: 0.9369\n",
      "Epoch 974: val_loss did not improve from 0.08781\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.1682 - accuracy: 0.9370 - val_loss: 0.0963 - val_accuracy: 0.9637\n",
      "Epoch 975/1000\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.1760 - accuracy: 0.9333\n",
      "Epoch 975: val_loss did not improve from 0.08781\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.1765 - accuracy: 0.9332 - val_loss: 0.0943 - val_accuracy: 0.9653\n",
      "Epoch 976/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1649 - accuracy: 0.9378\n",
      "Epoch 976: val_loss did not improve from 0.08781\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1648 - accuracy: 0.9378 - val_loss: 0.0907 - val_accuracy: 0.9655\n",
      "Epoch 977/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1706 - accuracy: 0.9362\n",
      "Epoch 977: val_loss did not improve from 0.08781\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1707 - accuracy: 0.9362 - val_loss: 0.1058 - val_accuracy: 0.9589\n",
      "Epoch 978/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1834 - accuracy: 0.9312\n",
      "Epoch 978: val_loss did not improve from 0.08781\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1834 - accuracy: 0.9312 - val_loss: 0.1005 - val_accuracy: 0.9636\n",
      "Epoch 979/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2283 - accuracy: 0.9194\n",
      "Epoch 979: val_loss did not improve from 0.08781\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2281 - accuracy: 0.9195 - val_loss: 0.0966 - val_accuracy: 0.9631\n",
      "Epoch 980/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1916 - accuracy: 0.9303\n",
      "Epoch 980: val_loss did not improve from 0.08781\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1914 - accuracy: 0.9303 - val_loss: 0.1485 - val_accuracy: 0.9478\n",
      "Epoch 981/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1887 - accuracy: 0.9299\n",
      "Epoch 981: val_loss did not improve from 0.08781\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1885 - accuracy: 0.9300 - val_loss: 0.1012 - val_accuracy: 0.9617\n",
      "Epoch 982/1000\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.2139 - accuracy: 0.9228\n",
      "Epoch 982: val_loss did not improve from 0.08781\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2128 - accuracy: 0.9233 - val_loss: 0.1066 - val_accuracy: 0.9605\n",
      "Epoch 983/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1726 - accuracy: 0.9359\n",
      "Epoch 983: val_loss did not improve from 0.08781\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1725 - accuracy: 0.9359 - val_loss: 0.0985 - val_accuracy: 0.9642\n",
      "Epoch 984/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1803 - accuracy: 0.9324\n",
      "Epoch 984: val_loss did not improve from 0.08781\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1805 - accuracy: 0.9323 - val_loss: 0.0913 - val_accuracy: 0.9655\n",
      "Epoch 985/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1822 - accuracy: 0.9314\n",
      "Epoch 985: val_loss did not improve from 0.08781\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1828 - accuracy: 0.9311 - val_loss: 0.1048 - val_accuracy: 0.9621\n",
      "Epoch 986/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2155 - accuracy: 0.9218\n",
      "Epoch 986: val_loss did not improve from 0.08781\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2153 - accuracy: 0.9219 - val_loss: 0.1005 - val_accuracy: 0.9618\n",
      "Epoch 987/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2009 - accuracy: 0.9279\n",
      "Epoch 987: val_loss did not improve from 0.08781\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2014 - accuracy: 0.9277 - val_loss: 0.2127 - val_accuracy: 0.9368\n",
      "Epoch 988/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2284 - accuracy: 0.9171\n",
      "Epoch 988: val_loss did not improve from 0.08781\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2285 - accuracy: 0.9171 - val_loss: 0.1193 - val_accuracy: 0.9590\n",
      "Epoch 989/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1955 - accuracy: 0.9292\n",
      "Epoch 989: val_loss did not improve from 0.08781\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1956 - accuracy: 0.9291 - val_loss: 0.0959 - val_accuracy: 0.9647\n",
      "Epoch 990/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1788 - accuracy: 0.9348\n",
      "Epoch 990: val_loss did not improve from 0.08781\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1790 - accuracy: 0.9348 - val_loss: 0.1038 - val_accuracy: 0.9595\n",
      "Epoch 991/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1707 - accuracy: 0.9363\n",
      "Epoch 991: val_loss did not improve from 0.08781\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1705 - accuracy: 0.9363 - val_loss: 0.0908 - val_accuracy: 0.9650\n",
      "Epoch 992/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1675 - accuracy: 0.9360\n",
      "Epoch 992: val_loss improved from 0.08781 to 0.08382, saving model to /home/jupyter/trained_models_srikar/best_epoch_model_2024-04-01_03-02-23.h5\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.1674 - accuracy: 0.9360 - val_loss: 0.0838 - val_accuracy: 0.9692\n",
      "Epoch 993/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1860 - accuracy: 0.9333\n",
      "Epoch 993: val_loss did not improve from 0.08382\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1863 - accuracy: 0.9332 - val_loss: 0.1165 - val_accuracy: 0.9587\n",
      "Epoch 994/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1790 - accuracy: 0.9353\n",
      "Epoch 994: val_loss did not improve from 0.08382\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1789 - accuracy: 0.9353 - val_loss: 0.0900 - val_accuracy: 0.9657\n",
      "Epoch 995/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1559 - accuracy: 0.9397\n",
      "Epoch 995: val_loss did not improve from 0.08382\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1559 - accuracy: 0.9397 - val_loss: 0.0882 - val_accuracy: 0.9668\n",
      "Epoch 996/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2788 - accuracy: 0.9030\n",
      "Epoch 996: val_loss did not improve from 0.08382\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2790 - accuracy: 0.9029 - val_loss: 0.2508 - val_accuracy: 0.9231\n",
      "Epoch 997/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2688 - accuracy: 0.9105\n",
      "Epoch 997: val_loss did not improve from 0.08382\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2685 - accuracy: 0.9106 - val_loss: 0.1156 - val_accuracy: 0.9607\n",
      "Epoch 998/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1905 - accuracy: 0.9308\n",
      "Epoch 998: val_loss did not improve from 0.08382\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1905 - accuracy: 0.9307 - val_loss: 0.1041 - val_accuracy: 0.9628\n",
      "Epoch 999/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2110 - accuracy: 0.9250\n",
      "Epoch 999: val_loss did not improve from 0.08382\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.2109 - accuracy: 0.9250 - val_loss: 0.1175 - val_accuracy: 0.9561\n",
      "Epoch 1000/1000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1862 - accuracy: 0.9309\n",
      "Epoch 1000: val_loss did not improve from 0.08382\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.1865 - accuracy: 0.9308 - val_loss: 0.0976 - val_accuracy: 0.9652\n",
      "model saved locally\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, CSVLogger\n",
    "\n",
    "current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "model_path = \"/home/jupyter/trained_models_srikar/\"\n",
    "model_name = f'model_{current_time}.h5'\n",
    "\n",
    "# Define a callback to save the best model based on validation loss\n",
    "checkpoint = ModelCheckpoint(model_path + \"best_epoch_\" + model_name, \n",
    "                             monitor='val_loss', \n",
    "                             verbose=1, \n",
    "                             save_best_only=True, \n",
    "                             mode='min')\n",
    "\n",
    "# TensorBoard callback to log training metrics\n",
    "log_dir = \"/home/jupyter/logs/\" + current_time\n",
    "tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# CSVLogger callback to save training logs in CSV format\n",
    "csv_logger = CSVLogger(model_path + \"training_logs.csv\", separator=',', append=False)\n",
    "\n",
    "callbacks = [checkpoint, tensorboard, csv_logger]\n",
    "\n",
    "model_history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=1000,\n",
    "    batch_size=32,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "model.save(model_path + model_name)\n",
    "print(\"model saved locally\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1afe34f5-72ec-48e4-99ed-63f4304436f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-15 13:22:00.424686: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-15 13:22:09.641220: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-15 13:22:23.525319: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-05-15 13:22:23.525542: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-05-15 13:22:23.525557: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import mapping\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from datetime import datetime\n",
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import sys\n",
    "\n",
    "geojson_datapath = '/home/jupyter/label_folder/newextent_1123.geojson'\n",
    "\n",
    "\n",
    "def clip_tiff(tiff, geojson = geojson_datapath):\n",
    "\n",
    "    with open(geojson) as clip_geojson:\n",
    "        clip_geojson = gpd.read_file(clip_geojson)\n",
    "        clip_geometry = clip_geojson.geometry.values[0]\n",
    "        clip_geojson = mapping(clip_geometry)\n",
    "\n",
    "    with rasterio.open(tiff) as src:\n",
    "        # Perform the clip\n",
    "        clip_image, clip_transform = mask(src, [clip_geojson], crop=True)\n",
    "\n",
    "    return clip_image\n",
    "\n",
    "def tensorify_image(image):\n",
    "    ## resizing and process input funciton condensed into one.\n",
    "    tensor_image = tf.convert_to_tensor(image)\n",
    "    tensor_image = tf.transpose(tensor_image, perm=[1, 2, 0])\n",
    "    return tensor_image\n",
    "\n",
    "\n",
    "def bandwise_normalize(input_tensor, epsilon=1e-8):\n",
    "    # Convert the input_tensor to a float32 type\n",
    "    input_tensor = tf.cast(input_tensor, tf.float32)\n",
    "\n",
    "    # Calculate the minimum and maximum values along the channel axis\n",
    "    min_val = tf.reduce_min(input_tensor, axis=2, keepdims=True)\n",
    "    max_val = tf.reduce_max(input_tensor, axis=2, keepdims=True)\n",
    "\n",
    "    # Check for potential numerical instability\n",
    "    denom = max_val - min_val\n",
    "    denom = tf.where(tf.abs(denom) < epsilon, epsilon, denom)\n",
    "\n",
    "    # Normalize the tensor band-wise to the range [0, 1]\n",
    "    normalized_tensor = (input_tensor - min_val) / denom\n",
    "\n",
    "    return normalized_tensor\n",
    "\n",
    "\n",
    "def pad_to_multiple(image, TILE_HT, TILE_WD):\n",
    "    # Get the current dimensions\n",
    "    height, width, channels = image.shape\n",
    "\n",
    "    # Calculate the target dimensions\n",
    "    target_height = tf.cast(tf.math.ceil(height / TILE_HT) * TILE_HT, tf.int32)\n",
    "    target_width = tf.cast(tf.math.ceil(width / TILE_WD) * TILE_WD, tf.int32)\n",
    "\n",
    "    # Calculate the amount of padding\n",
    "    pad_height = target_height - height\n",
    "    pad_width = target_width - width\n",
    "\n",
    "    # Pad the image\n",
    "    padded_image = tf.image.resize_with_crop_or_pad(image, target_height, target_width)\n",
    "\n",
    "    return padded_image\n",
    "\n",
    "\n",
    "def tile_image(fullimg, CHANNELS=1, TILE_HT=128, TILE_WD=128):\n",
    "    fullimg = pad_to_multiple(fullimg, TILE_HT, TILE_WD)\n",
    "    images = tf.expand_dims(fullimg, axis=0)\n",
    "    tiles = tf.image.extract_patches(\n",
    "        images=images,\n",
    "        sizes=[1, TILE_HT, TILE_WD, 1],\n",
    "        strides=[1, TILE_HT, TILE_WD, 1],\n",
    "        rates=[1, 1, 1, 1],\n",
    "        padding=\"VALID\",\n",
    "    )\n",
    "\n",
    "    tiles = tf.squeeze(tiles, axis=0)\n",
    "    nrows = tiles.shape[0]\n",
    "    ncols = tiles.shape[1]\n",
    "    all_tiles = tf.reshape(tiles, [nrows * ncols, TILE_HT, TILE_WD, CHANNELS])\n",
    "    ordered_tiles = tf.reshape(tiles, [nrows, ncols, TILE_HT, TILE_WD, CHANNELS])\n",
    "    return ordered_tiles, all_tiles, fullimg.shape\n",
    "\n",
    "\n",
    "\n",
    "def stitch_segmentation_patches(segmentation_patches, dims, PATCH_HEIGHT, PATCH_WIDTH):\n",
    "    height, width = dims[0], dims[1]\n",
    "    num_rows, num_cols = segmentation_patches.shape[:2]\n",
    "\n",
    "    # Convert TensorFlow tensor to NumPy array\n",
    "    segmentation_patches_np = segmentation_patches.numpy()\n",
    "\n",
    "    stitched_array = np.zeros((height, width), dtype=int)\n",
    "\n",
    "    # Reshape the segmentation_patches array\n",
    "    segmentation_patches_reshaped = segmentation_patches_np.reshape(\n",
    "        (num_rows, num_cols, PATCH_HEIGHT, PATCH_WIDTH)\n",
    "    )\n",
    "    print(\"segmentation_patches_reshaped.shape\", segmentation_patches_reshaped.shape)\n",
    "\n",
    "    # Calculate the indices for stitching\n",
    "    row_indices_patch = np.arange(0, height, PATCH_HEIGHT)\n",
    "    col_indices_patch = np.arange(0, width, PATCH_WIDTH)\n",
    "    print(\"row_indices_patch\", row_indices_patch.shape)\n",
    "    print(\"col_indices_patch\", col_indices_patch.shape)\n",
    "\n",
    "    # Use nested loops to stitch patches into the final array\n",
    "    for i in range(num_rows):\n",
    "        for j in range(num_cols):\n",
    "            row_start = row_indices_patch[i]\n",
    "            col_start = col_indices_patch[j]\n",
    "            row_end = row_start + PATCH_HEIGHT\n",
    "            col_end = col_start + PATCH_WIDTH\n",
    "\n",
    "            stitched_array[\n",
    "                row_start:row_end, col_start:col_end\n",
    "            ] = segmentation_patches_reshaped[i, j]\n",
    "\n",
    "    print(\"stitched_array\", stitched_array.shape)\n",
    "    return stitched_array\n",
    "\n",
    "\n",
    "def prediction(test_image_path, model_path):\n",
    "    IMAGE_CHANNELS = 8\n",
    "    model = keras.models.load_model(model_path)\n",
    "    input_shape = model.layers[0].input_shape\n",
    "\n",
    "    PATCH_HEIGHT = input_shape[0][-3]\n",
    "    PATCH_WIDTH = input_shape[0][-2]\n",
    "\n",
    "    image = clip_tiff(test_image_path)\n",
    "    new_image = tensorify_image(image)\n",
    "    normalized_image = bandwise_normalize(new_image)\n",
    "    display_patches, inference_patches, dims = tile_image(\n",
    "        normalized_image, IMAGE_CHANNELS, PATCH_HEIGHT, PATCH_WIDTH\n",
    "    )\n",
    "    print(\"dims\", dims)\n",
    "    start_time = time.time()\n",
    "    predictions = model.predict(inference_patches, batch_size=2048)\n",
    "    end_time_pred = time.time()\n",
    "\n",
    "    # Calculate the elapsed time\n",
    "    elapsed_time = end_time_pred - start_time\n",
    "\n",
    "    # Print the elapsed time\n",
    "    print(f\"Time taken for predictions: {elapsed_time} seconds\")\n",
    "\n",
    "    logits = predictions\n",
    "\n",
    "    # Set values of class 0 to a very large negative number\n",
    "    mask = tf.one_hot(\n",
    "        0, depth=21, on_value=float(\"-inf\"), off_value=0, dtype=tf.float32\n",
    "    )\n",
    "    logits_with_mask = logits + mask\n",
    "\n",
    "    # Perform argmax along the last axis (axis=-1)\n",
    "    argmax_result = tf.argmax(logits_with_mask, axis=-1)\n",
    "\n",
    "    tiles = display_patches\n",
    "    nrows = tiles.shape[0]\n",
    "    ncols = tiles.shape[1]\n",
    "    segmentation_patches = tf.reshape(\n",
    "        argmax_result, [nrows, ncols, PATCH_HEIGHT, PATCH_WIDTH]\n",
    "    )\n",
    "\n",
    "    stitched_array = stitch_segmentation_patches(\n",
    "        segmentation_patches, dims, PATCH_HEIGHT, PATCH_WIDTH\n",
    "    )\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate the elapsed time\n",
    "    elapsed_time_stitch = end_time - start_time\n",
    "\n",
    "    # Print the elapsed time\n",
    "    print(f\"Time taken including stitching: {elapsed_time_stitch} seconds\")\n",
    "\n",
    "    return argmax_result,stitched_array\n",
    "\n",
    "\n",
    "def resize_img(image,label):\n",
    "  image = tf.image.resize_with_crop_or_pad(image, label.shape[0], label.shape[1])\n",
    "  print(image.shape, label.shape)\n",
    "  return image, label\n",
    "\n",
    "\n",
    "def process_input(image, label):\n",
    "\n",
    "    tensor_image = tf.convert_to_tensor(image)\n",
    "    tensor_image = tf.expand_dims(tensor_image,-1)\n",
    "    #tensor_image = tf.transpose(tensor_image, perm=[1, 2, 0])\n",
    "    tensor_label = tf.convert_to_tensor(label)\n",
    "    tensor_label = tf.transpose(tensor_label, perm=[1, 2, 0])\n",
    "    #tensor_label = tf.expand_dims(tensor_label,-1)\n",
    "\n",
    "\n",
    "    if tensor_label.shape != tensor_image.shape:\n",
    "      tensor_image, tensor_label = resize_img(tensor_image, tensor_label)\n",
    "\n",
    "    tensor_image = tf.squeeze(tensor_image)\n",
    "    tensor_label = tf.squeeze(tensor_label)\n",
    "\n",
    "    print(tensor_image.shape)\n",
    "    print(tensor_label.shape)\n",
    "\n",
    "    return tensor_image.numpy().astype(int), tensor_label.numpy().astype(int)\n",
    "\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "  '''\n",
    "  Computes IOU and Dice Score.\n",
    "\n",
    "  Args:\n",
    "    y_true (tensor) - ground truth label map\n",
    "    y_pred (tensor) - predicted label map\n",
    "  '''\n",
    "\n",
    "  class_names = [\n",
    "    \"lake\",\n",
    "    \"settlement\",\n",
    "    \"shrub land\",\n",
    "    \"grass land\",\n",
    "    \"homogenous forest\",\n",
    "    \"agriculture1 (with vegetation)\",\n",
    "    \"agriculture2 (without vegetation)\",\n",
    "    \"open area\",\n",
    "    \"clove plantation\",\n",
    "    \"mixed forest1\",\n",
    "    \"mixed forest2\",\n",
    "    \"rice field1\",\n",
    "    \"rice field2\",\n",
    "    \"rice field3\",\n",
    "    \"mixed garden\",\n",
    "    \"grass land2\",\n",
    "    \"grass land3\",\n",
    "    \"mixed garden2\",\n",
    "    \"agroforestry\",\n",
    "    \"clouds\"]\n",
    "\n",
    "\n",
    "  class_wise_iou = []\n",
    "  class_wise_dice_score = []\n",
    "  class_wise_accuracy = []\n",
    "  class_wise_precision = []\n",
    "  class_wise_recall = []\n",
    "\n",
    "  smoothening_factor = 0.00001\n",
    "\n",
    "  print(np.unique(y_true)[1:])\n",
    "  print(np.unique(y_pred))\n",
    "\n",
    "  for i in np.unique(y_true)[1:]:\n",
    "\n",
    "    intersection = np.sum((y_pred == i) * (y_true == i))\n",
    "    y_true_area = np.sum((y_true == i))\n",
    "    y_pred_area = np.sum((y_pred == i))\n",
    "    combined_area = y_true_area + y_pred_area\n",
    "\n",
    "    iou = (intersection + smoothening_factor) / (combined_area - intersection + smoothening_factor)\n",
    "    class_wise_iou.append(iou)\n",
    "\n",
    "    dice_score =  2 * ((intersection + smoothening_factor) / (combined_area + smoothening_factor))\n",
    "    class_wise_dice_score.append(dice_score)\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = np.sum((y_pred == i) & (y_true == i)) / np.sum(y_true == i)\n",
    "    class_wise_accuracy.append(accuracy)\n",
    "\n",
    "    # Precision\n",
    "    precision = intersection / (y_pred_area + smoothening_factor)\n",
    "    class_wise_precision.append(precision)\n",
    "\n",
    "    # Recall\n",
    "    recall = intersection / (y_true_area + smoothening_factor)\n",
    "    class_wise_recall.append(recall)\n",
    "\n",
    "  # Mean IOU\n",
    "  mean_iou = np.mean(class_wise_iou)\n",
    "\n",
    "\n",
    "\n",
    "  return  class_wise_iou,class_wise_dice_score,class_wise_accuracy, class_wise_precision, class_wise_recall, mean_iou\n",
    "\n",
    "def label_image_processing(predicted_array,label_image_path):\n",
    "\n",
    "    ground_truth = clip_tiff(label_image_path)\n",
    "\n",
    "    resized_predicted_array, new_ground_truth = process_input(predicted_array, ground_truth)\n",
    "\n",
    "    new_boolean_mask = new_ground_truth != 0\n",
    "\n",
    "    new_predict = np.where(new_boolean_mask,resized_predicted_array,0)\n",
    "\n",
    "    return new_predict, new_ground_truth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9270f52f-1e3c-4f7e-96cc-0b2af52b3bc4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-15 13:23:19.855512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-05-15 13:23:21.512001: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-05-15 13:23:21.515319: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-05-15 13:23:21.519371: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-15 13:23:21.520958: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-05-15 13:23:21.523696: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-05-15 13:23:21.526339: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-05-15 13:23:35.808835: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-05-15 13:23:35.835454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-05-15 13:23:35.837171: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-05-15 13:23:35.838757: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13584 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dims (3696, 4560, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-15 13:23:59.661343: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8201\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/jupyter/trained_models_srikar/model_2024-05-15_05-39-21.h5\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      7\u001b[0m test_image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/jupyter/image_folder/area2_0530_2022_8bands.tif\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 9\u001b[0m a_r, predicted_array \u001b[38;5;241m=\u001b[39m \u001b[43mprediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_image_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 147\u001b[0m, in \u001b[0;36mprediction\u001b[0;34m(test_image_path, model_path)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdims\u001b[39m\u001b[38;5;124m\"\u001b[39m, dims)\n\u001b[1;32m    146\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 147\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43minference_patches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m end_time_pred \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# Calculate the elapsed time\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/engine/training.py:2253\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2251\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39msteps():\n\u001b[1;32m   2252\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_predict_batch_begin(step)\n\u001b[0;32m-> 2253\u001b[0m     tmp_batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2254\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   2255\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:986\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    982\u001b[0m   _, _, filtered_flat_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    983\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn\u001b[38;5;241m.\u001b[39m_function_spec\u001b[38;5;241m.\u001b[39mcanonicalize_function_inputs(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    984\u001b[0m           \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds))\n\u001b[1;32m    985\u001b[0m   \u001b[38;5;66;03m# If we did not create any variables the trace we have is good enough.\u001b[39;00m\n\u001b[0;32m--> 986\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_concrete_stateful_fn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    987\u001b[0m \u001b[43m      \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_concrete_stateful_fn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn_with_cond\u001b[39m(inner_args, inner_kwds, inner_filtered_flat_args):\n\u001b[1;32m    990\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Conditionally runs initialization if it's needed.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1865\u001b[0m     args,\n\u001b[1;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1867\u001b[0m     executing_eagerly)\n\u001b[1;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "label_image_path = '/home/jupyter/label_folder/continuous_label_raster.tif'\n",
    "\n",
    "geojson_datapath = '/home/jupyter/label_folder/newextent_1123.geojson'\n",
    "\n",
    "model_path = '/home/jupyter/trained_models_srikar/model_2024-05-15_05-39-21.h5'\n",
    "\n",
    "test_image_path = '/home/jupyter/image_folder/area2_0530_2022_8bands.tif'\n",
    "\n",
    "a_r, predicted_array = prediction(test_image_path, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d8bbe0e2-6b5d-497c-b4bc-d6cf37b9679a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Assuming argmax_result is a TensorFlow tensor\n",
    "argmax_result_np = a_r.numpy()  # Convert TensorFlow tensor to NumPy array\n",
    "\n",
    "# Get unique values\n",
    "unique_values = np.unique(argmax_result_np)\n",
    "\n",
    "print(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "95defe0a-94a0-4e05-83cf-67cafbbb880d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unique_values = np.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7827297a-766e-48d8-8866-a21841536ea2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  9, 10])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0d43db-21d0-4662-9e4b-462cd9de04a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa348874-2327-4ffc-9f60-fb0f62ed4b7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dims (3696, 4560, 8)\n",
      "33/33 [==============================] - 2s 65ms/step\n",
      "Time taken for predictions: 4.175443887710571 seconds\n",
      "segmentation_patches_reshaped.shape (231, 285, 16, 16)\n",
      "row_indices_patch (231,)\n",
      "col_indices_patch (285,)\n",
      "stitched_array (3696, 4560)\n",
      "Time taken including stitching: 5.486307144165039 seconds\n",
      "(3694, 4583, 1) (3694, 4583, 1)\n",
      "(3694, 4583)\n",
      "(3694, 4583)\n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 20]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "label_image_path = '/home/jupyter/label_folder/continuous_label_raster.tif'\n",
    "\n",
    "geojson_datapath = '/home/jupyter/label_folder/newextent_1123.geojson'\n",
    "\n",
    "model_path = '/home/jupyter/trained_models_srikar/best_epoch_model_2024-05-15_02-51-42.h5'\n",
    "\n",
    "test_image_path = '/home/jupyter/image_folder/area2_0530_2022_8bands.tif'\n",
    "\n",
    "l, predicted_array = prediction(test_image_path, model_path)\n",
    "new_predict, ground_truth_array = label_image_processing(predicted_array,label_image_path)\n",
    "\n",
    "eval_results = compute_metrics(ground_truth_array, new_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f0827bd-3b4d-47ab-bbc8-fd00721434ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth_array.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e88e619e-eb5e-4784-9701-ea9e203f09b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_predict.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "30214bdd-72a6-4d58-b066-06f22921a220",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3694, 4583, 1) (3694, 4583, 1)\n",
      "(3694, 4583)\n",
      "(3694, 4583)\n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]\n"
     ]
    }
   ],
   "source": [
    "new_predict, ground_truth_array = label_image_processing(predicted_array,label_image_path)\n",
    "\n",
    "eval_results = compute_metrics(ground_truth_array, new_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6c779743-a154-4dcf-b827-a16b436c5168",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.99843720655713,\n",
       "  0.9621420027006249,\n",
       "  0.8943815765448053,\n",
       "  0.8554710361162345,\n",
       "  0.994460416524465,\n",
       "  0.8741312746172538,\n",
       "  0.9208860761419522,\n",
       "  0.8785714298104956,\n",
       "  0.9821134868604919,\n",
       "  0.9710014208968923,\n",
       "  0.9753704266407469,\n",
       "  0.9089506186890337,\n",
       "  0.9663355409317452,\n",
       "  0.9182813818638546,\n",
       "  0.9865104721813615,\n",
       "  0.8758169944786193,\n",
       "  0.8660508114076025,\n",
       "  0.9371990790512893,\n",
       "  0.6972477203097376,\n",
       "  0.9983320978508776],\n",
       " [0.9992179922537859,\n",
       "  0.9807057814586481,\n",
       "  0.9442464903120451,\n",
       "  0.9221065926803744,\n",
       "  0.9972225155584071,\n",
       "  0.9328388979545965,\n",
       "  0.9588138398697076,\n",
       "  0.9353612225129754,\n",
       "  0.9909760403531915,\n",
       "  0.9852873880891605,\n",
       "  0.9875316685220987,\n",
       "  0.9523039696660957,\n",
       "  0.9828795972735342,\n",
       "  0.9574000893611476,\n",
       "  0.9932094371100619,\n",
       "  0.9337979140514027,\n",
       "  0.928217835046809,\n",
       "  0.9675815874566872,\n",
       "  0.8216216534696851,\n",
       "  0.9991653530558906],\n",
       " [0.9985108766375326,\n",
       "  0.9714387211367673,\n",
       "  0.975319333189002,\n",
       "  0.9593175853018373,\n",
       "  0.9961368265786331,\n",
       "  0.9005568814638027,\n",
       "  0.9650510204081633,\n",
       "  0.9025157232704403,\n",
       "  0.9984324380813042,\n",
       "  0.974468982107208,\n",
       "  0.995194631117271,\n",
       "  0.9703459637561779,\n",
       "  0.9787590832867524,\n",
       "  0.9366943569177886,\n",
       "  0.9896723646723646,\n",
       "  0.9528888888888889,\n",
       "  0.8928571428571429,\n",
       "  0.9429233361415332,\n",
       "  0.7638190954773869,\n",
       "  0.9997030768659764],\n",
       " [0.9999261099617166,\n",
       "  0.9901513498514365,\n",
       "  0.9150924214603039,\n",
       "  0.8876745571108847,\n",
       "  0.9983105722945864,\n",
       "  0.9675213633866607,\n",
       "  0.9526567591219925,\n",
       "  0.9706877004432052,\n",
       "  0.9836301853354986,\n",
       "  0.9963486990977802,\n",
       "  0.9799858130949721,\n",
       "  0.9349206200806252,\n",
       "  0.9870349464852453,\n",
       "  0.9790419132364014,\n",
       "  0.9967718759082788,\n",
       "  0.9154568666485323,\n",
       "  0.9664948204511645,\n",
       "  0.9935641345016332,\n",
       "  0.8888888369070855,\n",
       "  0.9986282066592659],\n",
       " [0.9985108765705549,\n",
       "  0.97143872044658,\n",
       "  0.9753193310774642,\n",
       "  0.9593175821544698,\n",
       "  0.9961368258789515,\n",
       "  0.9005568778816353,\n",
       "  0.9650510179462983,\n",
       "  0.9025157138101079,\n",
       "  0.9984324370379011,\n",
       "  0.9744689819863765,\n",
       "  0.9951946310265257,\n",
       "  0.970345947770248,\n",
       "  0.9787590805512602,\n",
       "  0.9366943542346193,\n",
       "  0.9896723611478904,\n",
       "  0.9528888804187655,\n",
       "  0.8928571215986401,\n",
       "  0.9429233341555954,\n",
       "  0.7638190570945197,\n",
       "  0.9997030764949326],\n",
       " 0.9230845535087605)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86b88ede-d0f3-4c34-bf92-980919920525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.9984107079391904,\n",
       "  0.8317522700692829,\n",
       "  0.39901746834239915,\n",
       "  0.5148471626313381,\n",
       "  0.7364657274412195,\n",
       "  0.23959828106313383,\n",
       "  0.37460408958450325,\n",
       "  0.004153696737760158,\n",
       "  0.853545454678595,\n",
       "  0.8936892400248729,\n",
       "  0.8897405005520033,\n",
       "  0.004893980344307009,\n",
       "  0.5606490881122738,\n",
       "  0.5540875318866897,\n",
       "  0.7022593633480056,\n",
       "  0.23057433082285192,\n",
       "  0.12380954467120134,\n",
       "  0.39843228919540985,\n",
       "  5.0251253756218405e-08,\n",
       "  0.8771714842221684],\n",
       " [0.9992047220386227,\n",
       "  0.9081492994088154,\n",
       "  0.5704252847308815,\n",
       "  0.6797347957916766,\n",
       "  0.8482352585802033,\n",
       "  0.38657407874255184,\n",
       "  0.5450356110755807,\n",
       "  0.00827302990410517,\n",
       "  0.9209868071406215,\n",
       "  0.9438605038162955,\n",
       "  0.9416536295140371,\n",
       "  0.009740292049670585,\n",
       "  0.7184819356401327,\n",
       "  0.7130712025596824,\n",
       "  0.8250909112271075,\n",
       "  0.3747426329804898,\n",
       "  0.22033902075552925,\n",
       "  0.5698270738582862,\n",
       "  1.0050250751243681e-07,\n",
       "  0.9345672377439199],\n",
       " [0.9986919862356706,\n",
       "  0.872113676731794,\n",
       "  0.4747780904957783,\n",
       "  0.7736220472440944,\n",
       "  0.9765400014047904,\n",
       "  0.2657120127287192,\n",
       "  0.6637755102040817,\n",
       "  0.0041928721174004195,\n",
       "  0.9811892569756505,\n",
       "  0.9369846367502821,\n",
       "  0.9494843574756768,\n",
       "  0.004942339373970346,\n",
       "  0.7724986025712689,\n",
       "  0.768834144944142,\n",
       "  0.8080484330484331,\n",
       "  0.24266666666666667,\n",
       "  0.12380952380952381,\n",
       "  0.5031592249368155,\n",
       "  0.0,\n",
       "  0.9033144044835393],\n",
       " [0.9997179844623095,\n",
       "  0.9472912479184356,\n",
       "  0.7143322452301881,\n",
       "  0.6061696642514919,\n",
       "  0.7497303706051929,\n",
       "  0.7091295041493684,\n",
       "  0.4623312003156873,\n",
       "  0.30769207100609924,\n",
       "  0.8677449160187201,\n",
       "  0.9508380308849861,\n",
       "  0.9339510102935654,\n",
       "  0.3333329629633745,\n",
       "  0.6715257515269054,\n",
       "  0.6648501345928904,\n",
       "  0.8428677531839979,\n",
       "  0.8222891318587612,\n",
       "  0.9999998076923446,\n",
       "  0.6568600476852899,\n",
       "  0.0,\n",
       "  0.9680601404208026],\n",
       " [0.9986919861686807,\n",
       "  0.872113676112175,\n",
       "  0.4747780894678976,\n",
       "  0.7736220447059644,\n",
       "  0.9765400007188734,\n",
       "  0.2657120116717899,\n",
       "  0.6637755085107767,\n",
       "  0.004192872073449978,\n",
       "  0.9811892559502673,\n",
       "  0.9369846366340986,\n",
       "  0.9494843573890995,\n",
       "  0.0049423392925479525,\n",
       "  0.7724986004122454,\n",
       "  0.76883414274181,\n",
       "  0.8080484301707677,\n",
       "  0.24266666450962965,\n",
       "  0.12380952086167808,\n",
       "  0.5031592238770868,\n",
       "  0.0,\n",
       "  0.9033144041482707],\n",
       " 0.509385113095923)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eded1df6-8b40-47c8-b6f2-6ab91b1898f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_wise_iou,class_wise_dice_score,class_wise_accuracy, class_wise_precision, class_wise_recall, mean_iou = eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9355f66d-e2df-490a-9807-c1bbd246f981",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Flatten the arrays to 1D\n",
    "ground_truth_flat = ground_truth_array.flatten()\n",
    "predictions_flat = new_predict.flatten()\n",
    "\n",
    "# Create the confusion matrix\n",
    "conf_matrix = confusion_matrix(ground_truth_flat, predictions_flat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e4cdc9a-988f-420f-8904-ff777c3f9543",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_names = [\n",
    "    \"lake\",\n",
    "    \"settlement\",\n",
    "    \"shrub land\",\n",
    "    \"grass land\",\n",
    "    \"homogenous forest\",\n",
    "    \"agriculture1 (with vegetation)\",\n",
    "    \"agriculture2 (without vegetation)\",\n",
    "    \"open area\",\n",
    "    \"clove plantation\",\n",
    "    \"mixed forest1\",\n",
    "    \"mixed forest2\",\n",
    "    \"rice field1\",\n",
    "    \"rice field2\",\n",
    "    \"rice field3\",\n",
    "    \"mixed garden\",\n",
    "    \"grass land2\",\n",
    "    \"grass land3\",\n",
    "    \"mixed garden2\",\n",
    "    \"agroforestry\",\n",
    "    \"clouds\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5defbdd8-8539-4d0b-b1d7-3198bc4215ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to class_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "class_names = [\n",
    "    \"lake\",\n",
    "    \"settlement\",\n",
    "    \"shrub land\",\n",
    "    \"grass land\",\n",
    "    \"homogenous forest\",\n",
    "    \"agriculture1 (with vegetation)\",\n",
    "    \"agriculture2 (without vegetation)\",\n",
    "    \"open area\",\n",
    "    \"clove plantation\",\n",
    "    \"mixed forest1\",\n",
    "    \"mixed forest2\",\n",
    "    \"rice field1\",\n",
    "    \"rice field2\",\n",
    "    \"rice field3\",\n",
    "    \"mixed garden\",\n",
    "    \"grass land2\",\n",
    "    \"grass land3\",\n",
    "    \"mixed garden2\",\n",
    "    \"agroforestry\",\n",
    "    \"clouds\",\n",
    "]\n",
    "\n",
    "class_metrics = {}\n",
    "\n",
    "class_metrics[\"class_wise_iou\"] = dict(zip(class_names, class_wise_iou))\n",
    "class_metrics[\"class_wise_dice_score\"] = dict(zip(class_names, class_wise_dice_score))\n",
    "class_metrics[\"class_wise_accuracy\"] = dict(zip(class_names, class_wise_accuracy))\n",
    "class_metrics[\"class_wise_precision\"] = dict(zip(class_names, class_wise_precision))\n",
    "class_metrics[\"class_wise_recall\"] = dict(zip(class_names, class_wise_recall))\n",
    "\n",
    "# Writing to CSV\n",
    "with open('class_metrics_all_images_1.csv', 'w', newline='') as csvfile:\n",
    "    fieldnames = ['Class Name', 'Class Wise IoU', 'Class Wise Dice Score', 'Class Wise Accuracy', 'Class Wise Precision', 'Class Wise Recall']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    \n",
    "    writer.writeheader()\n",
    "    for class_name in class_names:\n",
    "        writer.writerow({\n",
    "            'Class Name': class_name,\n",
    "            'Class Wise IoU': class_metrics[\"class_wise_iou\"][class_name],\n",
    "            'Class Wise Dice Score': class_metrics[\"class_wise_dice_score\"][class_name],\n",
    "            'Class Wise Accuracy': class_metrics[\"class_wise_accuracy\"][class_name],\n",
    "            'Class Wise Precision': class_metrics[\"class_wise_precision\"][class_name],\n",
    "            'Class Wise Recall': class_metrics[\"class_wise_recall\"][class_name]\n",
    "        })\n",
    "\n",
    "print(\"Data has been saved to class_metrics.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c6e313-33d3-43a9-a545-a2558120748f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1-lake\n",
    "#2-settlement\n",
    "#3-shrub land\n",
    "#4-grass land\n",
    "#5-homogenous forest\n",
    "#6-agriculture1 (with vegetation)\n",
    "#7-agriculture2 (without vegetation)\n",
    "#8-open area\n",
    "#9-clove plantation\n",
    "#10-mixed forest1\n",
    "#11-mixed forest2\n",
    "#12-rice field1\n",
    "#13-rice field2\n",
    "#14-rice field3\n",
    "#15-mixed garden\n",
    "#16-grass land2\n",
    "#17-grass land3\n",
    "#18-mixed garden2\n",
    "#19-agroforestry\n",
    "#20-cloud\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f05b5f4-a1d2-4200-b7f7-f16c145d5105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False, xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "# plt.show()\n",
    "\n",
    "plt.savefig('confusion_all_images')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bac2be2-a6ed-42e1-8733-560f713197ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "1 5 5 230\n",
    "2 190 60 15\n",
    "3 65 240 125\n",
    "4 105 200 95\n",
    "5 30 115 10\n",
    "6 255 196 34\n",
    "7 110 85 5\n",
    "8 235 235 220\n",
    "9 120 216 47\n",
    "10 84 142 128\n",
    "11 84 142 128\n",
    "12 50 255 215\n",
    "13 50 255 215\n",
    "14 50 255 215\n",
    "15 193 255 0\n",
    "16 105 200 95\n",
    "17 105 200 95\n",
    "18 193 255 0\n",
    "19 255 50 185\n",
    "20 255 255 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78440e0b-30a4-4628-b4b2-db38d3e4176a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_segmentation_mask(stitched_array):\n",
    "    # Define the class-color mapping\n",
    "    class_colors = {\n",
    "        1: (5, 5, 230),\n",
    "        2: (190, 60, 15),\n",
    "        3: (65, 240, 125),\n",
    "        4: (105, 200, 95),\n",
    "        5: (30, 115, 10),\n",
    "        6: (255, 196, 34),\n",
    "        7: (110, 85, 5),\n",
    "        8: (235, 235, 220),\n",
    "        9: (120, 216, 47),\n",
    "        10: (84, 142, 128),\n",
    "        11: (84, 142, 128),\n",
    "        12: (50, 255, 215),\n",
    "        13: (50, 255, 215),\n",
    "        14: (50, 255, 215),\n",
    "        15: (193, 255, 0),\n",
    "        16: (105, 200, 95),\n",
    "        17: (105, 200, 95),\n",
    "        18: (193, 255, 0),\n",
    "        19: (255, 50, 185),\n",
    "        20: (255, 255, 255)\n",
    "    }\n",
    "\n",
    "    # Create a colormap using the class-color mapping\n",
    "    colors = [class_colors[i] for i in range(1, 21)]\n",
    "    normalized_colors_array = np.array([tuple(np.array(v) / 255.0) for v in class_colors.values()])\n",
    "\n",
    "    cmap = ListedColormap(normalized_colors_array)\n",
    "\n",
    "    # Create a figure and axis for the plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "\n",
    "    # Plot the segmentation mask using the custom colormap\n",
    "    image = ax.imshow(stitched_array, cmap=cmap, vmin=1, vmax=20)\n",
    "\n",
    "    # Add a colorbar to show the class-color mapping\n",
    "    cbar = plt.colorbar(image, ax=ax, ticks=list(class_colors.keys()))\n",
    "    cbar.set_label('Classes')\n",
    "\n",
    "    # save the plot\n",
    "    current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    filename = f'content/segmentation_mask_{current_time}.png'\n",
    "    plt.savefig(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "309ca694-952e-4a99-8629-133fd3d3db2e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "save_segmentation_mask(predicted_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8daf096-9072-4485-bfb2-f8add8cc62a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow 2.10 (Local)",
   "language": "python",
   "name": "tf2-2-10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
