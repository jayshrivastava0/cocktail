{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNabELG3UpXdpmDYCSRWvQR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/realtechsupport/cocktail/blob/main/sandbox/modularize_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Target preprocessing - clipping and preprocessing"
      ],
      "metadata": {
        "id": "nrdBFONKb-A2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#imports and dowloads\n",
        "\n",
        "!pip install rasterio\n",
        "!pip install gdal\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import rasterio\n",
        "from osgeo import gdal, osr\n",
        "import keras\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "KRolAAoZcgrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#target clipping\n",
        "\n",
        "rasterfile = gdal.Open(datapath + ps)\n",
        "\n",
        "print('\\nPerforming the clip operation...\\n')\n",
        "\n",
        "\n",
        "warp_options = gdal.WarpOptions(cutlineDSName = roipath + roishape, cropToCutline = True)\n",
        "rasterfile_new = ps.split('.tif')[0] + '_roi.tif'\n",
        "print(roipath + rasterfile_new, datapath + ps )\n",
        "ds = gdal.Warp(roipath + rasterfile_new, datapath + ps,  options = warp_options)\n",
        "\n",
        "cols = ds.RasterXSize\n",
        "rows = ds.RasterYSize\n",
        "bands = ds.RasterCount\n",
        "projInfo = ds.GetProjection()\n",
        "spatialRef = osr.SpatialReference()\n",
        "spatialRef.ImportFromWkt(projInfo)\n",
        "spatialRefProj = spatialRef.ExportToProj4()\n",
        "ds = None\n",
        "\n",
        "print('\\nClipped raster input: ', rasterfile_new)\n",
        "print('Checking spatial reference info\\n')\n",
        "print (\"WKT format: \" + str(spatialRef))\n",
        "print (\"Proj4 format: \" + str(spatialRefProj))\n",
        "print (\"Number of columns: \" + str(cols))\n",
        "print (\"Number of rows: \" + str(rows))\n",
        "print (\"Number of bands: \" + str(bands))\n",
        "\n",
        "\n",
        "datapath = '/content/gdrive/MyDrive/exp/'\n",
        "ps = 'crop_mask.tif'\n",
        "patch_size = 256\n",
        "roipath = '/content/gdrive/MyDrive/exp/other images/roi_folder/'\n",
        "roishape = 'area2_square.geojson'\n",
        "\n",
        "#---------------------------------\n",
        "\n",
        "#target preprocessing\n",
        "\n",
        "def target_preprocessing(folder_path, patch_size):\n",
        "\n",
        "    with rasterio.open(folder_path) as src:\n",
        "        # Read the TIFF data\n",
        "        output_mask = src.read()\n",
        "\n",
        "        # Get the shape of the TIFF data\n",
        "        num_bands, height, width = output_mask.shape\n",
        "\n",
        "        # Calculate the new width and height that are multiples of the patch size\n",
        "        new_width = int(np.floor(width / patch_size)) * patch_size\n",
        "        new_height = int(np.floor(height / patch_size)) * patch_size\n",
        "\n",
        "        print(\"cropped dimensions:\", new_height, new_width)\n",
        "\n",
        "        output_mask = np.moveaxis(output_mask, 0, -1)\n",
        "\n",
        "        # Crop the input_image to the new dimensions\n",
        "        cropped_mask = output_mask[:new_height, :new_width, :]\n",
        "\n",
        "    new_mask = np.squeeze(cropped_mask)\n",
        "\n",
        "    masks = []\n",
        "    for i in range(0, new_mask.shape[0], patch_size):\n",
        "        for j in range(0, new_mask.shape[1], patch_size):\n",
        "            patch = new_mask[i:i+patch_size, j:j+patch_size]\n",
        "            masks.append(patch)\n",
        "\n",
        "    mask_array = np.array(masks)\n",
        "\n",
        "    useful_masks = []\n",
        "    useless = 0\n",
        "    indexes = []\n",
        "    for index,img in enumerate(mask_array):\n",
        "\n",
        "        val, counts = np.unique(img, return_counts=True)\n",
        "\n",
        "        if (1 - (counts[0]/counts.sum())) > 0.05:\n",
        "          useful_masks.append(img)\n",
        "          indexes.append(index)\n",
        "        else:\n",
        "          #print(\"I am useless\")\n",
        "          useless +=1\n",
        "\n",
        "\n",
        "    print(\"Total useful images are: \", len(mask_array)-useless)\n",
        "    print(indexes)\n",
        "    print(\"Total useless images are: \", useless)\n",
        "\n",
        "    useful_masks_array = np.array(useful_masks)\n",
        "\n",
        "    useful_masks_onehot = to_categorical(useful_masks_array)\n",
        "\n",
        "\n",
        "    return useful_masks, useful_masks_onehot, indexes\n",
        "\n",
        "\n",
        "folder_path = '/content/gdrive/MyDrive/exp/other images/roi_folder/crop_mask_roi.tif'\n",
        "patch_size = 256\n",
        "target = target_preprocessing(folder_path,patch_size)"
      ],
      "metadata": {
        "id": "R-6D0L2VbBNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image preprocessing:\n",
        "\n",
        "- contains clipping, preprocessing, sampling and multi-image processing functions"
      ],
      "metadata": {
        "id": "WMvVHZhKc-21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def newclipping(datapath,roipath,ps, roishape):\n",
        "  rasterfile = gdal.Open(datapath + ps)\n",
        "\n",
        "  print('\\nPerforming the clip operation...\\n')\n",
        "\n",
        "\n",
        "  warp_options = gdal.WarpOptions(cutlineDSName = roipath + roishape, cropToCutline = True)\n",
        "  rasterfile_new = ps.split('.tif')[0] + '_roi.tif'\n",
        "  print(roipath + rasterfile_new, datapath + ps )\n",
        "  ds = gdal.Warp(roipath + rasterfile_new, datapath + ps,  options = warp_options)\n",
        "\n",
        "  cols = ds.RasterXSize\n",
        "  rows = ds.RasterYSize\n",
        "  bands = ds.RasterCount\n",
        "  projInfo = ds.GetProjection()\n",
        "  spatialRef = osr.SpatialReference()\n",
        "  spatialRef.ImportFromWkt(projInfo)\n",
        "  spatialRefProj = spatialRef.ExportToProj4()\n",
        "  ds = None\n",
        "\n",
        "  print('\\nClipped raster input: ', rasterfile_new)\n",
        "  print('Checking spatial reference info\\n')\n",
        "  print (\"WKT format: \" + str(spatialRef))\n",
        "  print (\"Proj4 format: \" + str(spatialRefProj))\n",
        "  print (\"Number of columns: \" + str(cols))\n",
        "  print (\"Number of rows: \" + str(rows))\n",
        "  print (\"Number of bands: \" + str(bands))\n",
        "\n",
        "  return roipath + rasterfile_new"
      ],
      "metadata": {
        "id": "VwcJoMjqdB-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gORZC0OVdTJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(filelocation,patch_size):\n",
        "    # Load the GeoTIFF file\n",
        "    with rasterio.open(filelocation) as src:\n",
        "        # Read the TIFF data\n",
        "        tiff_data = src.read()\n",
        "        print(\"total number of nan in original\",np.count_nonzero(np.isnan(tiff_data)))\n",
        "\n",
        "        # Get the shape of the TIFF data\n",
        "        num_bands, height, width = tiff_data.shape\n",
        "\n",
        "        print(\"Original image dimensions:\", num_bands, height, width)\n",
        "        unique_elements, counts_elements = np.unique(tiff_data, return_counts=True)\n",
        "        print(unique_elements, counts_elements )\n",
        "        print(\"total unique\",len(counts_elements))\n",
        "\n",
        "        print(np.min(tiff_data), np.max(tiff_data))\n",
        "\n",
        "\n",
        "        normalized_image = np.zeros_like(tiff_data, dtype='float32')\n",
        "\n",
        "        for band, count in enumerate(range(tiff_data.shape[0])):\n",
        "            band_data = tiff_data[band, :, :]\n",
        "            band_min = np.min(band_data)\n",
        "            band_max = np.max(band_data)\n",
        "            print(\"band-\", count+1,\"maximum-\",band_max,\"minimum-\",band_min)\n",
        "            #print(band_data)\n",
        "            #epsilon is added to prevent division by zero\n",
        "            normalized_band = (band_data - band_min) / (band_max - band_min + 1e-10)\n",
        "            normalized_image[band, :, :] = normalized_band\n",
        "\n",
        "        # Calculate the new width and height that are multiples of the patch size\n",
        "        new_width = int(np.floor(width / patch_size)) * patch_size\n",
        "        new_height = int(np.floor(height / patch_size)) * patch_size\n",
        "\n",
        "        print(\"cropped dimensions:\", new_height, new_width)\n",
        "\n",
        "        input_image = np.moveaxis(normalized_image, 0, -1)\n",
        "\n",
        "        # Crop the input_image to the new dimensions\n",
        "        cropped_array = input_image[:new_height, :new_width, :]\n",
        "\n",
        "    print(\"total number of nan\",np.count_nonzero(np.isnan(cropped_array)))\n",
        "    print(\"Cropped array shape:\", cropped_array.shape)\n",
        "    print(np.min(cropped_array), np.max(cropped_array))\n",
        "\n",
        "    patches = []\n",
        "    for i in range(0, cropped_array.shape[0], patch_size):\n",
        "        for j in range(0, cropped_array.shape[1], patch_size):\n",
        "            patch = cropped_array[i:i+patch_size, j:j+patch_size]\n",
        "            patches.append(patch)\n",
        "    print(\"patches are created\")\n",
        "    return patches"
      ],
      "metadata": {
        "id": "hjpMHNlTNQP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sampling(patches,indexes):\n",
        "    useful_patches = []\n",
        "    for number_of_masks,i in enumerate(indexes):\n",
        "      print(i)\n",
        "      if i < len(patches):\n",
        "        useful_patches.append(patches[i])\n",
        "      else:\n",
        "        break\n",
        "    return useful_patches, number_of_masks\n",
        "\n"
      ],
      "metadata": {
        "id": "npUwvkThNyh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_images_in_folder(datapath, patch_size, roipath,target, roishape = 'area2_square.geojson'):\n",
        "    # Initialize an empty list to store the sampled patches and target_patches\n",
        "    sampled_image_patches = []\n",
        "    target_patches = []\n",
        "\n",
        "    # Loop through all files in the folder\n",
        "    for ps in os.listdir(datapath):\n",
        "\n",
        "        # #print(type(filename),filename.split('.tif')[0] + '_roi.tif' )\n",
        "        # print(folder_path + ps)\n",
        "        # print(roipath + roishape)\n",
        "        # print(ps)\n",
        "\n",
        "        # Apply clipping\n",
        "        clipped_image_path = newclipping(datapath,roipath,ps, roishape)\n",
        "\n",
        "        # Preprocess(normalize,resize,create patches) the clipped image\n",
        "        patches = preprocessing(clipped_image_path, patch_size)\n",
        "        print(len(patches))\n",
        "\n",
        "        # Sample patches from the processed image patches and create corresponding target patches\n",
        "        #print(target[2])\n",
        "        #print(len(target[2]))\n",
        "        useful_patches, number_of_masks = sampling(patches,target[2])\n",
        "        sampled_image_patches.extend(useful_patches)\n",
        "        target_patches.extend(target[1][0:number_of_masks])\n",
        "        print(len(sampled_image_patches),len(target_patches) )\n",
        "\n",
        "\n",
        "    return sampled_image_patches,target_patches"
      ],
      "metadata": {
        "id": "57tIha5_Lg7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datapath = '/content/gdrive/MyDrive/exp/other images/test_folder/'\n",
        "roipath = '/content/gdrive/MyDrive/exp/other images/roi_folder/'\n",
        "patch_size = 256\n",
        "\n",
        "dataset = process_images_in_folder(datapath, patch_size, roipath, target)"
      ],
      "metadata": {
        "id": "M2o-WeGodc0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training:\n",
        "1. load the model\n",
        "2. create the training and val sets and train the model"
      ],
      "metadata": {
        "id": "wXE_C-RQdmAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def get_model(img_size, num_classes):\n",
        "    inputs = keras.Input(shape=img_size + (8,))\n",
        "    x = inputs\n",
        "\n",
        "    # Encoder\n",
        "    conv1 = layers.Conv2D(16, 3, activation=\"relu\", padding=\"same\")(x)\n",
        "    conv1 = layers.Conv2D(16, 3, activation=\"relu\", padding=\"same\")(conv1)\n",
        "    pool1 = layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "\n",
        "    conv2 = layers.Conv2D(32, 3, activation=\"relu\", padding=\"same\")(pool1)\n",
        "    conv2 = layers.Conv2D(32, 3, activation=\"relu\", padding=\"same\")(conv2)\n",
        "    pool2 = layers.MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "\n",
        "    # Bottleneck\n",
        "    conv3 = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(pool2)\n",
        "    conv3 = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(conv3)\n",
        "\n",
        "    # Decoder\n",
        "    up4 = layers.UpSampling2D(size=(2, 2))(conv3)\n",
        "    concat4 = layers.Concatenate()([up4, conv2])\n",
        "    conv4 = layers.Conv2D(32, 3, activation=\"relu\", padding=\"same\")(concat4)\n",
        "    conv4 = layers.Conv2D(32, 3, activation=\"relu\", padding=\"same\")(conv4)\n",
        "\n",
        "    up5 = layers.UpSampling2D(size=(2, 2))(conv4)\n",
        "    concat5 = layers.Concatenate()([up5, conv1])\n",
        "    conv5 = layers.Conv2D(16, 3, activation=\"relu\", padding=\"same\")(concat5)\n",
        "    conv5 = layers.Conv2D(16, 3, activation=\"relu\", padding=\"same\")(conv5)\n",
        "\n",
        "    # Output\n",
        "    outputs = layers.Conv2D(num_classes, 1, activation=\"softmax\")(conv5)\n",
        "\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "img_size = (256, 256)\n",
        "num_classes = 21\n",
        "model = get_model(img_size=img_size, num_classes=num_classes)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "SweYFhHtdsii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "img_size = (256, 256)\n",
        "num_imgs = len(dataset[0])\n",
        "\n",
        "input_img_paths = dataset[0]\n",
        "target_paths = dataset[1]\n",
        "\n",
        "random.Random(1337).shuffle(input_img_paths)\n",
        "random.Random(1337).shuffle(target_paths)\n",
        "\n",
        "\n",
        "input_imgs = np.zeros((num_imgs,) + img_size + (8,), dtype=\"float32\")\n",
        "targets = np.zeros((num_imgs,) + img_size + (21,), dtype=\"uint8\")\n",
        "\n",
        "for i in range(num_imgs):\n",
        "\n",
        "    input_imgs[i] = input_img_paths[i]\n",
        "    targets[i] = target_paths[i]\n",
        "\n",
        "num_val_samples = round(0.2 * num_imgs)\n",
        "\n",
        "train_input_imgs = input_imgs[:-num_val_samples]\n",
        "train_targets = targets[:-num_val_samples]\n",
        "\n",
        "val_input_imgs = input_imgs[-num_val_samples:]\n",
        "val_targets = targets[-num_val_samples:]\n",
        "\n"
      ],
      "metadata": {
        "id": "01WfkuQ-dv0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"new_segmentation.hdf5\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "\n",
        "history = model.fit(train_input_imgs, train_targets,\n",
        "                    epochs=5,\n",
        "                    callbacks=callbacks,\n",
        "                    batch_size=8,\n",
        "                    validation_data=(val_input_imgs, val_targets))"
      ],
      "metadata": {
        "id": "Htr_Ru5Td0zJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = range(1, len(history.history[\"loss\"]) + 1)\n",
        "loss = history.history[\"loss\"]\n",
        "val_loss = history.history[\"val_loss\"]\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
        "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
        "plt.title(\"Training and validation loss\")\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "iPgacdBbd3dt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing prediction:\n",
        "Same from old code"
      ],
      "metadata": {
        "id": "xKU4xSKseLaT"
      }
    }
  ]
}