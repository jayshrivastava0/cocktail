{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main CPU Code\n",
    "\n",
    "\n",
    "### Use the cell below to update cofig.json (Which pass onto the parameter to every other script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install rasterio\n",
    "!pip install geopandas\n",
    "!pip install Pillow\n",
    "\n",
    "\n",
    "\n",
    "# import sys\n",
    "# %cd /home/jupyter/eval_and_pred\n",
    "\n",
    "# sys.argv = [\n",
    "# 'your_script.py',\n",
    "# '--model_path', './trained_model/patch_h8_w8_batch_32_on_0530_2022.hdf5',\n",
    "# '--google_storage_path', 'gs://tf_records_bucket/model/',\n",
    "# '--model_name', 'patch_h8_w8_batch_32_on_0530_2022.hdf5',\n",
    "# '--test_image_path', 'gs://tf_records_bucket/labels/area2_0530_2022_8bands.tif',\n",
    "# '--img_size', '8', '8',\n",
    "# '--num_bands', '8',\n",
    "# '--num_classes', '21',\n",
    "# '--gcs_tfrecords', 'gs://tf_records_bucket/tf_records/Untitled Folder/',\n",
    "# '--class_name', '18',\n",
    "# '--label_path', 'gs://tf_records_bucket/labels/continuous_label_raster.tif',\n",
    "# '--class_optmized_model', '1',\n",
    "# '--bucket_name', 'tf_records_bucket',\n",
    "# '--threshold_percentage', '99.9',\n",
    "# '--patch_height', '8',\n",
    "# '--patch_width', '8',\n",
    "# '--batch_size', '32',\n",
    "# '--epochs', '3'\n",
    "# ]\n",
    "\n",
    "\n",
    "import os\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "from google.cloud import storage\n",
    "from pipeline_scripts.model_file import get_model\n",
    "from pipeline_scripts.writing_config import *\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "### +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "def load_config(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "    return config\n",
    "\n",
    "\n",
    "def pretrain():\n",
    "    # Load configuration\n",
    "    config = load_config('./pipeline_scripts/config.json')\n",
    "    # config[\"img_size\"] = config[\"img_size\"]\n",
    "    model_path = config.get(\"model_path\")\n",
    "    model_name = config.get(\"model_name\")\n",
    "    img_size_value = config.get(\"img_size\")\n",
    "    img_size = (img_size_value, img_size_value)\n",
    "    num_classes = config.get(\"num_classes\")\n",
    "    num_bands = config.get(\"num_bands\")\n",
    "    \n",
    "\n",
    "    # Create a dummy model\n",
    "    model = get_model(\n",
    "        img_size=img_size, \n",
    "        num_classes=num_classes, \n",
    "        num_bands=num_bands\n",
    "    )\n",
    "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\")\n",
    "\n",
    "    # Save the dummy model\n",
    "    model.save(model_path)\n",
    "    print(\"Dummy model saved at:\", model_path)\n",
    "\n",
    "\n",
    "    # Optionally, you can return the path to the saved dummy model\n",
    "    return model_path\n",
    "\n",
    "### ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "# pretrain()\n",
    "\n",
    "\n",
    "\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import mapping\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import time\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from shapely.geometry import mapping\n",
    "\n",
    "from pipeline_scripts.prediction import *\n",
    "from pipeline_scripts.eval import *\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from google.cloud import storage\n",
    "import os\n",
    "import csv\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "\n",
    "# metrics_results = compute_metrics(new_ground_truth, new_predict)\n",
    "\n",
    "\n",
    "# metrics_results\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "class_colors = {\n",
    "        1: ( 5, 5, 230),\n",
    "        2: (190, 60, 15),\n",
    "        3: (65, 240, 125),\n",
    "        4: (105, 200, 95),\n",
    "        5: ( 30, 115, 10),\n",
    "        6: ( 255, 196, 34),\n",
    "        7: (110, 85, 5),\n",
    "        8: ( 235, 235, 220),\n",
    "        9: (120, 216, 47),\n",
    "        10: ( 84, 142, 128),\n",
    "        11: ( 84, 142, 128),\n",
    "        12: ( 84, 142, 128),\n",
    "        13: ( 50, 255, 215),\n",
    "        14: ( 50, 255, 215),\n",
    "        15: ( 50, 255, 215),\n",
    "        16: ( 193, 255, 0),\n",
    "        17: ( 105, 200, 95),\n",
    "        18: (105, 200, 95),\n",
    "        19: ( 105, 200, 95),\n",
    "        20: (193, 255, 0),\n",
    "        21: ( 255, 50, 185),\n",
    "        22: (255, 255, 255),\n",
    "}\n",
    "\n",
    "# Create a colormap using the class-color mapping\n",
    "colors = [class_colors[i] for i in range(1, 23)]\n",
    "normalized_colors_array = np.array([tuple(np.array(v) / 255.0) for v in class_colors.values()])\n",
    "\n",
    "cmap_image = ListedColormap(normalized_colors_array)\n",
    "\n",
    "\n",
    "\n",
    "class CustomMetricsCSVLogger(Callback):\n",
    "    def __init__(self, filename, separator=',', append=True):\n",
    "        super(CustomMetricsCSVLogger, self).__init__()\n",
    "        self.filename = filename\n",
    "        self.separator = separator\n",
    "        self.append = append\n",
    "        self.keys = None\n",
    "        self.append_header = True\n",
    "        self.max_epoch = 0  # Track the highest epoch number encountered\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        # Initialize min and max class-wise IOU at the beginning of each epoch\n",
    "        self.min_class_wise_iou = 100\n",
    "        self.max_class_wise_iou = 0\n",
    "\n",
    "        # Check if the file exists in Cloud Storage, if not, create it\n",
    "        if not self.file_exists():\n",
    "            self.create_file()\n",
    "\n",
    "    def file_exists(self):\n",
    "        # Check if the file exists in Cloud Storage\n",
    "        storage_client = storage.Client()\n",
    "        bucket_name, blob_name = self.parse_gcs_path(self.filename)\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(blob_name)\n",
    "        return blob.exists()\n",
    "\n",
    "    def create_file(self):\n",
    "        # Create the file in Cloud Storage and write the header\n",
    "        storage_client = storage.Client()\n",
    "        bucket_name, blob_name = self.parse_gcs_path(self.filename)\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(blob_name)\n",
    "\n",
    "        # Write the header to the blob\n",
    "        header = 'epoch,loss,val_loss,class_wise_iou,class_wise_dice_score,class_wise_accuracy,class_wise_precision,class_wise_recall,mean_iou,min_class_wise_iou,max_class_wise_iou\\n'\n",
    "        blob.upload_from_string(header)\n",
    "\n",
    "    def parse_gcs_path(self, gcs_path):\n",
    "        # Parse the Google Cloud Storage path to extract bucket name and blob name\n",
    "        parts = gcs_path.replace('gs://', '').split('/')\n",
    "        return parts[0], '/'.join(parts[1:])\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        if self.keys is None:\n",
    "            self.keys = sorted(logs.keys())\n",
    "\n",
    "        # Extract class-wise IOU\n",
    "        class_wise_iou = logs.get('class_wise_iou', 0.0)\n",
    "\n",
    "        # Update min and max class-wise IOU\n",
    "        self.min_class_wise_iou = min(self.min_class_wise_iou, class_wise_iou)\n",
    "        self.max_class_wise_iou = max(self.max_class_wise_iou, class_wise_iou)\n",
    "\n",
    "        # Append the row to the file in Cloud Storage\n",
    "        storage_client = storage.Client()\n",
    "        bucket_name, blob_name = self.parse_gcs_path(self.filename)\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(blob_name)\n",
    "\n",
    "        # Download existing content\n",
    "        existing_content = blob.download_as_text() if blob.exists() else \"\"\n",
    "\n",
    "        # Extract metrics values from logs\n",
    "        metrics_values = [str(logs[key]) for key in ['loss', 'val_loss', 'class_wise_iou', 'class_wise_dice_score',\n",
    "                                                    'class_wise_accuracy', 'class_wise_precision', 'class_wise_recall', 'mean_iou']]\n",
    "\n",
    "        # Check if metrics for the current epoch already exist\n",
    "        epoch_exists = any(f\"{epoch},\" in line for line in existing_content.split('\\n'))\n",
    "\n",
    "        # If the file is empty or epoch entry doesn't exist, append the metrics\n",
    "        if not existing_content or not epoch_exists:\n",
    "            updated_content = existing_content + f\"{epoch},{','.join(metrics_values)},{self.min_class_wise_iou},{self.max_class_wise_iou}\\n\"\n",
    "        else:\n",
    "            # Get the maximum epoch number in the existing content\n",
    "            max_existing_epoch = max(\n",
    "                int(line.split(',')[0]) for line in existing_content.split('\\n') if line.strip() and not line.startswith('epoch')\n",
    "            )\n",
    "\n",
    "            # Increment the epoch for the new entries\n",
    "            updated_content = existing_content + f\"{max_existing_epoch + 1},{','.join(metrics_values)},{self.min_class_wise_iou},{self.max_class_wise_iou}\\n\"\n",
    "\n",
    "        # Update the highest epoch number\n",
    "        self.max_epoch = max(self.max_epoch, epoch)\n",
    "\n",
    "        # Upload updated content\n",
    "        blob.upload_from_string(updated_content)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import io\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "\n",
    "class PredictSegmentationCallback(Callback):\n",
    "    def __init__(self, test_image_path, output_save_path):\n",
    "        super(PredictSegmentationCallback, self).__init__()\n",
    "        self.test_image_path = test_image_path\n",
    "        self.output_save_path = output_save_path  # This variable may contain the full path to the image\n",
    "        self.last_predicted_array = None\n",
    "        self.epoch_counter = 0\n",
    "        self.start_time_str = datetime.now().strftime(\"%m_%d_%H_%M\")  # Changed format for date-time\n",
    "\n",
    "    def parse_gcs_path(self, gcs_path):\n",
    "        parts = gcs_path.replace('gs://', '').split('/')\n",
    "        return parts[0], '/'.join(parts[1:])\n",
    "\n",
    "    def create_folder_if_not_exists(self, folder_path):\n",
    "        storage_client = storage.Client()\n",
    "        bucket_name, folder_blob_name = self.parse_gcs_path(folder_path)\n",
    "        bucket = storage_client.bucket(bucket_name)  \n",
    "\n",
    "        # Ensure the folder_blob_name ends with '/'\n",
    "        if not folder_blob_name.endswith('/'):\n",
    "            folder_blob_name += '/'\n",
    "\n",
    "        # Check if the folder already exists\n",
    "        folder_blob = bucket.blob(folder_blob_name)\n",
    "        if not folder_blob.exists():\n",
    "            # Create an empty blob to represent the folder\n",
    "            folder_blob.upload_from_string('')\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.epoch_counter += 1\n",
    "        if self.epoch_counter % 2 == 0:\n",
    "            self.last_predicted_array = prediction_function_img(self.test_image_path)\n",
    "            pil_image = Image.fromarray((self.last_predicted_array * 255).astype(np.uint8))\n",
    "            pil_image_colored = pil_image.convert('P', palette=Image.ADAPTIVE, colors=len(class_colors))\n",
    "            pil_image_colored.putpalette(np.array(normalized_colors_array * 255, dtype=np.uint8).flatten())\n",
    "\n",
    "            # Get the name of the test image without extension\n",
    "            test_image_name = os.path.splitext(os.path.basename(self.test_image_path))[0]\n",
    "\n",
    "            # Construct the image name with the start time and epoch\n",
    "            image_name = f\"{self.start_time_str}_epoch_{epoch}.png\"\n",
    "\n",
    "            # Construct the GCS path for saving the image inside the folder\n",
    "            gcs_content_save_path = os.path.join(self.output_save_path, test_image_name, image_name)\n",
    "\n",
    "            # Remove the leading 'gs://' and split the path to extract the bucket name\n",
    "            bucket_name, relative_path = self.parse_gcs_path(gcs_content_save_path)\n",
    "\n",
    "            # Upload the image to GCS\n",
    "            storage_client = storage.Client()\n",
    "            bucket = storage_client.bucket(bucket_name)\n",
    "            blob = bucket.blob(relative_path)  \n",
    "            with io.BytesIO() as output:\n",
    "                pil_image_colored.save(output, format='PNG')\n",
    "                output.seek(0)\n",
    "                try:\n",
    "                    # Convert the output to bytes\n",
    "                    image_bytes = output.read()\n",
    "\n",
    "                    # Upload the image bytes to GCS using upload_from_string\n",
    "                    blob.upload_from_string(image_bytes, content_type='image/png')\n",
    "\n",
    "                    print(f'Image saved at: {gcs_content_save_path}')\n",
    "\n",
    "                    # Check if the image exists in GCS\n",
    "                    if blob.exists():\n",
    "                        print(f'Image successfully uploaded to GCS: {gcs_content_save_path}')\n",
    "                    else:\n",
    "                        print(f'Image not found in GCS after upload: {gcs_content_save_path}')\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f'Error uploading image: {e}')\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "# imports================\n",
    "\n",
    "import os\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "from google.cloud import storage\n",
    "from pipeline_scripts.model_file import get_model\n",
    "\n",
    "# Necessary Functions------------------------------------------\n",
    "\n",
    "feature_description = {\n",
    "    \"image\": tf.io.VarLenFeature(tf.float32),\n",
    "    \"image_shape\": tf.io.VarLenFeature(tf.int64),\n",
    "    \"label\": tf.io.VarLenFeature(tf.float32),\n",
    "    \"label_shape\": tf.io.VarLenFeature(tf.int64),\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def parse(serialized_examples):\n",
    "    return tf.io.parse_example(serialized_examples, feature_description)\n",
    "\n",
    "\n",
    "def create_dataset(input_directory):\n",
    "    tfrecord_files = [\n",
    "        f\"{input_directory}{file}\"\n",
    "        for file in tf.io.gfile.listdir(input_directory)\n",
    "        if file.endswith(\".tfrecord\")\n",
    "    ]\n",
    "    dataset = tf.data.TFRecordDataset(tfrecord_files)\n",
    "    dataset = dataset.map(parse)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "# pre-processing functions\n",
    "def bandwise_normalize(input_tensor, epsilon=1e-8):\n",
    "    # Convert the input_tensor to a float32 type\n",
    "    input_tensor = tf.cast(input_tensor, tf.float32)\n",
    "\n",
    "    # Calculate the minimum and maximum values along the channel axis\n",
    "    min_val = tf.reduce_min(input_tensor, axis=2, keepdims=True)\n",
    "    max_val = tf.reduce_max(input_tensor, axis=2, keepdims=True)\n",
    "\n",
    "    # Check for potential numerical instability\n",
    "    denom = max_val - min_val\n",
    "    denom = tf.where(tf.abs(denom) < epsilon, epsilon, denom)\n",
    "\n",
    "    # Normalize the tensor band-wise to the range [0, 1]\n",
    "    normalized_tensor = (input_tensor - min_val) / denom\n",
    "\n",
    "    return normalized_tensor\n",
    "\n",
    "\n",
    "def pad_to_multiple(image, TILE_HT, TILE_WD):\n",
    "    # Get the current dimensions\n",
    "    height, width, channels = image.shape\n",
    "\n",
    "    # Calculate the target dimensions\n",
    "    target_height = tf.cast(tf.math.ceil(height / TILE_HT) * TILE_HT, tf.int32)\n",
    "    target_width = tf.cast(tf.math.ceil(width / TILE_WD) * TILE_WD, tf.int32)\n",
    "\n",
    "    # Calculate the amount of padding\n",
    "    pad_height = target_height - height\n",
    "    pad_width = target_width - width\n",
    "\n",
    "    # Pad the image\n",
    "    padded_image = tf.image.resize_with_crop_or_pad(image, target_height, target_width)\n",
    "\n",
    "    return padded_image\n",
    "\n",
    "\n",
    "def tile_image(fullimg, CHANNELS, TILE_HT, TILE_WD):\n",
    "    fullimg = pad_to_multiple(fullimg, TILE_HT, TILE_WD)\n",
    "    images = tf.expand_dims(fullimg, axis=0)\n",
    "    tiles = tf.image.extract_patches(\n",
    "        images=images,\n",
    "        sizes=[1, TILE_HT, TILE_WD, 1],\n",
    "        strides=[1, TILE_HT, TILE_WD, 1],\n",
    "        rates=[1, 1, 1, 1],\n",
    "        padding=\"VALID\",\n",
    "    )\n",
    "\n",
    "    tiles = tf.squeeze(tiles, axis=0)\n",
    "    nrows = tiles.shape[0]\n",
    "    ncols = tiles.shape[1]\n",
    "    tiles = tf.reshape(tiles, [nrows, ncols, TILE_HT, TILE_WD, CHANNELS])\n",
    "    return tiles\n",
    "\n",
    "\n",
    "def sampling(label_image, threshold_percentage=99.9):\n",
    "    num_zeros = tf.reduce_sum(\n",
    "        tf.cast(tf.equal(label_image, 0), tf.float32), axis=[2, 3, 4]\n",
    "    )\n",
    "\n",
    "    # Calculate the total number of elements in each patch\n",
    "    total_elements = tf.cast(tf.reduce_prod(tf.shape(label_image)[2:]), tf.float32)\n",
    "\n",
    "    # Calculate the percentage of zeros in each patch\n",
    "    percentage_zeros = (num_zeros / total_elements) * 100.0\n",
    "\n",
    "    boolean_mask = percentage_zeros <= threshold_percentage\n",
    "    # Apply the threshold logic\n",
    "    sampled_tensor = tf.cast(percentage_zeros >= threshold_percentage, tf.int32)\n",
    "    return boolean_mask, sampled_tensor\n",
    "\n",
    "\n",
    "def one_hot_encoding(label_tensor):\n",
    "    # Assuming your pixel values are float labels\n",
    "    float_labels = tf.squeeze(\n",
    "        label_tensor, axis=-1\n",
    "    )  # Assuming channel dimension is the last one\n",
    "\n",
    "    # Determine the number of classes dynamically\n",
    "    num_classes = tf.cast(tf.reduce_max(float_labels) + 1, tf.int32)\n",
    "\n",
    "    # One-hot encode each image\n",
    "    one_hot_encoded_images = tf.one_hot(\n",
    "        tf.dtypes.cast(float_labels, tf.int32), depth=num_classes\n",
    "    )\n",
    "\n",
    "    # Print the shape of the resulting tensor and the number of classes\n",
    "    # print(\"Shape of one-hot encoded images:\", one_hot_encoded_images.shape)\n",
    "    # print(\"Number of classes:\", num_classes)\n",
    "\n",
    "    return one_hot_encoded_images\n",
    "\n",
    "\n",
    "def parsing(\n",
    "    dataset,\n",
    "    patch_height,\n",
    "    patch_width,\n",
    "    threshold_percentage,\n",
    "    image_channels,\n",
    "    label_channels,\n",
    "):\n",
    "    image_patch_tensors_list = []\n",
    "    label_patch_tensors_list = []\n",
    "\n",
    "    for parsed_example in dataset:\n",
    "        image_shape = tf.sparse.to_dense(parsed_example[\"image_shape\"])\n",
    "        image = tf.reshape(tf.sparse.to_dense(parsed_example[\"image\"]), image_shape)\n",
    "        label_shape = tf.sparse.to_dense(parsed_example[\"label_shape\"])\n",
    "        label = tf.reshape(tf.sparse.to_dense(parsed_example[\"label\"]), label_shape)\n",
    "\n",
    "        # image normalization\n",
    "        image = bandwise_normalize(image)\n",
    "\n",
    "        # image and label patching\n",
    "        image_patches = tile_image(image, image_channels, patch_height, patch_width)\n",
    "        label_patches = tile_image(label, label_channels, patch_height, patch_width)\n",
    "\n",
    "        # sampling\n",
    "        sampled_mask, sampled_tensor = sampling(label_patches, threshold_percentage)\n",
    "        sampled_image_patches = tf.boolean_mask(image_patches, sampled_mask)\n",
    "        sampled_label_patches = tf.boolean_mask(label_patches, sampled_mask)\n",
    "\n",
    "        # one-hot encoding\n",
    "        sampled_label_patches = one_hot_encoding(sampled_label_patches)\n",
    "\n",
    "        # save them in the list\n",
    "        image_patch_tensors_list.append(sampled_image_patches)\n",
    "        label_patch_tensors_list.append(sampled_label_patches)\n",
    "\n",
    "    return image_patch_tensors_list, label_patch_tensors_list\n",
    "\n",
    "\n",
    "def train_test_datasets(\n",
    "    input_directory,\n",
    "    patch_height,\n",
    "    patch_width,\n",
    "    image_channels,\n",
    "    label_channels,\n",
    "    threshold_percentage,\n",
    "    batch_size,\n",
    "):\n",
    "    dataset = create_dataset(input_directory)\n",
    "    image_patch_tensors_list, label_patch_tensors_list = parsing(\n",
    "        dataset=dataset,\n",
    "        patch_height=patch_height,\n",
    "        patch_width=patch_width,\n",
    "        image_channels=image_channels,\n",
    "        label_channels=label_channels,\n",
    "        threshold_percentage=threshold_percentage,\n",
    "    )\n",
    "\n",
    "    # Combine images and labels from different pairs\n",
    "    combined_images = tf.concat(image_patch_tensors_list, axis=0)\n",
    "    combined_labels = tf.concat(label_patch_tensors_list, axis=0)\n",
    "\n",
    "    # Shuffle the combined data\n",
    "    combined_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (combined_images, combined_labels)\n",
    "    )\n",
    "    combined_dataset = combined_dataset.shuffle(buffer_size=combined_images.shape[0])\n",
    "\n",
    "    # Split the combined dataset into training and validation sets\n",
    "    train_size = int(0.8 * combined_images.shape[0])\n",
    "    train_dataset = combined_dataset.take(train_size)\n",
    "    val_dataset = combined_dataset.skip(train_size)\n",
    "\n",
    "    # Batch the data using TensorFlow's Dataset API\n",
    "    train_dataset = train_dataset.batch(batch_size)\n",
    "    val_dataset = val_dataset.batch(batch_size)\n",
    "\n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def upload_blob(bucket_name, source_file_name, destination_blob_name):\n",
    "    storage_client = storage.Client(project=\"gislogics\")\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "\n",
    "    print(\"File {} uploaded to {}.\".format(source_file_name, destination_blob_name))\n",
    "\n",
    "# Modeling--------------------------------------------------\n",
    "\n",
    "\n",
    "def train(**kwargs):\n",
    "    label_channels = 1\n",
    "    config = load_config('./pipeline_scripts/config.json')\n",
    "    # config[\"img_size\"] = tuple(config[\"img_size\"])\n",
    "    model_path = config.get(\"model_path\")\n",
    "    model_name = config.get(\"model_name\")\n",
    "    test_image_path = config.get(\"test_image_path\")\n",
    "    img_size_value = config.get(\"img_size\")\n",
    "    img_size = (img_size_value,img_size_value)\n",
    "    num_bands = config.get(\"num_bands\")\n",
    "    num_classes = config.get(\"num_classes\")\n",
    "    gcs_path = config.get(\"google_storage_path\")\n",
    "    gcs_tfrecords = config.get(\"gcs_tfrecords\")\n",
    "    class_name = config.get(\"class_name\")\n",
    "    class_optimization = config.get(\"class_optmized_model\")\n",
    "    bucket_name = config.get(\"bucket_name\")\n",
    "    threshold_percentage = config.get(\"threshold_percentage\")\n",
    "    patch_height = config.get(\"patch_height\")\n",
    "    patch_width = config.get(\"patch_width\")\n",
    "    batch_size = config.get(\"batch_size\")\n",
    "    num_epochs = config.get(\"epochs\")\n",
    "    \n",
    "    \n",
    "    input_directory = gcs_tfrecords\n",
    "    image_channels = num_bands\n",
    "    label_channels = 1\n",
    "    \n",
    "    \n",
    "\n",
    "    # get the train and test datasets\n",
    "    train_dataset, val_dataset = train_test_datasets(\n",
    "        gcs_tfrecords,\n",
    "        patch_height,\n",
    "        patch_width,\n",
    "        image_channels,\n",
    "        label_channels,\n",
    "        threshold_percentage,\n",
    "        batch_size,\n",
    "    )\n",
    "    \n",
    "\n",
    "    print(\"Train and Valid datasets are created\")\n",
    "\n",
    "    # create img_size\n",
    "    model = get_model(\n",
    "        img_size=img_size, \n",
    "        num_classes=num_classes, \n",
    "        num_bands=image_channels\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # compilation of model, with custom metric\n",
    "    # compilation of model, with custom metric\n",
    "    if class_optimization:\n",
    "        print(\"OPTIMIZING BOTH\")\n",
    "        metric = - compute_metrics(new_ground_truth, new_predict)[0][class_name] - np.mean(compute_metrics(new_ground_truth, new_predict)[0])\n",
    "        # Both are negative because they should be maximized\n",
    "    else:\n",
    "        metric = - np.mean(compute_metrics(new_ground_truth, new_predict)[0])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\", \n",
    "        metrics=[metric]\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Early stopping after 5 epochs \n",
    "    early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  \n",
    "    patience=5,  \n",
    "    restore_best_weights=True,  \n",
    "    verbose=1  \n",
    "    )\n",
    "    \n",
    "    \n",
    "    # including custom metrics in callbacks\n",
    "    custom_metrics_callback = keras.callbacks.LambdaCallback(\n",
    "    on_epoch_end=lambda epoch, logs: logs.update({\n",
    "        \"class_wise_iou\": compute_metrics(new_ground_truth, new_predict)[0][class_name],\n",
    "        \"class_wise_dice_score\": compute_metrics(new_ground_truth, new_predict)[1][class_name],\n",
    "        \"class_wise_accuracy\": compute_metrics(new_ground_truth, new_predict)[2][class_name],\n",
    "        \"class_wise_precision\": compute_metrics(new_ground_truth, new_predict)[3][class_name],\n",
    "        \"class_wise_recall\": compute_metrics(new_ground_truth, new_predict)[4][class_name],\n",
    "        \"mean_iou\": compute_metrics(new_ground_truth, new_predict)[5],\n",
    "        \"min_class_wise_iou\": np.min(compute_metrics(new_ground_truth, new_predict)[0]),\n",
    "        \"max_class_wise_iou\": np.max(compute_metrics(new_ground_truth, new_predict)[0]),\n",
    "        \"epoch\": epoch,\n",
    "        \"loss\": logs[\"loss\"],\n",
    "        \"val_loss\": logs[\"val_loss\"]}))\n",
    "\n",
    "    \n",
    "    # callbacks and logging\n",
    "    csv_logger = keras.callbacks.CSVLogger(\n",
    "    input_directory + \"logs/\" + f\"training_logs_{model_name}.csv\",\n",
    "    append=True\n",
    "    )\n",
    "\n",
    "    custom_metrics_csv_logger = CustomMetricsCSVLogger(\n",
    "        input_directory + \"logs/\" + f\"training_logs_{model_name}.csv\",\n",
    "        append=True\n",
    "    )\n",
    "    \n",
    "    \n",
    "    log_dir = \"gs://tf_records_bucket/tf_records/Untitled Folder/logs/\"  # Specify the directory to save logs\n",
    "    tensorboard_callback = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "    output_of_image = \"gs://tf_records_bucket/tf_records/Untitled Folder/output\"\n",
    "    from tensorflow.keras.models import load_model\n",
    "    # Combine all callbacks\n",
    "    all_callbacks = [\n",
    "        keras.callbacks.ModelCheckpoint(model_path + model_name, save_best_only=False),\n",
    "        tensorboard_callback,\n",
    "        custom_metrics_callback,\n",
    "        custom_metrics_csv_logger,  # Add the custom_metrics_csv_logger here\n",
    "        # early_stopping,\n",
    "        PredictSegmentationCallback(test_image_path, output_of_image)\n",
    "    ]\n",
    "    # Load the model if a checkpoint exists\n",
    "    if os.path.exists(model_path + model_name):\n",
    "        model = load_model(model_path + model_name)\n",
    "        print(\"Loaded model from checkpoint\")\n",
    "    else:\n",
    "        # If no checkpoint exists, create a new model\n",
    "        model = get_model(\n",
    "            img_size=img_size, num_classes=num_classes, num_bands=image_channels\n",
    "        )\n",
    "        model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\")\n",
    "        print(\"Created a new model\")\n",
    "\n",
    "    # Continue training\n",
    "    model_history = model.fit(\n",
    "        train_dataset,\n",
    "        epochs=num_epochs,\n",
    "        callbacks=all_callbacks,\n",
    "        batch_size=32,\n",
    "        validation_data=val_dataset,\n",
    "    )\n",
    "\n",
    "    if early_stopping.stopped_epoch > 0:\n",
    "        print(f\"Training stopped at epoch {early_stopping.stopped_epoch} due to early stopping.\")\n",
    "    else:\n",
    "        print(\"Training completed all epochs.\")\n",
    "    # Save the model after training\n",
    "    # model.save(model_path + model_name)\n",
    "    # print(\"Model saved locally\")\n",
    "\n",
    "    upload_blob(bucket_name, model_path + model_name, \"model/\" + model_name)\n",
    "    print(\"Uploaded to cloud storage successfully\")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    train()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ! tensorboard --logdir gs://tf_records_bucket/tf_records/Untitled\\ Folder/logs/ \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.argv = [\n",
    "'your_script.py',\n",
    "'--model_path', './trained_model/patch_h8_w8_batch_32_on_0530_2022.hdf5',\n",
    "'--google_storage_path', 'gs://tf_records_bucket/model/',\n",
    "'--model_name', 'patch_h8_w8_batch_32_on_0530_2022.hdf5',\n",
    "'--test_image_path', 'gs://tf_records_bucket/labels/area2_0530_2022_8bands.tif',\n",
    "'--img_size', '8',\n",
    "'--num_bands', '8',\n",
    "'--num_classes', '23',\n",
    "'--gcs_tfrecords', 'gs://tf_records_bucket/tf_records/Untitled Folder/',\n",
    "'--class_name', '18',\n",
    "'--label_path', 'gs://tf_records_bucket/labels/image_extent_mask_1123.tif',\n",
    "'--class_optmized_model', '1',\n",
    "'--bucket_name', 'tf_records_bucket',\n",
    "'--threshold_percentage', '99.9',\n",
    "'--patch_height', '8',\n",
    "'--patch_width', '8',\n",
    "'--batch_size', '32',\n",
    "'--epochs', '300'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old code which i referred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install rasterio\n",
    "!pip install geopandas\n",
    "!pip install Pillow\n",
    "\n",
    "\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import mapping\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import time\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from shapely.geometry import mapping\n",
    "from pipeline_scripts.prediction import *\n",
    "from pipeline_scripts.eval import *\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from google.cloud import storage\n",
    "import os\n",
    "import csv\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "\n",
    "metrics_results = compute_metrics(new_ground_truth, new_predict)\n",
    "\n",
    "\n",
    "# metrics_results\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "class_colors = {\n",
    "        1: ( 5, 5, 230),\n",
    "        2: (190, 60, 15),\n",
    "        3: (65, 240, 125),\n",
    "        4: (105, 200, 95),\n",
    "        5: ( 30, 115, 10),\n",
    "        6: ( 255, 196, 34),\n",
    "        7: (110, 85, 5),\n",
    "        8: ( 235, 235, 220),\n",
    "        9: (120, 216, 47),\n",
    "        10: ( 84, 142, 128),\n",
    "        11: ( 84, 142, 128),\n",
    "        12: ( 84, 142, 128),\n",
    "        13: ( 50, 255, 215),\n",
    "        14: ( 50, 255, 215),\n",
    "        15: ( 50, 255, 215),\n",
    "        16: ( 193, 255, 0),\n",
    "        17: ( 105, 200, 95),\n",
    "        18: (105, 200, 95),\n",
    "        19: ( 105, 200, 95),\n",
    "        20: (193, 255, 0),\n",
    "        21: ( 255, 50, 185),\n",
    "        22: (255, 255, 255),\n",
    "}\n",
    "\n",
    "# Create a colormap using the class-color mapping\n",
    "colors = [class_colors[i] for i in range(1, 23)]\n",
    "normalized_colors_array = np.array([tuple(np.array(v) / 255.0) for v in class_colors.values()])\n",
    "\n",
    "cmap_image = ListedColormap(normalized_colors_array)\n",
    "\n",
    "\n",
    "\n",
    "class CustomMetricsCSVLogger(Callback):\n",
    "    def __init__(self, filename, separator=',', append=True):\n",
    "        super(CustomMetricsCSVLogger, self).__init__()\n",
    "        self.filename = filename\n",
    "        self.separator = separator\n",
    "        self.append = append\n",
    "        self.keys = None\n",
    "        self.append_header = True\n",
    "        self.max_epoch = 0  # Track the highest epoch number encountered\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        # Initialize min and max class-wise IOU at the beginning of each epoch\n",
    "        self.min_class_wise_iou = 100\n",
    "        self.max_class_wise_iou = 0\n",
    "\n",
    "        # Check if the file exists in Cloud Storage, if not, create it\n",
    "        if not self.file_exists():\n",
    "            self.create_file()\n",
    "\n",
    "    def file_exists(self):\n",
    "        # Check if the file exists in Cloud Storage\n",
    "        storage_client = storage.Client()\n",
    "        bucket_name, blob_name = self.parse_gcs_path(self.filename)\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(blob_name)\n",
    "        return blob.exists()\n",
    "\n",
    "    def create_file(self):\n",
    "        # Create the file in Cloud Storage and write the header\n",
    "        storage_client = storage.Client()\n",
    "        bucket_name, blob_name = self.parse_gcs_path(self.filename)\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(blob_name)\n",
    "\n",
    "        # Write the header to the blob\n",
    "        header = 'epoch,loss,val_loss,class_wise_iou,class_wise_dice_score,class_wise_accuracy,class_wise_precision,class_wise_recall,mean_iou,min_class_wise_iou,max_class_wise_iou\\n'\n",
    "        blob.upload_from_string(header)\n",
    "\n",
    "    def parse_gcs_path(self, gcs_path):\n",
    "        # Parse the Google Cloud Storage path to extract bucket name and blob name\n",
    "        parts = gcs_path.replace('gs://', '').split('/')\n",
    "        return parts[0], '/'.join(parts[1:])\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        if self.keys is None:\n",
    "            self.keys = sorted(logs.keys())\n",
    "\n",
    "        # Extract class-wise IOU\n",
    "        class_wise_iou = logs.get('class_wise_iou', 0.0)\n",
    "\n",
    "        # Update min and max class-wise IOU\n",
    "        self.min_class_wise_iou = min(self.min_class_wise_iou, class_wise_iou)\n",
    "        self.max_class_wise_iou = max(self.max_class_wise_iou, class_wise_iou)\n",
    "\n",
    "        # Append the row to the file in Cloud Storage\n",
    "        storage_client = storage.Client()\n",
    "        bucket_name, blob_name = self.parse_gcs_path(self.filename)\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(blob_name)\n",
    "\n",
    "        # Download existing content\n",
    "        existing_content = blob.download_as_text() if blob.exists() else \"\"\n",
    "\n",
    "        # Extract metrics values from logs\n",
    "        metrics_values = [str(logs[key]) for key in ['loss', 'val_loss', 'class_wise_iou', 'class_wise_dice_score',\n",
    "                                                    'class_wise_accuracy', 'class_wise_precision', 'class_wise_recall', 'mean_iou']]\n",
    "\n",
    "        # Check if metrics for the current epoch already exist\n",
    "        epoch_exists = any(f\"{epoch},\" in line for line in existing_content.split('\\n'))\n",
    "\n",
    "        # If the file is empty or epoch entry doesn't exist, append the metrics\n",
    "        if not existing_content or not epoch_exists:\n",
    "            updated_content = existing_content + f\"{epoch},{','.join(metrics_values)},{self.min_class_wise_iou},{self.max_class_wise_iou}\\n\"\n",
    "        else:\n",
    "            # Get the maximum epoch number in the existing content\n",
    "            max_existing_epoch = max(\n",
    "                int(line.split(',')[0]) for line in existing_content.split('\\n') if line.strip() and not line.startswith('epoch')\n",
    "            )\n",
    "\n",
    "            # Increment the epoch for the new entries\n",
    "            updated_content = existing_content + f\"{max_existing_epoch + 1},{','.join(metrics_values)},{self.min_class_wise_iou},{self.max_class_wise_iou}\\n\"\n",
    "\n",
    "        # Update the highest epoch number\n",
    "        self.max_epoch = max(self.max_epoch, epoch)\n",
    "\n",
    "        # Upload updated content\n",
    "        blob.upload_from_string(updated_content)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "class PredictSegmentationCallback(Callback):\n",
    "    def __init__(self, test_image_path, output_save_path):\n",
    "        super(PredictSegmentationCallback, self).__init__()\n",
    "        self.test_image_path = test_image_path\n",
    "        self.output_save_path = output_save_path\n",
    "        self.last_predicted_array = None\n",
    "\n",
    "    def parse_gcs_path(self, gcs_path):\n",
    "        # Parse the Google Cloud Storage path to extract bucket name and blob name\n",
    "        parts = gcs_path.replace('gs://', '').split('/')\n",
    "        return parts[0], '/'.join(parts[1:])\n",
    "\n",
    "    # def on_epoch_end(self, epoch, logs=None):\n",
    "    #     # Call your prediction function\n",
    "    #     self.last_predicted_array = prediction_function_img(self.test_image_path)\n",
    "    #     normalized_array_image = self.last_predicted_array / (np.max(self.last_predicted_array) + 1e-10)\n",
    "    #     # Display the predicted array with the specified colormap\n",
    "    #     # plt.figure(figsize=(8, 4))\n",
    "    #     # plt.imshow(normalized_array_image, cmap=cmap_image)  # Adjust the colormap as needed\n",
    "    #     # plt.title(f'Predicted Array - Epoch {epoch}')\n",
    "    #     # plt.show()\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        # Save the last predicted array after training\n",
    "        if self.last_predicted_array is None:\n",
    "            # Convert NumPy array to PIL Image\n",
    "            self.last_predicted_array = prediction_function_img(self.test_image_path)\n",
    "            pil_image = Image.fromarray((self.last_predicted_array * 255).astype(np.uint8))\n",
    "\n",
    "            # Apply the colormap to the PIL Image\n",
    "            pil_image_colored = pil_image.convert('P', palette=Image.ADAPTIVE, colors=len(class_colors))\n",
    "            pil_image_colored.putpalette(np.array(normalized_colors_array * 255, dtype=np.uint8).flatten())\n",
    "\n",
    "            # Extract image name from the test_image_path for Google Cloud Storage\n",
    "            image_name = os.path.basename(self.test_image_path)\n",
    "\n",
    "            # Create the full GCS path for saving the predicted array as an image\n",
    "            gcs_content_save_path = f'{self.output_save_path}/output_{image_name}.png'\n",
    "\n",
    "            # Upload the image directly to GCS\n",
    "            storage_client = storage.Client()\n",
    "            bucket_name, blob_name = self.parse_gcs_path(gcs_content_save_path)\n",
    "            bucket = storage_client.bucket(bucket_name)\n",
    "            blob = bucket.blob(blob_name)\n",
    "\n",
    "            # Save the PIL Image to a BytesIO object\n",
    "            image_io = io.BytesIO()\n",
    "            pil_image_colored.save(image_io, format='PNG')\n",
    "\n",
    "            image_io.seek(0)\n",
    "\n",
    "            # Upload the image content to GCS\n",
    "            blob.upload_from_file(image_io, content_type='image/png')  # Set the correct content type\n",
    "\n",
    "            print(f'Last predicted array saved at: {gcs_content_save_path}')\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "# imports================\n",
    "\n",
    "import os\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "from google.cloud import storage\n",
    "from pipeline_scripts.model_file import get_model\n",
    "\n",
    "# Necessary Functions------------------------------------------\n",
    "\n",
    "feature_description = {\n",
    "    \"image\": tf.io.VarLenFeature(tf.float32),\n",
    "    \"image_shape\": tf.io.VarLenFeature(tf.int64),\n",
    "    \"label\": tf.io.VarLenFeature(tf.float32),\n",
    "    \"label_shape\": tf.io.VarLenFeature(tf.int64),\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def parse(serialized_examples):\n",
    "    return tf.io.parse_example(serialized_examples, feature_description)\n",
    "\n",
    "\n",
    "def create_dataset(input_directory):\n",
    "    tfrecord_files = [\n",
    "        f\"{input_directory}{file}\"\n",
    "        for file in tf.io.gfile.listdir(input_directory)\n",
    "        if file.endswith(\".tfrecord\")\n",
    "    ]\n",
    "    dataset = tf.data.TFRecordDataset(tfrecord_files)\n",
    "    dataset = dataset.map(parse)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "# pre-processing functions\n",
    "def bandwise_normalize(input_tensor, epsilon=1e-8):\n",
    "    # Convert the input_tensor to a float32 type\n",
    "    input_tensor = tf.cast(input_tensor, tf.float32)\n",
    "\n",
    "    # Calculate the minimum and maximum values along the channel axis\n",
    "    min_val = tf.reduce_min(input_tensor, axis=2, keepdims=True)\n",
    "    max_val = tf.reduce_max(input_tensor, axis=2, keepdims=True)\n",
    "\n",
    "    # Check for potential numerical instability\n",
    "    denom = max_val - min_val\n",
    "    denom = tf.where(tf.abs(denom) < epsilon, epsilon, denom)\n",
    "\n",
    "    # Normalize the tensor band-wise to the range [0, 1]\n",
    "    normalized_tensor = (input_tensor - min_val) / denom\n",
    "\n",
    "    return normalized_tensor\n",
    "\n",
    "\n",
    "def pad_to_multiple(image, TILE_HT, TILE_WD):\n",
    "    # Get the current dimensions\n",
    "    height, width, channels = image.shape\n",
    "\n",
    "    # Calculate the target dimensions\n",
    "    target_height = tf.cast(tf.math.ceil(height / TILE_HT) * TILE_HT, tf.int32)\n",
    "    target_width = tf.cast(tf.math.ceil(width / TILE_WD) * TILE_WD, tf.int32)\n",
    "\n",
    "    # Calculate the amount of padding\n",
    "    pad_height = target_height - height\n",
    "    pad_width = target_width - width\n",
    "\n",
    "    # Pad the image\n",
    "    padded_image = tf.image.resize_with_crop_or_pad(image, target_height, target_width)\n",
    "\n",
    "    return padded_image\n",
    "\n",
    "\n",
    "def tile_image(fullimg, CHANNELS, TILE_HT, TILE_WD):\n",
    "    fullimg = pad_to_multiple(fullimg, TILE_HT, TILE_WD)\n",
    "    images = tf.expand_dims(fullimg, axis=0)\n",
    "    tiles = tf.image.extract_patches(\n",
    "        images=images,\n",
    "        sizes=[1, TILE_HT, TILE_WD, 1],\n",
    "        strides=[1, TILE_HT, TILE_WD, 1],\n",
    "        rates=[1, 1, 1, 1],\n",
    "        padding=\"VALID\",\n",
    "    )\n",
    "\n",
    "    tiles = tf.squeeze(tiles, axis=0)\n",
    "    nrows = tiles.shape[0]\n",
    "    ncols = tiles.shape[1]\n",
    "    tiles = tf.reshape(tiles, [nrows, ncols, TILE_HT, TILE_WD, CHANNELS])\n",
    "    return tiles\n",
    "\n",
    "\n",
    "def sampling(label_image, threshold_percentage=99.9):\n",
    "    num_zeros = tf.reduce_sum(\n",
    "        tf.cast(tf.equal(label_image, 0), tf.float32), axis=[2, 3, 4]\n",
    "    )\n",
    "\n",
    "    # Calculate the total number of elements in each patch\n",
    "    total_elements = tf.cast(tf.reduce_prod(tf.shape(label_image)[2:]), tf.float32)\n",
    "\n",
    "    # Calculate the percentage of zeros in each patch\n",
    "    percentage_zeros = (num_zeros / total_elements) * 100.0\n",
    "\n",
    "    boolean_mask = percentage_zeros <= threshold_percentage\n",
    "    # Apply the threshold logic\n",
    "    sampled_tensor = tf.cast(percentage_zeros >= threshold_percentage, tf.int32)\n",
    "    return boolean_mask, sampled_tensor\n",
    "\n",
    "\n",
    "def one_hot_encoding(label_tensor):\n",
    "    # Assuming your pixel values are float labels\n",
    "    float_labels = tf.squeeze(\n",
    "        label_tensor, axis=-1\n",
    "    )  # Assuming channel dimension is the last one\n",
    "\n",
    "    # Determine the number of classes dynamically\n",
    "    num_classes = tf.cast(tf.reduce_max(float_labels) + 1, tf.int32)\n",
    "\n",
    "    # One-hot encode each image\n",
    "    one_hot_encoded_images = tf.one_hot(\n",
    "        tf.dtypes.cast(float_labels, tf.int32), depth=num_classes\n",
    "    )\n",
    "\n",
    "    # Print the shape of the resulting tensor and the number of classes\n",
    "    # print(\"Shape of one-hot encoded images:\", one_hot_encoded_images.shape)\n",
    "    # print(\"Number of classes:\", num_classes)\n",
    "\n",
    "    return one_hot_encoded_images\n",
    "\n",
    "\n",
    "def parsing(\n",
    "    dataset,\n",
    "    patch_height,\n",
    "    patch_width,\n",
    "    threshold_percentage,\n",
    "    image_channels,\n",
    "    label_channels,\n",
    "):\n",
    "    image_patch_tensors_list = []\n",
    "    label_patch_tensors_list = []\n",
    "\n",
    "    for parsed_example in dataset:\n",
    "        image_shape = tf.sparse.to_dense(parsed_example[\"image_shape\"])\n",
    "        image = tf.reshape(tf.sparse.to_dense(parsed_example[\"image\"]), image_shape)\n",
    "        label_shape = tf.sparse.to_dense(parsed_example[\"label_shape\"])\n",
    "        label = tf.reshape(tf.sparse.to_dense(parsed_example[\"label\"]), label_shape)\n",
    "\n",
    "        # image normalization\n",
    "        image = bandwise_normalize(image)\n",
    "\n",
    "        # image and label patching\n",
    "        image_patches = tile_image(image, image_channels, patch_height, patch_width)\n",
    "        label_patches = tile_image(label, label_channels, patch_height, patch_width)\n",
    "\n",
    "        # sampling\n",
    "        sampled_mask, sampled_tensor = sampling(label_patches, threshold_percentage)\n",
    "        sampled_image_patches = tf.boolean_mask(image_patches, sampled_mask)\n",
    "        sampled_label_patches = tf.boolean_mask(label_patches, sampled_mask)\n",
    "\n",
    "        # one-hot encoding\n",
    "        sampled_label_patches = one_hot_encoding(sampled_label_patches)\n",
    "\n",
    "        # save them in the list\n",
    "        image_patch_tensors_list.append(sampled_image_patches)\n",
    "        label_patch_tensors_list.append(sampled_label_patches)\n",
    "\n",
    "    return image_patch_tensors_list, label_patch_tensors_list\n",
    "\n",
    "\n",
    "def train_test_datasets(\n",
    "    input_directory,\n",
    "    patch_height,\n",
    "    patch_width,\n",
    "    image_channels,\n",
    "    label_channels,\n",
    "    threshold_percentage,\n",
    "    batch_size,\n",
    "):\n",
    "    dataset = create_dataset(input_directory)\n",
    "    image_patch_tensors_list, label_patch_tensors_list = parsing(\n",
    "        dataset=dataset,\n",
    "        patch_height=patch_height,\n",
    "        patch_width=patch_width,\n",
    "        image_channels=image_channels,\n",
    "        label_channels=label_channels,\n",
    "        threshold_percentage=threshold_percentage,\n",
    "    )\n",
    "\n",
    "    # Combine images and labels from different pairs\n",
    "    combined_images = tf.concat(image_patch_tensors_list, axis=0)\n",
    "    combined_labels = tf.concat(label_patch_tensors_list, axis=0)\n",
    "\n",
    "    # Shuffle the combined data\n",
    "    combined_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (combined_images, combined_labels)\n",
    "    )\n",
    "    combined_dataset = combined_dataset.shuffle(buffer_size=combined_images.shape[0])\n",
    "\n",
    "    # Split the combined dataset into training and validation sets\n",
    "    train_size = int(0.8 * combined_images.shape[0])\n",
    "    train_dataset = combined_dataset.take(train_size)\n",
    "    val_dataset = combined_dataset.skip(train_size)\n",
    "\n",
    "    # Batch the data using TensorFlow's Dataset API\n",
    "    train_dataset = train_dataset.batch(batch_size)\n",
    "    val_dataset = val_dataset.batch(batch_size)\n",
    "\n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def upload_blob(bucket_name, source_file_name, destination_blob_name):\n",
    "    storage_client = storage.Client(project=\"gislogics\")\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "\n",
    "    print(\"File {} uploaded to {}.\".format(source_file_name, destination_blob_name))\n",
    "\n",
    "# Modeling--------------------------------------------------\n",
    "\n",
    "\n",
    "def train(**kwargs):\n",
    "    input_directory = kwargs.get(\"input_directory\")\n",
    "    threshold_percentage = kwargs.get(\"threshold_percentage\")\n",
    "    image_channels = kwargs.get(\"image_channels\")\n",
    "    label_channels = kwargs.get(\"label_channels\")\n",
    "    patch_height = kwargs.get(\"patch_height\")\n",
    "    patch_width = kwargs.get(\"patch_width\")\n",
    "    batch_size = kwargs.get(\"batch_size\")\n",
    "    num_classes = kwargs.get(\"num_classes\")\n",
    "    model_path = kwargs.get(\"model_path\")\n",
    "    bucket_name = kwargs.get(\"bucket_name\")\n",
    "    img_size = (patch_height, patch_width)\n",
    "    model_name = f\"patch_h{patch_height}_w{patch_width}_batch_{batch_size}_on_0609_0617.hdf5\"\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    # get the train and test datasets\n",
    "    train_dataset, val_dataset = train_test_datasets(\n",
    "        input_directory,\n",
    "        patch_height,\n",
    "        patch_width,\n",
    "        image_channels,\n",
    "        label_channels,\n",
    "        threshold_percentage,\n",
    "        batch_size,\n",
    "    )\n",
    "    \n",
    "    print(\"Train and Valid datasets are created\")\n",
    "\n",
    "    # create img_size\n",
    "    model = get_model(\n",
    "        img_size=img_size, \n",
    "        num_classes=num_classes, \n",
    "        num_bands=image_channels\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # compilation of model, with custom metric\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\", \n",
    "        metrics=[np.mean(compute_metrics(new_ground_truth, new_predict)[0])]\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Early stopping after 5 epochs \n",
    "    early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  \n",
    "    patience=5,  \n",
    "    restore_best_weights=True,  \n",
    "    verbose=1  \n",
    "    )\n",
    "    \n",
    "    \n",
    "    # including custom metrics in callbacks\n",
    "    custom_metrics_callback = keras.callbacks.LambdaCallback(\n",
    "    on_epoch_end=lambda epoch, logs: logs.update({\n",
    "        \"class_wise_iou\": metrics_results[0][19],\n",
    "        \"class_wise_dice_score\": metrics_results[1][19],\n",
    "        \"class_wise_accuracy\": metrics_results[2][19],\n",
    "        \"class_wise_precision\": metrics_results[3][19],\n",
    "        \"class_wise_recall\": metrics_results[4][19],\n",
    "        \"mean_iou\": metrics_results[5],\n",
    "        \"min_class_wise_iou\": np.min(metrics_results[0]),\n",
    "        \"max_class_wise_iou\": np.max(metrics_results[0]),\n",
    "        \"epoch\": epoch,\n",
    "        \"loss\": logs[\"loss\"],\n",
    "        \"val_loss\": logs[\"val_loss\"]}))\n",
    "\n",
    "    \n",
    "    # callbacks and logging\n",
    "    csv_logger = keras.callbacks.CSVLogger(\n",
    "    input_directory + \"logs/\" + f\"training_logs_{model_name}.csv\",\n",
    "    append=True\n",
    "    )\n",
    "\n",
    "    custom_metrics_csv_logger = CustomMetricsCSVLogger(\n",
    "        input_params[\"input_directory\"] + \"logs/\" + f\"training_logs_{model_name}.csv\",\n",
    "        append=True\n",
    "    )\n",
    "    \n",
    "    \n",
    "    log_dir = \"gs://tf_records_bucket/tf_records/Untitled Folder/logs/\"  # Specify the directory to save logs\n",
    "    tensorboard_callback = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "    output_of_image = \"gs://tf_records_bucket/tf_records/Untitled Folder/output\"\n",
    "    from tensorflow.keras.models import load_model\n",
    "    # Combine all callbacks\n",
    "    all_callbacks = [\n",
    "        keras.callbacks.ModelCheckpoint(model_path + model_name, save_best_only=False),\n",
    "        tensorboard_callback,\n",
    "        custom_metrics_callback,\n",
    "        custom_metrics_csv_logger,  # Add the custom_metrics_csv_logger here\n",
    "        # early_stopping,\n",
    "        PredictSegmentationCallback(test_image_path, output_of_image)\n",
    "    ]\n",
    "    # Load the model if a checkpoint exists\n",
    "    if os.path.exists(model_path + model_name):\n",
    "        model = load_model(model_path + model_name)\n",
    "        print(\"Loaded model from checkpoint\")\n",
    "    else:\n",
    "        # If no checkpoint exists, create a new model\n",
    "        model = get_model(\n",
    "            img_size=img_size, num_classes=num_classes, num_bands=image_channels\n",
    "        )\n",
    "        model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\")\n",
    "        print(\"Created a new model\")\n",
    "\n",
    "    # Continue training\n",
    "    model_history = model.fit(\n",
    "        train_dataset,\n",
    "        epochs=3,\n",
    "        callbacks=all_callbacks,\n",
    "        batch_size=32,\n",
    "        validation_data=val_dataset,\n",
    "    )\n",
    "\n",
    "    if early_stopping.stopped_epoch > 0:\n",
    "        print(f\"Training stopped at epoch {early_stopping.stopped_epoch} due to early stopping.\")\n",
    "    else:\n",
    "        print(\"Training completed all epochs.\")\n",
    "    # Save the model after training\n",
    "    # model.save(model_path + model_name)\n",
    "    # print(\"Model saved locally\")\n",
    "\n",
    "    upload_blob(bucket_name, model_path + model_name, \"model/\" + model_name)\n",
    "    print(\"Uploaded to cloud storage successfully\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_params = {\n",
    "        \"input_directory\": \"gs://tf_records_bucket/tf_records/Untitled Folder/\",  # make sure / is there at the end,\n",
    "        \"bucket_name\": \"tf_records_bucket\",\n",
    "        \"threshold_percentage\": 99.9,\n",
    "        \"image_channels\": 8,  # 8 bands images as input\n",
    "        \"label_channels\": 1,\n",
    "        \"patch_height\": 8,\n",
    "        \"patch_width\": 8,\n",
    "        \"batch_size\": 32,\n",
    "        \"num_classes\": 23,\n",
    "        \"model_path\": \"trained_model/\",\n",
    "    }  # makesure there is a slash at the end of the path  # Choose an appropriate batch size\n",
    "\n",
    "    train(**input_params)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ! tensorboard --logdir gs://tf_records_bucket/tf_records/Untitled\\ Folder/logs/ "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
